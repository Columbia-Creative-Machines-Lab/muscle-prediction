%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document



% The following packages can be found on http:\\www.ctan.org
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{hyperref}
\usepackage{verbatim}
\usepackage{caption}
\usepackage{clrscode3e}

\title{\LARGE \bf
Modelling the behavior of a soft material-actuator
}

\author{%Aslan Miriyev, Sam Cohen, Neil Chen%Huibert Kwakernaak$^{1}$ and Pradeep Misra$^{2}$% <-this % stops a space
\begin{tabular}{ c c c }
\renewcommand{\arraystretch}{1}
	Aslan Miriyev & Sam Cohen & Neil Chen \\
    aslan.miriyev@columbia.edu & slc2206@columbia.edu & neil.chen@columbia.edu
\end{tabular}
}
\begin{document}
\renewcommand{\arraystretch}{1.2}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
%Inspired by natural muscle, a key challenge in soft robotics is to develop self-contained electrically driven soft actuators with high strain density. Various characteristics of existing technologies, such as the high voltages required to trigger electroactive polymers ( > 1KV), low strain ( < 10%) of shape memory alloys and the need for external compressors and pressure-regulating components for hydraulic or pneumatic fluidicelastomer actuators, limit their practicality for untethered applications. Here we show a single self-contained soft robust composite material that combines the elastic properties of a polymeric matrix and the extreme volume change accompanying liquid–vapor transition. The material combines a high strain (up to 900%) and correspondingly high stress (up to 1.3 MPa) with low density (0.84 g cm−3). Along with its extremely low cost (about 3 cent per gram), simplicity of fabrication and environment-friendliness, these properties could enable new kinds of electrically driven entirely soft robots.
Self-contained electrically-driven soft actuators with high strain density, low cost, simple fabrication methods, and low current draw represent a key challenge in soft robotics. We present several methods for predictive analysis of the behavior of such an actuator, with a focus on time-series analysis of actuator load and wear.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
A soft, robust, self-contained composite material-actuator exhibiting high actuation stress along with very high actuation strain resolves many of the issues confronted by traditional soft actuation solutions (FEAs, PAMs, DEAs, etc.). The complex electrical, chemical, and physical behavior of these actuators, however, presents multiple steady-state and transient response controls problems.

Here we approach one of these problems: we attempt to evaluate the load characteristic, and in turn the wear, of a single soft material-actuator sample over time.

We evaluate the performance of three separate approaches to modelling the behavior of this soft material-actuator:
\begin{enumerate}
	\item Function approximation of actuator load cycle behavior via deep learning.
  \item Regression analysis on derived features of actuator load cycle characteristic.
  \item Fourier analysis/time-series autoregression methods.
\end{enumerate}

Ultimately we find that more input features and/or samples are necessary to usefully model the behavior of such an actuator.

%This template, modified in MS Word 2003 and saved as ÒWord 97-2003 \& 6.0/95 Ð RTFÓ for the PC, provides authors with most of the formatting specifications needed for preparing electronic versions of their papers. All standard paper components have been specified for three reasons: (1) ease of use when formatting individual papers, (2) automatic compliance to electronic requirements that facilitate the concurrent or later production of electronic products, and (3) conformity of style throughout a conference proceedings. Margins, column widths, line spacing, and type styles are built-in; examples of the type styles are provided throughout this document and are identified in italic type, within parentheses, following the example. Some components, such as multi-leveled equations, graphics, and tables are not prescribed, although the various table text styles are provided. The formatter will need to create these components, incorporating the applicable criteria that follow.

\section{DATA}

\subsection{Experimental Setup}
The experimental setup and data generation process consists of a new (or \href{https://www.cambridge.org/core/journals/mrs-communications/article/rejuvenation-of-soft-materialactuator/0D5EA2D555F28C616AB00B773DDFF313}{rejuvenated}) actuator sample enclosed in an Instron load cell. The actuator is sufficiently constrained such that it can neither extend nor contract.

During measurement, a cycle begins whenever \verb|Load| reaches $0N$. At the peak of a cycle, a \verb|Load| threshold of roughly $135N$ has been reached, and current is no longer supplied to the actuator. Thus cycle \verb|Load| minima and maxima remain roughly fixed between cycles. This behavior is visible in \textbf{Figure \ref{CyclesOverlaid}}.

\subsection{Dataset}

The dataset (\verb|raw_cycles.csv|, which can be downloaded \href{https://github.com/Columbia-Creative-Machines-Lab/muscle-prediction}{here}) consists of $2$ features and $170,103$ time-series samples measured at $0.100s$ intervals. Please preview the dataset online, or refer to \textbf{Table \ref{Dataset}}, for more information.

The data represents $47$ actuation \textit{cycles}. A \textit{cycle} consists of a \textit{heating} phase and a cooling phase. During \textit{heating}, the actuator exerts positive force, and thus \verb|Load| is convex increasing with time. During \textit{cooling}, the actuator gradually exerts less force, and thus \verb|Load| is convex decreasing with time. 

\begin{table}[h] % h for here, not the top of the document
  \centering
  \begin{tabular}{ c | c c }
      Feature & Unit & Format \\
      \hline
      \verb|Time| & seconds & 3 decimal places \\
      \verb|Load| & Newtons & 5 decimal places
  \end{tabular}
  \label{Dataset}
  \caption*{Table \ref{Dataset}. Unit and format of features in input dataset.}
\end{table}

\label{CyclesOverlaid}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{assets/cyclesoverlaid.png}
    \caption{Heating-cooling cycles over time.}
\end{figure}

Specified in this manner, the dataset consists of $47$ complete cycles. Note that the local maximum and local minimum \verb|Load| values of each cycle are roughly equal.

\subsection{Exploratory Data Analysis}

We begin by plotting and quantifying certain characteristics of the dataset:
\begin{enumerate}
\itemsep0em 
\item Cycle period over cycle number:  \textbf{Figure \ref{PeriodGrowth}}
\item Cycle heating time over cycle number: \textbf{Figure \ref{HeatingDurations}}
\item Cycle cooling time over cycle number: \textbf{Figure \ref{CoolingDurations}}

\end{enumerate}


\label{PeriodGrowth}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{assets/period_growth.png}
    \caption{\textbf{Figure \ref{PeriodGrowth}.} Period growth as a function of cycle number.}
\end{figure}

\label{HeatingDurations}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{assets/heating_durations.png}
    \caption{\textbf{Figure \ref{HeatingDurations}.} Heating duration as a function of cycle number.}
\end{figure}

\label{CoolingDurations}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{assets/cooling_durations.png}
    \caption{\textbf{Figure \ref{CoolingDurations}.} Cooling duration as a function of cycle number.}
\end{figure}

We find that the average cycle heating and cooling times are 91.29 seconds and 265.32 seconds, respectfully. We also note that cooling and heating durations appear to grow exponentially over the number of cycles. This trend is likely due to a variety of physical phenomena occurring within the actuator, such as gradual ethanol escape and internal heat build up.

\section{DERIVED FEATURES}

\subsection{Motivation}

We attempt to inject expert knowledge about the actuator system's behavior by constructing $11$ derived features. These features encode approximately constant maximum/minimum-\verb|Load| behavior, $n^\text{th}$-order moments of each cycle, cycle index, and more. Each feature is manually computed over the entire dataset; derived features are excluded from validation/testing sets.

\subsection{Computation}

Table \ref{derived} enumerates all derived features.

%\clearpage
\begin{table}[h]
  \centering
  \begin{tabular}{ c | c }
    \textbf{Feature} & \textbf{Meaning} \\
    \hline
    Time & Absolute time since $0.00s$ \\
    Load & Absolute load from $0N$ \\
    Min & One-hot encoding of local minima \\
    Max & One-hot encoding of local maxima \\
    Cycle & Index of cycle from $0$ \\
    Area & Area under cycle curve, calculated as $\int_{t_{min}}^{t_{max}}$ \verb|Load| $dt_i$ \\
    Heating & Time elapsed while heating, calculated as $t_{max} - t_{min}$ \\
    Cooling & Time elapsed while cooling, calculated as $t_{end} - t_{max}$ \\
    HCprop & Proportion $\frac{\texttt{Heating}}{\texttt{Cooling}}$ of heating time to cooling time \\
    Period & Period of cycle, equivalent to \verb|Heating| $+$ \verb|Cooling| \\
    Tail & Time elapsed while $\verb|Load| \leq \big{(}0.1 \times \verb|Max Load|\big{)}$ \\
    Belly & \begin{tabular}{c} Multiple integral between \verb|Cooling| curve and \\ line tangent to both \verb|Max Load| and \verb|Load|$_{t_{end}}$\end{tabular} \\
    Kurt & Kurtosis of curve (fourth standardized moment) \\
    Skew & Skewness of curve
  \end{tabular}
  \label{derived}
  \caption*{\textbf{Table \ref{derived}.} All derived features and calculations involved.}
\end{table}


\section{Learning Periodic Functions}
We begin by fine-tuning a model to learn simple periodic functions. As our eventual goal is to predict the behavior of a non-periodic Time series, we opt to similarly treat this task as a Time series forecasting problem in which our samples are not independent, but are rather related to one another across time.

\subsection{Formating Data for Supervised Learning}


To build our set of examples, we first calculated the period \textit{P} of a cycle.

A single example is formatted such that it contains three full cycles worth of \textit{input} points (\textit{Px3}) and one full cycle worth of \textit{label} points (\textit{P}).

We then utilize the "sliding window" method to step along our univariate time series one point at a time, collecting the proceeding \textit{Px4} points to build an example. This process continues along the dataset until there aren't enough remaining points to produce a full \textit{input} and \textit{label} pairing.

The final result is stored in a Pandas Dataframe to be fed into the Neural Network.

\subsection{Network Architecture}
To best capture the time dependency of our data, we apply Long short-term memory (LSTM) neural networks to this task.

The network's architecture is as follows:
\begin{enumerate}
\item Input Layer (\textit{Px3} data points)
\item LSTM Layer  (120 memory gate neurons)
\item Dense Layer (100 neurons)
\item Output Layer (\textit{P} predictions)
\end{enumerate}

\subsection{Prediction \& Results}
Below are our results on a sine wave, triangle wave, and square wave.

\label{SineForecasting}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{assets/sine_forecast.png}
    \caption{\textbf{Figure \ref{SineForecasting}.} Forecasting the sine wave with a LSTM network.}
\end{figure}

\label{TriangleForecasting}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{assets/triangle_forecast.png}
    \caption{\textbf{Figure \ref{TriangleForecasting}.} Forecasting the Triangle wave with a LSTM network.}
\end{figure}

\label{SquareForecasting}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{assets/square_forecast_500epoch.png}
    \caption{\textbf{Figure \ref{SquareForecasting}.} Forecasting the Square wave with a LSTM network.}
\end{figure}

We quantify our prediction results by calculating the \textit{Root Mean Squared Error} (RMSE) and the \textit{Mean Absolute Error} (MAE) percentage for each wave forecast. To calculate the MAE \%, we divide the MAE over all test data points by the difference between the cycle's peak and trough. By making the error relative to cycle height to produce a percentage, our results are more interpretable. \textbf{Table \ref{PeriodicResults}} contains these results across all three periodic functions. Note that each model trains for \textit{1000 epochs} and uses a \textit{150 batch size}.

\label{PeriodicResults}
\begin{table}[h] % h for here, not the top of the document
  \centering
  \begin{tabular}{ c | c c }
      Function type & MAE \% & RMSE \\
      \hline
      \verb|Sine| & 0.269  & 0.006 \\
      \verb|Triangle| & 1.36 & 0.014\\
      \verb|Square| & 5.34 & 0.193
  \end{tabular}
  \caption{\textbf{Table \ref{PeriodicResults}.} Periodic function forecasting results.}
\end{table}


\section{Learning Non-periodic Behavior}

\subsection{Data Formatting}
We first modify the original data formatting phase to allow for a variable number of input and output points as cycle periods vary and require a variable number of points to capture.

To do so, we first perform a  preprocessing step to calculate the period of each cycle based on it's peak and trough locations. We then use the same sliding window method that collects four cycles worth of points for each example. It's important to note that Keras LSTM networks must have a fixed size input and output, and so we store each example in a larger fixed-length array (a \textit{container}). This length is preset and is always larger than an example worth of points, so as to not loose information. We represent \textit{Null} values with -10. To help ensure that the network doesn't confuse these values with real data, we shift all data points until the smallest value is 0.

\subsection{Network Architecture}

Each layer in our network is much larger. Even after sub-sampling one data point for every fifty, the input data is far more dense than that of the periodic waves.

\begin{enumerate}
\item Input Layer (\textit{Px3} data points)
\item LSTM Layer  (500 memory gate neurons)
\item Dense Layer (300 neurons)
\item Dense Layer (150 neurons)
\item Output Layer (\textit{P} predictions)
\end{enumerate}
\vspace{8pt}
\section{Classical approaches}
\subsection{Motivation}
Simple linear/logistic regression methods, tuned properly, can outperform more complex models. We attempt to develop such methods with the goal of visualizing and predicting trends in actuator behavior.

\subsection{Regression analysis}
We use the following canonical approach to generate models and evaluate their performance on derived feature prediction:
\begin{enumerate}
  \item Generate time-series cross-validation training-validation splits, delimiting data by each cycle.
  \item Perform grid-search on regression parameters to identify models which best predict derived features
\end{enumerate}

We then develop a regression model from these derived features:
\begin{enumerate}
  \item Split cycles into four piecewise curves, as enumerated in \ref{ssec:Piecewise}.
  \item Perform grid-search on regression parameters, minimizing loss (per piecewise curve) between predicted \verb|Load| and ground truth \verb|Load| as a function of \verb|Time|. 
\end{enumerate}

\subsection{Piecewise cycle splitting}\label{ssec:Piecewise}
We split curves into continuous piecewise sections using two simple methods: fixed boundaries and $K$-means clustering.

\subsubsection{Fixed boundaries}
We delimit curves at four points. Let $t_0 = 0$ denote the time at which a cycle begins; $t_{max} =$ the time at which a cycle reaches its maximum \verb|Load|; and $t_f$ denote the time at which a cycle finishes, such that $t_f =$ the \verb|Period| of a cycle. Then:

\begin{align*}
  t_1 &= \frac{t_{max} - t_0}{2} \\
  t_2 &= t_{max} \\
  t_3 &= \frac{t_f - t_{max}}{2}
\end{align*}

and we regress separately on the following subsections of a cycle's \verb|Period|. Let $c_i=$ the set of observations in the $i^{\text{th}}$ cycle of the dataset $X$. Then we define the following subsets of $c_i$:
\begin{align*}
  p_1 = c_i[t_0:t_1] \\
  p_2 = c_i[t_1:t_2] \\
  p_3 = c_i[t_2:t_3] \\
  p_4 = c_i[t_3:t_f]
\end{align*}

\subsubsection{$K$-means clustering}
We delimit curves based on the clusterings generated by $K$-means with $K=2$.  We run $K$-means, preserving no parameters, independently on the heating and cooling curves of each cycle. New (validation) points can be classified into cluster via the $1-$nearest-neighbors algorithm. $K$-means (as used here) computes the centroids of observations as

$$
m_i^{(t+1)} = \frac{1}{\vert S_i^{(t)} \vert} \sum_{x_j \in S_i^{(t)}} x_j
$$

where $S_i =$ the set of observations $x_j$ in cluster $i$. Since the load characteristic of the heating curve of each cycle is convex increasing, and we force $K=2$, we can identify the elbow of each heating curve in an unsupervised manner. Similar reasoning applies to the cooling curve of each cycle. Then observations are once again clustered into one of $p_1, p_2, p_3, p_4$.

\subsection{Piecewise regression performance}
We identify a model which reports optimal validation accuracy via the following algorithm, using the definitions of $t_i$ expressed in \ref{ssec:Piecewise}. $r_i$ denotes a regression function using ridge (Tikhonov) regularization.
\begin{codebox}
  \Procname{$\proc{TRAIN}(X,y)$}
  \li $P_1, \dots , P_4 \gets \{\}$
  \li $\Gamma \gets [3.40, 10.0, 3.40, 5.60]$ \hspace{3pt} \Comment regularization penalties
  \li $X \gets X \cup \, \bigcup_{i=2}^5 i^{\text{th}}\text{-order features of }X$
  \li \For $c \in X$ \hspace{60pt} \Comment $c \gets $ a discrete cycle in $X$ \Do
    \li \For $i \gets 1 \To 4$ \Do
      \li $p_i \gets c[t_{i-1}:t_i]$ \Comment $t_2 \gets t_{max}$ and $t_4 \gets t_f$
      \li $P_i \gets P_i \cup p_i$
    \End
  \End
  \li \For $i \gets 1 \To 4$ \Do
    \li fit $r_i$ on $(P_i, y[P_i])$, $\mathcal{L}_2$ parameter $\alpha_{r_i} \gets \Gamma_i$
  \End
\end{codebox}

Then new observations, with input features \verb|Time| and \verb|Cycle| (cycle index) only, map to predicted \verb|Load| as follows. Let $X=$ the matrix of \verb|Time| and \verb|Cycle| observations for a single cycle.

\begin{codebox}
  \Procname{$\proc{PRED}(X)$}
  \li \For $i \gets 1 \To 4$ \Do
    \li predict $y_i \gets r_i(X)$
  \End
  \li \Return $y_1 + \dots + y_4$
\end{codebox}

The performance of this algorithm is listed in \textbf{Table \ref{Performance}}, where the best possible score is $1.000$.

\begin{table}[h]
  \centering
  \begin{tabular}{ c | c c }
    \textbf{Feature} & \textbf{$R^2$ coefficient} \\
    \hline
    Min & $1.000$ \\
    Max & $1.000$ \\
    Cycle & $1.000$ \\
    Area & $0.953$ \\
    Heating & $0.934$ \\
    Cooling & $0.569$ \\
    HCprop & $0.733$ \\
    Period & $0.773$ \\
    Tail & $0.569$ \\
    Belly & $0.716$ \\
    Kurt & $0.854$ \\
    Skew & $0.479$
  \end{tabular}
  \label{Performance}
  \caption*{\textbf{Table \ref{Performance}.} Performance of piecewise regression algorithm.}
\end{table}

Since the minimum and maximum \verb|Load| of the experimental setup are constrained at roughly constant values, the \verb|Period| of each cycle is the dominant predictor of cycle \verb|Load| characteristic. Recursive feature elimination (RFE) confirms this. Regression on \verb|Load| values, given \verb|Time| and \verb|Cycle| index, is then fairly effective; see \textbf{Figure \ref{Redblue}}.

\label{Redblue}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{assets/piecewise_fit.png}
  \caption*{\textbf{Figure \ref{Redblue}.} Forecasting the $40^{\text{th}}$ cycle \texttt{Load} characteristic, with algorithm trained on $39$ previous cycles. (Predicted piecewise \texttt{Load} curves in \textit{red}; ground truth observations in \textit{blue}).}
\end{figure}

\subsection{Fourier analysis}
Fourier analysis on cycle \verb|Load| and \verb|Time| behavior may reveal oscillatory components underlying the complex actuator \verb|Load| characteristic. Applying a discrete Fourier transform (DFT) to raw cycle data results in a DFT magnitude response with some easily-identifiable important frequencies, as seen in \textbf{Figure \ref{Logfourier}}.

While some frequencies clearly have stronger DFT magnitude than others, Fourier analysis does not effectively capture the varying \verb|Period| phenomena that drives the \verb|Load| characteristic of the actuator. Extrapolating the top-$1000$ frequencies from DFT analysis enables the $48^{\text{th}}$-cycle prediction shown in \textbf{Figure \ref{Extrapolation}}.

\label{Logfourier}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{assets/fourier_analysis.png}
  \caption*{\textbf{Figure \ref{Logfourier}.} Discrete Fourier transform (log-magnitude) of raw cycle data.}
\end{figure}

\label{Extrapolation}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{assets/fourier_extrapolation.png}
  \caption*{\textbf{Figure \ref{Extrapolation}.} Extrapolation of top-$1000$ frequencies from DFT analysis. Ground truth in \textit{blue}; prediction in \textit{green}.}
\end{figure}

\subsection{Time-series analysis}

Conventional time-series methods, including autoregressive integrated moving average (ARIMA) models tuned to take into account non-linear trends and non-constant periodicity, were attempted. These models performed poorly, but parts of our methodology in using them are included here for the benefit of future work.

\textbf{Figure \ref{Autocorrelation}} depicts the autocorrelation behavior of the raw cycle signal, where \textit{Lag} measures delay in units of cycle index. We observe little correlation between delayed windows of cycles, varying inversely with lag.

\label{Autocorrelation}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.45\textwidth]{assets/autocorrelation.png}
  \caption*{\textbf{Figure \ref{Autocorrelation}.} (Serial) autocorrelation of cycles. Horizontal lines correspond to $95\%$ and $99\%$ confidence bands.}
\end{figure}



\section*{ACKNOWLEDGMENT}

We would like to acknowledge \href{http://aslanmiriyev.com/}{Aslan Miriyev (Ph.D.)} and \href{https://www.hodlipson.com/}{Professor Hod Lipson} for their vision and expert mentorship over the course of this work, and for the providing the opportunity to work with unique and powerful data and hardware. We also thank \href{http://ogchang.com/}{Oscar Chang}, \href{http://www.cs.columbia.edu/~bchen/}{Boyuan Chen}, and \href{http://www.cs.columbia.edu/~dechant/}{Chad DeChant} at the \href{https://www.creativemachineslab.com/}{Creative Machines Lab}, administered by Professor Lipson, for their patient and insightful guidance.

\begin{thebibliography}{99}

\bibitem{c1} G. O. Young, ÒSynthetic structure of industrial plastics (Book style with paper title and editor),Ó 	in Plastics, 2nd ed. vol. 3, J. Peters, Ed.  New York: McGraw-Hill, 1964, pp. 15Ð64.
\bibitem{c2} W.-K. Chen, Linear Networks and Systems (Book style).	Belmont, CA: Wadsworth, 1993, pp. 123Ð135.
\bibitem{c3} H. Poor, An Introduction to Signal Detection and Estimation.   New York: Springer-Verlag, 1985, ch. 4.
\bibitem{c4} B. Smith, ÒAn approach to graphs of linear forms (Unpublished work style),Ó unpublished.
\bibitem{c5} E. H. Miller, ÒA note on reflector arrays (Periodical styleÑAccepted for publication),Ó IEEE Trans. Antennas Propagat., to be publised.
\bibitem{c6} J. Wang, ÒFundamentals of erbium-doped fiber amplifiers arrays (Periodical styleÑSubmitted for publication),Ó IEEE J. Quantum Electron., submitted for publication.
\bibitem{c7} C. J. Kaufman, Rocky Mountain Research Lab., Boulder, CO, private communication, May 1995.
\bibitem{c8} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ÒElectron spectroscopy studies on magneto-optical media and plastic substrate interfaces(Translation Journals style),Ó IEEE Transl. J. Magn.Jpn., vol. 2, Aug. 1987, pp. 740Ð741 [Dig. 9th Annu. Conf. Magnetics Japan, 1982, p. 301].
\bibitem{c9} M. Young, The Techincal Writers Handbook.  Mill Valley, CA: University Science, 1989.
\bibitem{c10} J. U. Duncombe, ÒInfrared navigationÑPart I: An assessment of feasibility (Periodical style),Ó IEEE Trans. Electron Devices, vol. ED-11, pp. 34Ð39, Jan. 1959.
\bibitem{c11} S. Chen, B. Mulgrew, and P. M. Grant, ÒA clustering technique for digital communications channel equalization using radial basis function networks,Ó IEEE Trans. Neural Networks, vol. 4, pp. 570Ð578, July 1993.
\bibitem{c12} R. W. Lucky, ÒAutomatic equalization for digital communication,Ó Bell Syst. Tech. J., vol. 44, no. 4, pp. 547Ð588, Apr. 1965.
\bibitem{c13} S. P. Bingulac, ÒOn the compatibility of adaptive controllers (Published Conference Proceedings style),Ó in Proc. 4th Annu. Allerton Conf. Circuits and Systems Theory, New York, 1994, pp. 8Ð16.
\bibitem{c14} G. R. Faulhaber, ÒDesign of service systems with priority reservation,Ó in Conf. Rec. 1995 IEEE Int. Conf. Communications, pp. 3Ð8.
\bibitem{c15} W. D. Doyle, ÒMagnetization reversal in films with biaxial anisotropy,Ó in 1987 Proc. INTERMAG Conf., pp. 2.2-1Ð2.2-6.
\bibitem{c16} G. W. Juette and L. E. Zeffanella, ÒRadio noise currents n short sections on bundle conductors (Presented Conference Paper style),Ó presented at the IEEE Summer power Meeting, Dallas, TX, June 22Ð27, 1990, Paper 90 SM 690-0 PWRS.
\bibitem{c17} J. G. Kreifeldt, ÒAn analysis of surface-detected EMG as an amplitude-modulated noise,Ó presented at the 1989 Int. Conf. Medicine and Biological Engineering, Chicago, IL.
\bibitem{c18} J. Williams, ÒNarrow-band analyzer (Thesis or Dissertation style),Ó Ph.D. dissertation, Dept. Elect. Eng., Harvard Univ., Cambridge, MA, 1993. 
\bibitem{c19} N. Kawasaki, ÒParametric study of thermal and chemical nonequilibrium nozzle flow,Ó M.S. thesis, Dept. Electron. Eng., Osaka Univ., Osaka, Japan, 1993.
\bibitem{c20} J. P. Wilkinson, ÒNonlinear resonant circuit devices (Patent style),Ó U.S. Patent 3 624 12, July 16, 1990. 






\end{thebibliography}




\end{document}
