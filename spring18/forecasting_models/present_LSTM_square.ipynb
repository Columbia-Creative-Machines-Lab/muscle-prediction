{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/tensorflow/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from timeit import default_timer as timer\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy import signal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17132148769575131281\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11275639194\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 18294776726622805833\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
      "]\n",
      "----------------------------\n",
      "\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6256693373675497556\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 355336192\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 2669969478470983420\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "print(device_lib.list_local_devices())\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 4} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)\n",
    "print(\"----------------------------\")\n",
    "print\n",
    "print(device_lib.list_local_devices())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "* Record trials in an organized excel sheet, or write out each one to a .txt file \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Generate sine wave data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points: 1257\n"
     ]
    }
   ],
   "source": [
    "# #Define characteristics of the data to generate (square wave also has period 2PI)\n",
    "num_cycles = 20\n",
    "sample_rate = 0.1\n",
    "points_per_cycle = int((2*np.pi) / sample_rate)\n",
    "\n",
    "#Sample from a pure sine wave\n",
    "data = [signal.square(i) for i in np.arange(start=0, stop=num_cycles*2*np.pi, step=sample_rate)]\n",
    "\n",
    "print(\"Total data points: \" + str(len(data)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Formatting\n",
    "* `series_to_examples`: Function to convert a 1D series into example data consisting of `X` and `Y` columns\n",
    "* `prepare_data`: Function to prepare and split data into `train`/`test` sets\n",
    "\n",
    "* NOTE: We can also add scaling in these functions, to squash values closer together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 1D series into example data of X and Y columns\n",
    "def series_to_examples(series, n_prev=1, n_forecast=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Transform time series data into supervised learning dataset.\n",
    "    Arguments: \n",
    "        series: Sequence of observations as a list\n",
    "        n_prev: Number of lag observations as input (X)\n",
    "        n_forecast: Number of observations to predict (Y)\n",
    "        dropnan: Boolean to drop rows with NaN values resulting from shift\n",
    "    Returns:\n",
    "        Pandas Dataframe\n",
    "        \n",
    "    Note: by this logic we should have N = (n_prev + n_forecast) columns\n",
    "     and (series_len - N + 1) rows\n",
    "    \"\"\"\n",
    "    \n",
    "    #Robust check for whether or not series is univariate or multi-variate\n",
    "    #If not a simple list (univariate), then the columns of shape will tell you how many variables\n",
    "    num_dims = 1 if type(series) is list else series.shape[1]\n",
    "    \n",
    "    #convert the series to a dataframe format for shift\n",
    "    df = pd.DataFrame(series)\n",
    "    \n",
    "    #Cols is a list of lists where each inner list is a whole column\n",
    "    cols, names = list(), list()\n",
    "    \n",
    "    #Build each column of the input, first column will be fully shifted (i.e oldest example seen) \n",
    "    for i in range(n_prev, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        #names just a list??\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(num_dims)]\n",
    "    \n",
    "    #Build each column of the output forecasts (t, t+1, ...)\n",
    "    for i in range(0, n_forecast):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(num_dims)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(num_dims)]\n",
    "    \n",
    "    #Build a final dataframe columns side by side & attach names\n",
    "    final = pd.concat(cols, axis=1)\n",
    "    final.columns = names\n",
    "\n",
    "    \n",
    "    #Remove all rows with NaN values\n",
    "    if dropnan:\n",
    "        final.dropna(inplace=True)\n",
    "        \n",
    "    print(\"in series_to_examples\")\n",
    "    print(final.shape)\n",
    "        \n",
    "    return final  \n",
    "\n",
    "\n",
    "\n",
    "#Prepare and split data into train/test sets\n",
    "def prepare_data(series, n_test, n_prev, n_forecast):#, batch_size):\n",
    "    \n",
    "    #Prepare data handles intital split!!!\n",
    "    \n",
    "    #NEW: Split before sending into series_to_examples\n",
    "    train, test = series[:-n_test], series[-n_test:] \n",
    "    \n",
    "    \n",
    "    #TODO: Could scale here\n",
    "    \n",
    "    #Grab supervised-formatted data\n",
    "    df_train = series_to_examples(train, n_prev, n_forecast)\n",
    "    \n",
    "    #Drop excess examples to allow for varying batch size\n",
    "    #cutoff = int(len(df_train)/batch_size) * batch_size\n",
    "    #df_train = df_train[:cutoff]\n",
    "    \n",
    "    print(\"in prepare_data\")\n",
    "    print(df_train.shape)\n",
    "    #Extract and retain structure of values from dataframe\n",
    "    example_values_train = df_train.values\n",
    "    \n",
    "    return example_values_train, test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data structure to be passed to network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in series_to_examples\n",
      "(762, 248)\n",
      "in prepare_data\n",
      "(762, 248)\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "#Test size has to be large enough to encompass one full cycle\n",
    "\n",
    "n_prev = points_per_cycle * 3\n",
    "n_forecast = points_per_cycle\n",
    "\n",
    "\n",
    "#The test data has to encompass both n_prev points and n_forecast points\n",
    "#We feed the first n_prev points of the test into the network to make a prediction and compare this to n_forecast\n",
    "#(This is just enough to predict one cycle)\n",
    "n_test = n_prev + n_forecast\n",
    "\n",
    "\n",
    "train, test = prepare_data(data, n_test, n_prev, n_forecast)#, batch_size)\n",
    "\n",
    "print(n_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize total dataset and goal in red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fb4ac031f90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb4ac00e810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Set the figure parameters\n",
    "fig=plt.figure(figsize=(20, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "\n",
    "#Grab the points for each section of the plot\n",
    "train_points, test_input_points, test_label_points = data[:-n_test], data[-n_test:-n_forecast], data[-n_forecast:]\n",
    "\n",
    "#Plot the training data and the underlying curve in blue \n",
    "plt.plot(data, color=\"blue\")\n",
    "train_plot_x = [x for x in range(len(train_points))]\n",
    "plt.scatter(train_plot_x, train_points, color=\"blue\", s=30)\n",
    "\n",
    "#Plot the test example X in red\n",
    "test_input_xs = [x +len(train_points) for x in range(len(test_input_points))]\n",
    "plt.scatter(test_input_xs, test_input_points, color=\"red\", s=30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Plot the test target label in green\n",
    "test_label_xs = [x +(len(train_points)+len(test_input_points)) for x in range(len(test_label_points))]\n",
    "plt.scatter(test_label_xs, test_label_points, color=\"green\", s=30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #Plot the original data using the defined train and test sizes\n",
    "\n",
    "# #Line plot along the true data\n",
    "# plt.plot(data, color=\"blue\")\n",
    "\n",
    "# #Plot the training points in green\n",
    "# train_plot_x = [x for x in range(len(train_plot))]\n",
    "# plt.scatter(train_plot_x, train_plot, color=\"blue\", s=30)\n",
    "\n",
    "# #Note: we have to shift the test data to plot it\n",
    "# test_plot_x = [x +len(train_plot) for x in range(len(test_plot))]\n",
    "# plt.scatter(test_plot_x, test_plot, color=\"red\", s=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LSTM architecture\n",
    "\n",
    "* NOTE: We have to switch the input such that 2nd dimension (time_steps) is representative of how many steps to look into the past.  \n",
    "* NOTE: \"features\" is dim 1. It is just representative of the number of dimensions in our input, nothing to do with number of points, nor the number of points on the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit an LSTM network to training data\n",
    "# def mae_percent(y_true, y_pred):\n",
    "    \n",
    "#     cycle_height = max(y_true) - min(y_true)\n",
    "\n",
    "#     np.sum(abs(y_true-y_pred)\n",
    "    \n",
    "#     error_sum = 0\n",
    "#     for i in range(len(test_y)):\n",
    "#         error_sum += abs(test_y[i] - y_hat[i])\n",
    "    \n",
    "#     avg_error_sum = error_sum/len(test_y)\n",
    "#     return (avg_error_sum/cycle_height)*100\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def fit_lstm(train, n_prev, n_forecast, n_batch, nb_epoch, n_neurons):\n",
    "    \"\"\"\n",
    "    Function to convert data so it can be interpreted by the LSTM and then trains and returns the LSTM model\n",
    "    Note: \n",
    "    \n",
    "    Input to every LSTM layer must be 3 dimensional\n",
    "    - Samples: one sequence is one sample. A batch is comprised of 1 or more samples.\n",
    "    - Time Steps: One time step is one point of observation in the sample.\n",
    "    - Features: One feature is one observation at a time step.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    architecture = \"\"\n",
    "    # reshape training into [samples, timesteps, features]\n",
    "    \n",
    "    #Split the training data into X (input) and y (labels)\n",
    "    X, y = train[:, 0:n_prev], train[:, n_prev:]\n",
    "\n",
    "    print(\"X shape: \" + str(X.shape))\n",
    "    print(\"y shape: \" + str(y.shape))\n",
    "    \n",
    "    #Reshape X so Samples=num_examples, TimeSteps=1, Features = n_lag\n",
    "    #X = X.reshape(X.shape[0], 1, X.shape[1]) !!!!\n",
    "    X = X.reshape(X.shape[0],X.shape[1],1)\n",
    "    \n",
    "    #y = y.reshape(1, y.shape[0])\n",
    "    \n",
    "    #print(y.shape[1])\n",
    "    \n",
    "    model = Sequential()\n",
    "    #We feed in batch sizes of dimension: (n_batch, 1, n_lag)\n",
    "    print((n_batch, X.shape[1], X.shape[2]))\n",
    "    print(X.shape)\n",
    "    model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "    architecture += \"LSTM; \"\n",
    "    model.add(Dense(180))\n",
    "    #Last layer is a fully connected layer to output size n_forecast\n",
    "    model.add(Dense(y.shape[1]))\n",
    "    architecture += \"Dense output; \"\n",
    "    #Compile the network\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy']) #Hod wants mean squared error\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Fit the network to the training data\n",
    "    history = model.fit(X, y, validation_split=0.2, epochs=nb_epoch, batch_size=n_batch, verbose=1, shuffle=False)\n",
    "\n",
    "    \n",
    "#     for i in range(nb_epoch):\n",
    "#         print(i)\n",
    "#         start = timer()\n",
    "#         model.fit(X, y, epochs=1, batch_size=n_batch, verbose=0, shuffle=False)\n",
    "#         model.reset_states()\n",
    "#         end = timer()\n",
    "#         print(\"Epoch duration: \" + str(end - start))\n",
    "    return model, architecture, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(762, 248)\n",
      "(750, 248)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "train = train[12:]\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n",
      "(750, 248)\n",
      "X shape: (750, 186)\n",
      "y shape: (750, 62)\n",
      "(150, 186, 1)\n",
      "(750, 186, 1)\n",
      "Train on 600 samples, validate on 150 samples\n",
      "Epoch 1/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.9731 - acc: 0.0000e+00 - val_loss: 0.8944 - val_acc: 0.0000e+00\n",
      "Epoch 2/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.8182 - acc: 0.0000e+00 - val_loss: 0.6513 - val_acc: 0.0000e+00\n",
      "Epoch 3/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.5470 - acc: 0.0000e+00 - val_loss: 0.4345 - val_acc: 0.0000e+00\n",
      "Epoch 4/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.3674 - acc: 0.0000e+00 - val_loss: 0.2594 - val_acc: 0.0000e+00\n",
      "Epoch 5/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2454 - acc: 0.0000e+00 - val_loss: 0.2450 - val_acc: 0.0000e+00\n",
      "Epoch 6/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2368 - acc: 0.0000e+00 - val_loss: 0.2253 - val_acc: 0.0000e+00\n",
      "Epoch 7/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2167 - acc: 0.0000e+00 - val_loss: 0.2098 - val_acc: 0.0000e+00\n",
      "Epoch 8/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2103 - acc: 0.0000e+00 - val_loss: 0.2061 - val_acc: 0.0000e+00\n",
      "Epoch 9/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2015 - acc: 0.0000e+00 - val_loss: 0.1980 - val_acc: 0.0000e+00\n",
      "Epoch 10/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1955 - acc: 0.0033 - val_loss: 0.1941 - val_acc: 0.0667\n",
      "Epoch 11/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1921 - acc: 0.1000 - val_loss: 0.1912 - val_acc: 0.0800\n",
      "Epoch 12/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1870 - acc: 0.0883 - val_loss: 0.1851 - val_acc: 0.0667\n",
      "Epoch 13/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1834 - acc: 0.0600 - val_loss: 0.1821 - val_acc: 0.0267\n",
      "Epoch 14/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1798 - acc: 0.0300 - val_loss: 0.1774 - val_acc: 0.0400\n",
      "Epoch 15/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1755 - acc: 0.0650 - val_loss: 0.1731 - val_acc: 0.0667\n",
      "Epoch 16/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1708 - acc: 0.0883 - val_loss: 0.1677 - val_acc: 0.0800\n",
      "Epoch 17/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1651 - acc: 0.0883 - val_loss: 0.1615 - val_acc: 0.0667\n",
      "Epoch 18/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1585 - acc: 0.0567 - val_loss: 0.1544 - val_acc: 0.0400\n",
      "Epoch 19/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1507 - acc: 0.0350 - val_loss: 0.1460 - val_acc: 0.0267\n",
      "Epoch 20/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1415 - acc: 0.0333 - val_loss: 0.1332 - val_acc: 0.0267\n",
      "Epoch 21/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1298 - acc: 0.0200 - val_loss: 0.1240 - val_acc: 0.0000e+00\n",
      "Epoch 22/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1209 - acc: 0.0000e+00 - val_loss: 0.1149 - val_acc: 0.0000e+00\n",
      "Epoch 23/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1133 - acc: 0.0350 - val_loss: 0.1157 - val_acc: 0.0533\n",
      "Epoch 24/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1262 - acc: 0.0583 - val_loss: 0.1110 - val_acc: 0.0467\n",
      "Epoch 25/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1117 - acc: 0.0517 - val_loss: 0.1018 - val_acc: 0.0400\n",
      "Epoch 26/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1059 - acc: 0.0517 - val_loss: 0.1005 - val_acc: 0.0400\n",
      "Epoch 27/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1009 - acc: 0.0283 - val_loss: 0.0985 - val_acc: 0.0267\n",
      "Epoch 28/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0984 - acc: 0.0300 - val_loss: 0.0967 - val_acc: 0.0267\n",
      "Epoch 29/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0972 - acc: 0.0300 - val_loss: 0.0969 - val_acc: 0.0267\n",
      "Epoch 30/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0964 - acc: 0.0400 - val_loss: 0.0959 - val_acc: 0.0400\n",
      "Epoch 31/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0959 - acc: 0.0367 - val_loss: 0.0947 - val_acc: 0.0533\n",
      "Epoch 32/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0950 - acc: 0.0383 - val_loss: 0.0925 - val_acc: 0.0267\n",
      "Epoch 33/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0933 - acc: 0.0300 - val_loss: 0.0906 - val_acc: 0.0267\n",
      "Epoch 34/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0913 - acc: 0.0283 - val_loss: 0.0894 - val_acc: 0.0267\n",
      "Epoch 35/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0896 - acc: 0.0300 - val_loss: 0.0880 - val_acc: 0.0267\n",
      "Epoch 36/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0883 - acc: 0.0300 - val_loss: 0.0862 - val_acc: 0.0267\n",
      "Epoch 37/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0865 - acc: 0.0300 - val_loss: 0.0843 - val_acc: 0.0267\n",
      "Epoch 38/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0847 - acc: 0.0300 - val_loss: 0.0823 - val_acc: 0.0267\n",
      "Epoch 39/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0831 - acc: 0.0300 - val_loss: 0.0808 - val_acc: 0.0267\n",
      "Epoch 40/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0823 - acc: 0.0333 - val_loss: 0.0792 - val_acc: 0.0267\n",
      "Epoch 41/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0824 - acc: 0.0400 - val_loss: 0.0802 - val_acc: 0.0267\n",
      "Epoch 42/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0806 - acc: 0.0433 - val_loss: 0.0744 - val_acc: 0.0400\n",
      "Epoch 43/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0743 - acc: 0.0450 - val_loss: 0.0725 - val_acc: 0.0267\n",
      "Epoch 44/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0775 - acc: 0.0400 - val_loss: 0.0825 - val_acc: 0.0133\n",
      "Epoch 45/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0848 - acc: 0.0183 - val_loss: 0.0747 - val_acc: 0.0133\n",
      "Epoch 46/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0745 - acc: 0.0050 - val_loss: 0.0723 - val_acc: 0.0000e+00\n",
      "Epoch 47/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0740 - acc: 0.0000e+00 - val_loss: 0.0678 - val_acc: 0.0000e+00\n",
      "Epoch 48/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0697 - acc: 0.0000e+00 - val_loss: 0.0683 - val_acc: 0.0000e+00\n",
      "Epoch 49/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0701 - acc: 0.0133 - val_loss: 0.0683 - val_acc: 0.0000e+00\n",
      "Epoch 50/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0703 - acc: 0.0117 - val_loss: 0.0683 - val_acc: 0.0133\n",
      "Epoch 51/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0667 - acc: 0.0117 - val_loss: 0.0633 - val_acc: 0.0133\n",
      "Epoch 52/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0658 - acc: 0.0167 - val_loss: 0.0638 - val_acc: 0.0133\n",
      "Epoch 53/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0641 - acc: 0.0100 - val_loss: 0.0632 - val_acc: 0.0133\n",
      "Epoch 54/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0636 - acc: 0.0167 - val_loss: 0.0620 - val_acc: 0.0133\n",
      "Epoch 55/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0653 - acc: 0.0167 - val_loss: 0.0615 - val_acc: 0.0133\n",
      "Epoch 56/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0617 - acc: 0.0167 - val_loss: 0.0606 - val_acc: 0.0133\n",
      "Epoch 57/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0627 - acc: 0.0200 - val_loss: 0.0648 - val_acc: 0.0267\n",
      "Epoch 58/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0659 - acc: 0.0267 - val_loss: 0.0606 - val_acc: 0.0133\n",
      "Epoch 59/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0607 - acc: 0.0267 - val_loss: 0.0581 - val_acc: 0.0133\n",
      "Epoch 60/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0641 - acc: 0.0200 - val_loss: 0.0608 - val_acc: 0.0267\n",
      "Epoch 61/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0600 - acc: 0.0233 - val_loss: 0.0606 - val_acc: 0.0267\n",
      "Epoch 62/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0602 - acc: 0.0250 - val_loss: 0.0608 - val_acc: 0.0133\n",
      "Epoch 63/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0622 - acc: 0.0233 - val_loss: 0.0597 - val_acc: 0.0133\n",
      "Epoch 64/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0581 - acc: 0.0233 - val_loss: 0.0552 - val_acc: 0.0267\n",
      "Epoch 65/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0602 - acc: 0.0267 - val_loss: 0.0560 - val_acc: 0.0267\n",
      "Epoch 66/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0560 - acc: 0.0283 - val_loss: 0.0565 - val_acc: 0.0133\n",
      "Epoch 67/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0563 - acc: 0.0317 - val_loss: 0.0564 - val_acc: 0.0133\n",
      "Epoch 68/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0617 - acc: 0.0317 - val_loss: 0.0553 - val_acc: 0.0467\n",
      "Epoch 69/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0562 - acc: 0.0367 - val_loss: 0.0519 - val_acc: 0.0133\n",
      "Epoch 70/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0586 - acc: 0.0300 - val_loss: 0.0529 - val_acc: 0.0467\n",
      "Epoch 71/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0537 - acc: 0.0450 - val_loss: 0.0532 - val_acc: 0.0467\n",
      "Epoch 72/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0533 - acc: 0.0333 - val_loss: 0.0506 - val_acc: 0.0267\n",
      "Epoch 73/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0531 - acc: 0.0333 - val_loss: 0.0520 - val_acc: 0.0467\n",
      "Epoch 74/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0512 - acc: 0.0400 - val_loss: 0.0493 - val_acc: 0.0467\n",
      "Epoch 75/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0535 - acc: 0.0283 - val_loss: 0.0545 - val_acc: 0.0267\n",
      "Epoch 76/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0523 - acc: 0.0450 - val_loss: 0.0519 - val_acc: 0.0133\n",
      "Epoch 77/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0509 - acc: 0.0283 - val_loss: 0.0493 - val_acc: 0.0133\n",
      "Epoch 78/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0540 - acc: 0.0317 - val_loss: 0.0499 - val_acc: 0.0400\n",
      "Epoch 79/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0495 - acc: 0.0233 - val_loss: 0.0496 - val_acc: 0.0467\n",
      "Epoch 80/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0505 - acc: 0.0317 - val_loss: 0.0526 - val_acc: 0.0267\n",
      "Epoch 81/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0541 - acc: 0.0367 - val_loss: 0.0503 - val_acc: 0.0333\n",
      "Epoch 82/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0518 - acc: 0.0267 - val_loss: 0.0461 - val_acc: 0.0133\n",
      "Epoch 83/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0531 - acc: 0.0233 - val_loss: 0.0484 - val_acc: 0.0267\n",
      "Epoch 84/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0490 - acc: 0.0367 - val_loss: 0.0490 - val_acc: 0.0133\n",
      "Epoch 85/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0488 - acc: 0.0250 - val_loss: 0.0475 - val_acc: 0.0267\n",
      "Epoch 86/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0488 - acc: 0.0217 - val_loss: 0.0480 - val_acc: 0.0333\n",
      "Epoch 87/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0474 - acc: 0.0233 - val_loss: 0.0447 - val_acc: 0.0333\n",
      "Epoch 88/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0482 - acc: 0.0133 - val_loss: 0.0512 - val_acc: 0.0267\n",
      "Epoch 89/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0475 - acc: 0.0250 - val_loss: 0.0492 - val_acc: 0.0133\n",
      "Epoch 90/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0485 - acc: 0.0100 - val_loss: 0.0462 - val_acc: 0.0000e+00\n",
      "Epoch 91/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0485 - acc: 0.0100 - val_loss: 0.0451 - val_acc: 0.0133\n",
      "Epoch 92/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0456 - acc: 0.0200 - val_loss: 0.0443 - val_acc: 0.0133\n",
      "Epoch 93/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0458 - acc: 0.0083 - val_loss: 0.0472 - val_acc: 0.0133\n",
      "Epoch 94/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0458 - acc: 0.0150 - val_loss: 0.0454 - val_acc: 0.0000e+00\n",
      "Epoch 95/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0461 - acc: 0.0067 - val_loss: 0.0422 - val_acc: 0.0000e+00\n",
      "Epoch 96/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0464 - acc: 0.0033 - val_loss: 0.0491 - val_acc: 0.0267\n",
      "Epoch 97/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0456 - acc: 0.0283 - val_loss: 0.0480 - val_acc: 0.0000e+00\n",
      "Epoch 98/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0464 - acc: 0.0067 - val_loss: 0.0471 - val_acc: 0.0133\n",
      "Epoch 99/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0462 - acc: 0.0033 - val_loss: 0.0438 - val_acc: 0.0133\n",
      "Epoch 100/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0443 - acc: 0.0150 - val_loss: 0.0416 - val_acc: 0.0000e+00\n",
      "Epoch 101/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0442 - acc: 0.0033 - val_loss: 0.0484 - val_acc: 0.0267\n",
      "Epoch 102/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0438 - acc: 0.0167 - val_loss: 0.0446 - val_acc: 0.0133\n",
      "Epoch 103/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0462 - acc: 0.0150 - val_loss: 0.0412 - val_acc: 0.0000e+00\n",
      "Epoch 104/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0443 - acc: 0.0117 - val_loss: 0.0437 - val_acc: 0.0133\n",
      "Epoch 105/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0436 - acc: 0.0200 - val_loss: 0.0426 - val_acc: 0.0133\n",
      "Epoch 106/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0429 - acc: 0.0083 - val_loss: 0.0444 - val_acc: 0.0133\n",
      "Epoch 107/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0423 - acc: 0.0150 - val_loss: 0.0421 - val_acc: 0.0133\n",
      "Epoch 108/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0432 - acc: 0.0167 - val_loss: 0.0399 - val_acc: 0.0133\n",
      "Epoch 109/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0419 - acc: 0.0100 - val_loss: 0.0477 - val_acc: 0.0133\n",
      "Epoch 110/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0417 - acc: 0.0167 - val_loss: 0.0435 - val_acc: 0.0333\n",
      "Epoch 111/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0461 - acc: 0.0200 - val_loss: 0.0396 - val_acc: 0.0133\n",
      "Epoch 112/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0410 - acc: 0.0150 - val_loss: 0.0433 - val_acc: 0.0133\n",
      "Epoch 113/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0420 - acc: 0.0233 - val_loss: 0.0426 - val_acc: 0.0133\n",
      "Epoch 114/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0425 - acc: 0.0200 - val_loss: 0.0448 - val_acc: 0.0133\n",
      "Epoch 115/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0407 - acc: 0.0183 - val_loss: 0.0405 - val_acc: 0.0133\n",
      "Epoch 116/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0410 - acc: 0.0267 - val_loss: 0.0382 - val_acc: 0.0133\n",
      "Epoch 117/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0396 - acc: 0.0167 - val_loss: 0.0432 - val_acc: 0.0133\n",
      "Epoch 118/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0399 - acc: 0.0233 - val_loss: 0.0403 - val_acc: 0.0333\n",
      "Epoch 119/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0435 - acc: 0.0200 - val_loss: 0.0387 - val_acc: 0.0133\n",
      "Epoch 120/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0384 - acc: 0.0200 - val_loss: 0.0398 - val_acc: 0.0133\n",
      "Epoch 121/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0407 - acc: 0.0283 - val_loss: 0.0393 - val_acc: 0.0333\n",
      "Epoch 122/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0411 - acc: 0.0250 - val_loss: 0.0406 - val_acc: 0.0133\n",
      "Epoch 123/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0382 - acc: 0.0267 - val_loss: 0.0383 - val_acc: 0.0133\n",
      "Epoch 124/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0387 - acc: 0.0317 - val_loss: 0.0381 - val_acc: 0.0133\n",
      "Epoch 125/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0377 - acc: 0.0283 - val_loss: 0.0387 - val_acc: 0.0133\n",
      "Epoch 126/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0388 - acc: 0.0283 - val_loss: 0.0380 - val_acc: 0.0333\n",
      "Epoch 127/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0426 - acc: 0.0267 - val_loss: 0.0368 - val_acc: 0.0133\n",
      "Epoch 128/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0376 - acc: 0.0250 - val_loss: 0.0391 - val_acc: 0.0133\n",
      "Epoch 129/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0380 - acc: 0.0283 - val_loss: 0.0371 - val_acc: 0.0333\n",
      "Epoch 130/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0413 - acc: 0.0250 - val_loss: 0.0369 - val_acc: 0.0133\n",
      "Epoch 131/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0370 - acc: 0.0283 - val_loss: 0.0392 - val_acc: 0.0267\n",
      "Epoch 132/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0382 - acc: 0.0350 - val_loss: 0.0371 - val_acc: 0.0333\n",
      "Epoch 133/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0387 - acc: 0.0350 - val_loss: 0.0383 - val_acc: 0.0133\n",
      "Epoch 134/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0376 - acc: 0.0300 - val_loss: 0.0369 - val_acc: 0.0333\n",
      "Epoch 135/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0517 - acc: 0.0350 - val_loss: 0.0431 - val_acc: 0.0133\n",
      "Epoch 136/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0394 - acc: 0.0283 - val_loss: 0.0370 - val_acc: 0.0333\n",
      "Epoch 137/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0379 - acc: 0.0350 - val_loss: 0.0366 - val_acc: 0.0333\n",
      "Epoch 138/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0397 - acc: 0.0317 - val_loss: 0.0433 - val_acc: 0.0267\n",
      "Epoch 139/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0382 - acc: 0.0333 - val_loss: 0.0377 - val_acc: 0.0333\n",
      "Epoch 140/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0368 - acc: 0.0367 - val_loss: 0.0374 - val_acc: 0.0467\n",
      "Epoch 141/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0387 - acc: 0.0300 - val_loss: 0.0451 - val_acc: 0.0133\n",
      "Epoch 142/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0398 - acc: 0.0317 - val_loss: 0.0380 - val_acc: 0.0267\n",
      "Epoch 143/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0456 - acc: 0.0283 - val_loss: 0.0518 - val_acc: 0.0133\n",
      "Epoch 144/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0426 - acc: 0.0300 - val_loss: 0.0390 - val_acc: 0.0333\n",
      "Epoch 145/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0376 - acc: 0.0317 - val_loss: 0.0365 - val_acc: 0.0333\n",
      "Epoch 146/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0373 - acc: 0.0350 - val_loss: 0.0358 - val_acc: 0.0333\n",
      "Epoch 147/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0376 - acc: 0.0300 - val_loss: 0.0353 - val_acc: 0.0333\n",
      "Epoch 148/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0386 - acc: 0.0333 - val_loss: 0.0353 - val_acc: 0.0467\n",
      "Epoch 149/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0377 - acc: 0.0317 - val_loss: 0.0377 - val_acc: 0.0467\n",
      "Epoch 150/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0368 - acc: 0.0333 - val_loss: 0.0378 - val_acc: 0.0467\n",
      "Epoch 151/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0356 - acc: 0.0333 - val_loss: 0.0367 - val_acc: 0.0467\n",
      "Epoch 152/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0352 - acc: 0.0333 - val_loss: 0.0375 - val_acc: 0.0333\n",
      "Epoch 153/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0370 - acc: 0.0267 - val_loss: 0.0387 - val_acc: 0.0133\n",
      "Epoch 154/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0407 - acc: 0.0250 - val_loss: 0.0395 - val_acc: 0.0133\n",
      "Epoch 155/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0348 - acc: 0.0283 - val_loss: 0.0366 - val_acc: 0.0267\n",
      "Epoch 156/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0350 - acc: 0.0333 - val_loss: 0.0378 - val_acc: 0.0333\n",
      "Epoch 157/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0356 - acc: 0.0333 - val_loss: 0.0385 - val_acc: 0.0133\n",
      "Epoch 158/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0365 - acc: 0.0283 - val_loss: 0.0387 - val_acc: 0.0133\n",
      "Epoch 159/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0395 - acc: 0.0250 - val_loss: 0.0433 - val_acc: 0.0133\n",
      "Epoch 160/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0386 - acc: 0.0250 - val_loss: 0.0412 - val_acc: 0.0133\n",
      "Epoch 161/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0448 - acc: 0.0283 - val_loss: 0.0590 - val_acc: 0.0200\n",
      "Epoch 162/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0404 - acc: 0.0300 - val_loss: 0.0501 - val_acc: 0.0133\n",
      "Epoch 163/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0536 - acc: 0.0167 - val_loss: 0.0601 - val_acc: 0.0133\n",
      "Epoch 164/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0499 - acc: 0.0150 - val_loss: 0.0515 - val_acc: 0.0333\n",
      "Epoch 165/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0448 - acc: 0.0167 - val_loss: 0.0345 - val_acc: 0.0333\n",
      "Epoch 166/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0405 - acc: 0.0217 - val_loss: 0.0382 - val_acc: 0.0200\n",
      "Epoch 167/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0404 - acc: 0.0167 - val_loss: 0.0456 - val_acc: 0.0400\n",
      "Epoch 168/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0430 - acc: 0.0333 - val_loss: 0.0502 - val_acc: 0.0200\n",
      "Epoch 169/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0440 - acc: 0.0333 - val_loss: 0.0421 - val_acc: 0.0200\n",
      "Epoch 170/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0424 - acc: 0.0317 - val_loss: 0.0360 - val_acc: 0.0333\n",
      "Epoch 171/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0376 - acc: 0.0267 - val_loss: 0.0372 - val_acc: 0.0133\n",
      "Epoch 172/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0367 - acc: 0.0317 - val_loss: 0.0346 - val_acc: 0.0333\n",
      "Epoch 173/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0351 - acc: 0.0317 - val_loss: 0.0345 - val_acc: 0.0333\n",
      "Epoch 174/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0355 - acc: 0.0350 - val_loss: 0.0353 - val_acc: 0.0333\n",
      "Epoch 175/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0358 - acc: 0.0367 - val_loss: 0.0338 - val_acc: 0.0333\n",
      "Epoch 176/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0353 - acc: 0.0317 - val_loss: 0.0336 - val_acc: 0.0333\n",
      "Epoch 177/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0343 - acc: 0.0317 - val_loss: 0.0331 - val_acc: 0.0333\n",
      "Epoch 178/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0338 - acc: 0.0317 - val_loss: 0.0328 - val_acc: 0.0333\n",
      "Epoch 179/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0334 - acc: 0.0317 - val_loss: 0.0332 - val_acc: 0.0333\n",
      "Epoch 180/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0336 - acc: 0.0317 - val_loss: 0.0343 - val_acc: 0.0333\n",
      "Epoch 181/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0342 - acc: 0.0283 - val_loss: 0.0365 - val_acc: 0.0333\n",
      "Epoch 182/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0357 - acc: 0.0250 - val_loss: 0.0400 - val_acc: 0.0333\n",
      "Epoch 183/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0377 - acc: 0.0200 - val_loss: 0.0444 - val_acc: 0.0200\n",
      "Epoch 184/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0398 - acc: 0.0183 - val_loss: 0.0465 - val_acc: 0.0200\n",
      "Epoch 185/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0411 - acc: 0.0200 - val_loss: 0.0418 - val_acc: 0.0200\n",
      "Epoch 186/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0404 - acc: 0.0183 - val_loss: 0.0340 - val_acc: 0.0333\n",
      "Epoch 187/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0373 - acc: 0.0200 - val_loss: 0.0343 - val_acc: 0.0000e+00\n",
      "Epoch 188/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0350 - acc: 0.0200 - val_loss: 0.0344 - val_acc: 0.0133\n",
      "Epoch 189/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0339 - acc: 0.0217 - val_loss: 0.0319 - val_acc: 0.0200\n",
      "Epoch 190/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0332 - acc: 0.0183 - val_loss: 0.0332 - val_acc: 0.0333\n",
      "Epoch 191/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0338 - acc: 0.0283 - val_loss: 0.0335 - val_acc: 0.0333\n",
      "Epoch 192/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0342 - acc: 0.0267 - val_loss: 0.0324 - val_acc: 0.0333\n",
      "Epoch 193/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0337 - acc: 0.0217 - val_loss: 0.0319 - val_acc: 0.0333\n",
      "Epoch 194/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0330 - acc: 0.0200 - val_loss: 0.0317 - val_acc: 0.0133\n",
      "Epoch 195/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0325 - acc: 0.0200 - val_loss: 0.0315 - val_acc: 0.0133\n",
      "Epoch 196/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0323 - acc: 0.0200 - val_loss: 0.0318 - val_acc: 0.0333\n",
      "Epoch 197/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0323 - acc: 0.0167 - val_loss: 0.0326 - val_acc: 0.0333\n",
      "Epoch 198/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0327 - acc: 0.0117 - val_loss: 0.0341 - val_acc: 0.0200\n",
      "Epoch 199/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0335 - acc: 0.0100 - val_loss: 0.0366 - val_acc: 0.0000e+00\n",
      "Epoch 200/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0349 - acc: 0.0067 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 201/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0364 - acc: 0.0083 - val_loss: 0.0428 - val_acc: 0.0000e+00\n",
      "Epoch 202/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0379 - acc: 0.0150 - val_loss: 0.0441 - val_acc: 0.0000e+00\n",
      "Epoch 203/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0390 - acc: 0.0100 - val_loss: 0.0412 - val_acc: 0.0000e+00\n",
      "Epoch 204/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0390 - acc: 0.0100 - val_loss: 0.0345 - val_acc: 0.0000e+00\n",
      "Epoch 205/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0368 - acc: 0.0100 - val_loss: 0.0317 - val_acc: 0.0000e+00\n",
      "Epoch 206/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0342 - acc: 0.0083 - val_loss: 0.0330 - val_acc: 0.0000e+00\n",
      "Epoch 207/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0327 - acc: 0.0033 - val_loss: 0.0314 - val_acc: 0.0000e+00\n",
      "Epoch 208/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0318 - acc: 0.0100 - val_loss: 0.0307 - val_acc: 0.0000e+00\n",
      "Epoch 209/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0316 - acc: 0.0083 - val_loss: 0.0318 - val_acc: 0.0000e+00\n",
      "Epoch 210/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0323 - acc: 0.0183 - val_loss: 0.0321 - val_acc: 0.0000e+00\n",
      "Epoch 211/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0327 - acc: 0.0117 - val_loss: 0.0316 - val_acc: 0.0133\n",
      "Epoch 212/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0326 - acc: 0.0133 - val_loss: 0.0310 - val_acc: 0.0133\n",
      "Epoch 213/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0322 - acc: 0.0117 - val_loss: 0.0308 - val_acc: 0.0133\n",
      "Epoch 214/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0318 - acc: 0.0083 - val_loss: 0.0307 - val_acc: 0.0133\n",
      "Epoch 215/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0316 - acc: 0.0100 - val_loss: 0.0309 - val_acc: 0.0133\n",
      "Epoch 216/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0315 - acc: 0.0117 - val_loss: 0.0315 - val_acc: 0.0133\n",
      "Epoch 217/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0316 - acc: 0.0150 - val_loss: 0.0324 - val_acc: 0.0000e+00\n",
      "Epoch 218/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0321 - acc: 0.0117 - val_loss: 0.0339 - val_acc: 0.0000e+00\n",
      "Epoch 219/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0327 - acc: 0.0133 - val_loss: 0.0358 - val_acc: 0.0000e+00\n",
      "Epoch 220/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0336 - acc: 0.0117 - val_loss: 0.0380 - val_acc: 0.0000e+00\n",
      "Epoch 221/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0347 - acc: 0.0117 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
      "Epoch 222/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0358 - acc: 0.0117 - val_loss: 0.0415 - val_acc: 0.0000e+00\n",
      "Epoch 223/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0368 - acc: 0.0100 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 224/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0372 - acc: 0.0050 - val_loss: 0.0355 - val_acc: 0.0000e+00\n",
      "Epoch 225/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0361 - acc: 0.0067 - val_loss: 0.0309 - val_acc: 0.0133\n",
      "Epoch 226/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0339 - acc: 0.0117 - val_loss: 0.0306 - val_acc: 0.0000e+00\n",
      "Epoch 227/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0319 - acc: 0.0067 - val_loss: 0.0308 - val_acc: 0.0000e+00\n",
      "Epoch 228/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0308 - acc: 0.0117 - val_loss: 0.0296 - val_acc: 0.0000e+00\n",
      "Epoch 229/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0302 - acc: 0.0133 - val_loss: 0.0296 - val_acc: 0.0133\n",
      "Epoch 230/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0303 - acc: 0.0183 - val_loss: 0.0306 - val_acc: 0.0133\n",
      "Epoch 231/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0309 - acc: 0.0183 - val_loss: 0.0308 - val_acc: 0.0133\n",
      "Epoch 232/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0313 - acc: 0.0150 - val_loss: 0.0307 - val_acc: 0.0133\n",
      "Epoch 233/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0313 - acc: 0.0117 - val_loss: 0.0304 - val_acc: 0.0133\n",
      "Epoch 234/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0312 - acc: 0.0117 - val_loss: 0.0303 - val_acc: 0.0133\n",
      "Epoch 235/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0310 - acc: 0.0117 - val_loss: 0.0304 - val_acc: 0.0133\n",
      "Epoch 236/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0309 - acc: 0.0100 - val_loss: 0.0307 - val_acc: 0.0133\n",
      "Epoch 237/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0309 - acc: 0.0117 - val_loss: 0.0313 - val_acc: 0.0133\n",
      "Epoch 238/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0311 - acc: 0.0117 - val_loss: 0.0322 - val_acc: 0.0000e+00\n",
      "Epoch 239/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0314 - acc: 0.0117 - val_loss: 0.0333 - val_acc: 0.0000e+00\n",
      "Epoch 240/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0319 - acc: 0.0117 - val_loss: 0.0347 - val_acc: 0.0000e+00\n",
      "Epoch 241/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0325 - acc: 0.0117 - val_loss: 0.0362 - val_acc: 0.0000e+00\n",
      "Epoch 242/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0333 - acc: 0.0100 - val_loss: 0.0374 - val_acc: 0.0000e+00\n",
      "Epoch 243/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0340 - acc: 0.0100 - val_loss: 0.0375 - val_acc: 0.0000e+00\n",
      "Epoch 244/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0344 - acc: 0.0100 - val_loss: 0.0358 - val_acc: 0.0000e+00\n",
      "Epoch 245/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0344 - acc: 0.0067 - val_loss: 0.0323 - val_acc: 0.0000e+00\n",
      "Epoch 246/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0336 - acc: 0.0100 - val_loss: 0.0296 - val_acc: 0.0133\n",
      "Epoch 247/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0322 - acc: 0.0117 - val_loss: 0.0291 - val_acc: 0.0133\n",
      "Epoch 248/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0306 - acc: 0.0117 - val_loss: 0.0292 - val_acc: 0.0000e+00\n",
      "Epoch 249/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0296 - acc: 0.0167 - val_loss: 0.0286 - val_acc: 0.0000e+00\n",
      "Epoch 250/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0290 - acc: 0.0167 - val_loss: 0.0284 - val_acc: 0.0133\n",
      "Epoch 251/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0291 - acc: 0.0217 - val_loss: 0.0291 - val_acc: 0.0133\n",
      "Epoch 252/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0295 - acc: 0.0183 - val_loss: 0.0299 - val_acc: 0.0133\n",
      "Epoch 253/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0301 - acc: 0.0117 - val_loss: 0.0307 - val_acc: 0.0133\n",
      "Epoch 254/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0305 - acc: 0.0117 - val_loss: 0.0314 - val_acc: 0.0133\n",
      "Epoch 255/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0309 - acc: 0.0117 - val_loss: 0.0321 - val_acc: 0.0000e+00\n",
      "Epoch 256/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0312 - acc: 0.0100 - val_loss: 0.0328 - val_acc: 0.0000e+00\n",
      "Epoch 257/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0315 - acc: 0.0100 - val_loss: 0.0332 - val_acc: 0.0000e+00\n",
      "Epoch 258/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0318 - acc: 0.0100 - val_loss: 0.0334 - val_acc: 0.0000e+00\n",
      "Epoch 259/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0319 - acc: 0.0100 - val_loss: 0.0331 - val_acc: 0.0000e+00\n",
      "Epoch 260/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0320 - acc: 0.0083 - val_loss: 0.0322 - val_acc: 0.0000e+00\n",
      "Epoch 261/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0319 - acc: 0.0067 - val_loss: 0.0309 - val_acc: 0.0133\n",
      "Epoch 262/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0316 - acc: 0.0083 - val_loss: 0.0295 - val_acc: 0.0133\n",
      "Epoch 263/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0309 - acc: 0.0117 - val_loss: 0.0286 - val_acc: 0.0133\n",
      "Epoch 264/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0301 - acc: 0.0117 - val_loss: 0.0281 - val_acc: 0.0133\n",
      "Epoch 265/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0293 - acc: 0.0150 - val_loss: 0.0279 - val_acc: 0.0133\n",
      "Epoch 266/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0287 - acc: 0.0183 - val_loss: 0.0278 - val_acc: 0.0133\n",
      "Epoch 267/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0284 - acc: 0.0183 - val_loss: 0.0280 - val_acc: 0.0133\n",
      "Epoch 268/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0285 - acc: 0.0167 - val_loss: 0.0286 - val_acc: 0.0133\n",
      "Epoch 269/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0288 - acc: 0.0150 - val_loss: 0.0298 - val_acc: 0.0133\n",
      "Epoch 270/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0295 - acc: 0.0150 - val_loss: 0.0316 - val_acc: 0.0133\n",
      "Epoch 271/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0304 - acc: 0.0150 - val_loss: 0.0340 - val_acc: 0.0000e+00\n",
      "Epoch 272/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0317 - acc: 0.0183 - val_loss: 0.0368 - val_acc: 0.0000e+00\n",
      "Epoch 273/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0331 - acc: 0.0217 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 274/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0345 - acc: 0.0200 - val_loss: 0.0407 - val_acc: 0.0000e+00\n",
      "Epoch 275/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0354 - acc: 0.0200 - val_loss: 0.0376 - val_acc: 0.0000e+00\n",
      "Epoch 276/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0352 - acc: 0.0067 - val_loss: 0.0312 - val_acc: 0.0133\n",
      "Epoch 277/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0334 - acc: 0.0083 - val_loss: 0.0282 - val_acc: 0.0133\n",
      "Epoch 278/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0308 - acc: 0.0117 - val_loss: 0.0294 - val_acc: 0.0000e+00\n",
      "Epoch 279/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0292 - acc: 0.0167 - val_loss: 0.0281 - val_acc: 0.0000e+00\n",
      "Epoch 280/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0283 - acc: 0.0167 - val_loss: 0.0274 - val_acc: 0.0133\n",
      "Epoch 281/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0282 - acc: 0.0217 - val_loss: 0.0286 - val_acc: 0.0133\n",
      "Epoch 282/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0290 - acc: 0.0150 - val_loss: 0.0286 - val_acc: 0.0133\n",
      "Epoch 283/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0292 - acc: 0.0150 - val_loss: 0.0279 - val_acc: 0.0133\n",
      "Epoch 284/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0289 - acc: 0.0167 - val_loss: 0.0275 - val_acc: 0.0133\n",
      "Epoch 285/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0284 - acc: 0.0183 - val_loss: 0.0273 - val_acc: 0.0133\n",
      "Epoch 286/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0281 - acc: 0.0183 - val_loss: 0.0273 - val_acc: 0.0133\n",
      "Epoch 287/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0279 - acc: 0.0183 - val_loss: 0.0276 - val_acc: 0.0133\n",
      "Epoch 288/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0280 - acc: 0.0133 - val_loss: 0.0283 - val_acc: 0.0133\n",
      "Epoch 289/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0284 - acc: 0.0117 - val_loss: 0.0293 - val_acc: 0.0133\n",
      "Epoch 290/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0289 - acc: 0.0117 - val_loss: 0.0308 - val_acc: 0.0133\n",
      "Epoch 291/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0296 - acc: 0.0150 - val_loss: 0.0327 - val_acc: 0.0000e+00\n",
      "Epoch 292/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0306 - acc: 0.0183 - val_loss: 0.0347 - val_acc: 0.0000e+00\n",
      "Epoch 293/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0316 - acc: 0.0200 - val_loss: 0.0365 - val_acc: 0.0000e+00\n",
      "Epoch 294/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0325 - acc: 0.0217 - val_loss: 0.0371 - val_acc: 0.0000e+00\n",
      "Epoch 295/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0332 - acc: 0.0217 - val_loss: 0.0354 - val_acc: 0.0000e+00\n",
      "Epoch 296/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0332 - acc: 0.0167 - val_loss: 0.0314 - val_acc: 0.0133\n",
      "Epoch 297/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0322 - acc: 0.0133 - val_loss: 0.0279 - val_acc: 0.0133\n",
      "Epoch 298/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0304 - acc: 0.0150 - val_loss: 0.0276 - val_acc: 0.0133\n",
      "Epoch 299/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0286 - acc: 0.0150 - val_loss: 0.0276 - val_acc: 0.0000e+00\n",
      "Epoch 300/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0277 - acc: 0.0150 - val_loss: 0.0268 - val_acc: 0.0067\n",
      "Epoch 301/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0272 - acc: 0.0167 - val_loss: 0.0268 - val_acc: 0.0133\n",
      "Epoch 302/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0274 - acc: 0.0200 - val_loss: 0.0278 - val_acc: 0.0133\n",
      "Epoch 303/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0280 - acc: 0.0133 - val_loss: 0.0282 - val_acc: 0.0133\n",
      "Epoch 304/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0284 - acc: 0.0117 - val_loss: 0.0283 - val_acc: 0.0133\n",
      "Epoch 305/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0285 - acc: 0.0117 - val_loss: 0.0282 - val_acc: 0.0133\n",
      "Epoch 306/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0285 - acc: 0.0117 - val_loss: 0.0282 - val_acc: 0.0133\n",
      "Epoch 307/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0285 - acc: 0.0133 - val_loss: 0.0283 - val_acc: 0.0133\n",
      "Epoch 308/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0284 - acc: 0.0133 - val_loss: 0.0284 - val_acc: 0.0133\n",
      "Epoch 309/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0284 - acc: 0.0133 - val_loss: 0.0287 - val_acc: 0.0133\n",
      "Epoch 310/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0285 - acc: 0.0133 - val_loss: 0.0292 - val_acc: 0.0133\n",
      "Epoch 311/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0286 - acc: 0.0150 - val_loss: 0.0297 - val_acc: 0.0133\n",
      "Epoch 312/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0288 - acc: 0.0150 - val_loss: 0.0302 - val_acc: 0.0133\n",
      "Epoch 313/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0291 - acc: 0.0150 - val_loss: 0.0308 - val_acc: 0.0133\n",
      "Epoch 314/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0294 - acc: 0.0167 - val_loss: 0.0313 - val_acc: 0.0133\n",
      "Epoch 315/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0297 - acc: 0.0167 - val_loss: 0.0314 - val_acc: 0.0133\n",
      "Epoch 316/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0299 - acc: 0.0167 - val_loss: 0.0311 - val_acc: 0.0133\n",
      "Epoch 317/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0300 - acc: 0.0167 - val_loss: 0.0302 - val_acc: 0.0133\n",
      "Epoch 318/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0299 - acc: 0.0150 - val_loss: 0.0289 - val_acc: 0.0133\n",
      "Epoch 319/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0295 - acc: 0.0167 - val_loss: 0.0277 - val_acc: 0.0133\n",
      "Epoch 320/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0288 - acc: 0.0200 - val_loss: 0.0269 - val_acc: 0.0133\n",
      "Epoch 321/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0280 - acc: 0.0183 - val_loss: 0.0264 - val_acc: 0.0133\n",
      "Epoch 322/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0273 - acc: 0.0150 - val_loss: 0.0262 - val_acc: 0.0133\n",
      "Epoch 323/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0268 - acc: 0.0150 - val_loss: 0.0261 - val_acc: 0.0133\n",
      "Epoch 324/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0266 - acc: 0.0183 - val_loss: 0.0264 - val_acc: 0.0133\n",
      "Epoch 325/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0267 - acc: 0.0183 - val_loss: 0.0271 - val_acc: 0.0133\n",
      "Epoch 326/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0271 - acc: 0.0183 - val_loss: 0.0284 - val_acc: 0.0133\n",
      "Epoch 327/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0279 - acc: 0.0200 - val_loss: 0.0305 - val_acc: 0.0133\n",
      "Epoch 328/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0290 - acc: 0.0250 - val_loss: 0.0329 - val_acc: 0.0133\n",
      "Epoch 329/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0303 - acc: 0.0300 - val_loss: 0.0356 - val_acc: 0.0000e+00\n",
      "Epoch 330/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0315 - acc: 0.0283 - val_loss: 0.0378 - val_acc: 0.0000e+00\n",
      "Epoch 331/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0326 - acc: 0.0317 - val_loss: 0.0375 - val_acc: 0.0000e+00\n",
      "Epoch 332/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0330 - acc: 0.0300 - val_loss: 0.0335 - val_acc: 0.0133\n",
      "Epoch 333/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0326 - acc: 0.0217 - val_loss: 0.0282 - val_acc: 0.0133\n",
      "Epoch 334/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0308 - acc: 0.0183 - val_loss: 0.0266 - val_acc: 0.0133\n",
      "Epoch 335/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0284 - acc: 0.0150 - val_loss: 0.0274 - val_acc: 0.0000e+00\n",
      "Epoch 336/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0271 - acc: 0.0150 - val_loss: 0.0261 - val_acc: 0.0000e+00\n",
      "Epoch 337/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0263 - acc: 0.0167 - val_loss: 0.0259 - val_acc: 0.0133\n",
      "Epoch 338/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0265 - acc: 0.0200 - val_loss: 0.0269 - val_acc: 0.0133\n",
      "Epoch 339/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0271 - acc: 0.0217 - val_loss: 0.0270 - val_acc: 0.0133\n",
      "Epoch 340/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0274 - acc: 0.0217 - val_loss: 0.0265 - val_acc: 0.0133\n",
      "Epoch 341/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0272 - acc: 0.0200 - val_loss: 0.0261 - val_acc: 0.0133\n",
      "Epoch 342/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0269 - acc: 0.0167 - val_loss: 0.0259 - val_acc: 0.0133\n",
      "Epoch 343/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0265 - acc: 0.0167 - val_loss: 0.0259 - val_acc: 0.0133\n",
      "Epoch 344/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0264 - acc: 0.0167 - val_loss: 0.0262 - val_acc: 0.0133\n",
      "Epoch 345/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0264 - acc: 0.0200 - val_loss: 0.0266 - val_acc: 0.0133\n",
      "Epoch 346/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0266 - acc: 0.0183 - val_loss: 0.0275 - val_acc: 0.0133\n",
      "Epoch 347/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0270 - acc: 0.0183 - val_loss: 0.0287 - val_acc: 0.0133\n",
      "Epoch 348/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0276 - acc: 0.0233 - val_loss: 0.0303 - val_acc: 0.0133\n",
      "Epoch 349/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0284 - acc: 0.0267 - val_loss: 0.0321 - val_acc: 0.0133\n",
      "Epoch 350/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0293 - acc: 0.0283 - val_loss: 0.0339 - val_acc: 0.0133\n",
      "Epoch 351/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0302 - acc: 0.0283 - val_loss: 0.0352 - val_acc: 0.0133\n",
      "Epoch 352/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0310 - acc: 0.0367 - val_loss: 0.0354 - val_acc: 0.0133\n",
      "Epoch 353/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0315 - acc: 0.0350 - val_loss: 0.0334 - val_acc: 0.0133\n",
      "Epoch 354/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0315 - acc: 0.0283 - val_loss: 0.0296 - val_acc: 0.0133\n",
      "Epoch 355/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0307 - acc: 0.0217 - val_loss: 0.0265 - val_acc: 0.0133\n",
      "Epoch 356/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0289 - acc: 0.0183 - val_loss: 0.0261 - val_acc: 0.0133\n",
      "Epoch 357/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0271 - acc: 0.0150 - val_loss: 0.0263 - val_acc: 0.0000e+00\n",
      "Epoch 358/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0261 - acc: 0.0150 - val_loss: 0.0254 - val_acc: 0.0133\n",
      "Epoch 359/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0256 - acc: 0.0167 - val_loss: 0.0254 - val_acc: 0.0133\n",
      "Epoch 360/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0258 - acc: 0.0183 - val_loss: 0.0262 - val_acc: 0.0133\n",
      "Epoch 361/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0263 - acc: 0.0200 - val_loss: 0.0267 - val_acc: 0.0133\n",
      "Epoch 362/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0267 - acc: 0.0200 - val_loss: 0.0267 - val_acc: 0.0133\n",
      "Epoch 363/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0267 - acc: 0.0200 - val_loss: 0.0265 - val_acc: 0.0133\n",
      "Epoch 364/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0267 - acc: 0.0217 - val_loss: 0.0265 - val_acc: 0.0133\n",
      "Epoch 365/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0266 - acc: 0.0183 - val_loss: 0.0266 - val_acc: 0.0133\n",
      "Epoch 366/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0266 - acc: 0.0200 - val_loss: 0.0269 - val_acc: 0.0133\n",
      "Epoch 367/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0266 - acc: 0.0200 - val_loss: 0.0274 - val_acc: 0.0133\n",
      "Epoch 368/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0268 - acc: 0.0217 - val_loss: 0.0281 - val_acc: 0.0133\n",
      "Epoch 369/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0271 - acc: 0.0250 - val_loss: 0.0291 - val_acc: 0.0133\n",
      "Epoch 370/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0275 - acc: 0.0250 - val_loss: 0.0301 - val_acc: 0.0133\n",
      "Epoch 371/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0280 - acc: 0.0283 - val_loss: 0.0312 - val_acc: 0.0133\n",
      "Epoch 372/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0285 - acc: 0.0300 - val_loss: 0.0321 - val_acc: 0.0133\n",
      "Epoch 373/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0291 - acc: 0.0317 - val_loss: 0.0325 - val_acc: 0.0133\n",
      "Epoch 374/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0295 - acc: 0.0333 - val_loss: 0.0320 - val_acc: 0.0133\n",
      "Epoch 375/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0297 - acc: 0.0333 - val_loss: 0.0305 - val_acc: 0.0133\n",
      "Epoch 376/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0297 - acc: 0.0267 - val_loss: 0.0281 - val_acc: 0.0133\n",
      "Epoch 377/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0290 - acc: 0.0250 - val_loss: 0.0261 - val_acc: 0.0133\n",
      "Epoch 378/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0278 - acc: 0.0167 - val_loss: 0.0254 - val_acc: 0.0133\n",
      "Epoch 379/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0264 - acc: 0.0150 - val_loss: 0.0252 - val_acc: 0.0133\n",
      "Epoch 380/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0255 - acc: 0.0167 - val_loss: 0.0249 - val_acc: 0.0133\n",
      "Epoch 381/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0249 - acc: 0.0167 - val_loss: 0.0248 - val_acc: 0.0133\n",
      "Epoch 382/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0249 - acc: 0.0183 - val_loss: 0.0255 - val_acc: 0.0133\n",
      "Epoch 383/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0254 - acc: 0.0217 - val_loss: 0.0269 - val_acc: 0.0133\n",
      "Epoch 384/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0263 - acc: 0.0300 - val_loss: 0.0288 - val_acc: 0.0133\n",
      "Epoch 385/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0275 - acc: 0.0350 - val_loss: 0.0310 - val_acc: 0.0133\n",
      "Epoch 386/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0286 - acc: 0.0383 - val_loss: 0.0327 - val_acc: 0.0133\n",
      "Epoch 387/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0293 - acc: 0.0417 - val_loss: 0.0333 - val_acc: 0.0133\n",
      "Epoch 388/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0298 - acc: 0.0433 - val_loss: 0.0321 - val_acc: 0.0133\n",
      "Epoch 389/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0300 - acc: 0.0367 - val_loss: 0.0292 - val_acc: 0.0133\n",
      "Epoch 390/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0296 - acc: 0.0283 - val_loss: 0.0264 - val_acc: 0.0133\n",
      "Epoch 391/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0284 - acc: 0.0200 - val_loss: 0.0253 - val_acc: 0.0133\n",
      "Epoch 392/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0269 - acc: 0.0167 - val_loss: 0.0254 - val_acc: 0.0133\n",
      "Epoch 393/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0256 - acc: 0.0167 - val_loss: 0.0249 - val_acc: 0.0133\n",
      "Epoch 394/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0248 - acc: 0.0167 - val_loss: 0.0245 - val_acc: 0.0133\n",
      "Epoch 395/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0246 - acc: 0.0183 - val_loss: 0.0248 - val_acc: 0.0133\n",
      "Epoch 396/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0249 - acc: 0.0150 - val_loss: 0.0258 - val_acc: 0.0133\n",
      "Epoch 397/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0255 - acc: 0.0233 - val_loss: 0.0269 - val_acc: 0.0133\n",
      "Epoch 398/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0261 - acc: 0.0283 - val_loss: 0.0279 - val_acc: 0.0133\n",
      "Epoch 399/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0267 - acc: 0.0300 - val_loss: 0.0287 - val_acc: 0.0133\n",
      "Epoch 400/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0271 - acc: 0.0317 - val_loss: 0.0293 - val_acc: 0.0133\n",
      "Epoch 401/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0275 - acc: 0.0300 - val_loss: 0.0293 - val_acc: 0.0133\n",
      "Epoch 402/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0277 - acc: 0.0283 - val_loss: 0.0287 - val_acc: 0.0133\n",
      "Epoch 403/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0278 - acc: 0.0267 - val_loss: 0.0279 - val_acc: 0.0133\n",
      "Epoch 404/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0276 - acc: 0.0217 - val_loss: 0.0270 - val_acc: 0.0133\n",
      "Epoch 405/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0271 - acc: 0.0233 - val_loss: 0.0260 - val_acc: 0.0133\n",
      "Epoch 406/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0265 - acc: 0.0183 - val_loss: 0.0253 - val_acc: 0.0133\n",
      "Epoch 407/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0258 - acc: 0.0167 - val_loss: 0.0249 - val_acc: 0.0133\n",
      "Epoch 408/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0252 - acc: 0.0167 - val_loss: 0.0247 - val_acc: 0.0133\n",
      "Epoch 409/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0248 - acc: 0.0167 - val_loss: 0.0249 - val_acc: 0.0133\n",
      "Epoch 410/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0247 - acc: 0.0183 - val_loss: 0.0254 - val_acc: 0.0133\n",
      "Epoch 411/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0249 - acc: 0.0200 - val_loss: 0.0265 - val_acc: 0.0133\n",
      "Epoch 412/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0254 - acc: 0.0283 - val_loss: 0.0279 - val_acc: 0.0133\n",
      "Epoch 413/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0261 - acc: 0.0333 - val_loss: 0.0296 - val_acc: 0.0133\n",
      "Epoch 414/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0269 - acc: 0.0350 - val_loss: 0.0314 - val_acc: 0.0133\n",
      "Epoch 415/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0278 - acc: 0.0400 - val_loss: 0.0333 - val_acc: 0.0133\n",
      "Epoch 416/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0286 - acc: 0.0450 - val_loss: 0.0349 - val_acc: 0.0133\n",
      "Epoch 417/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0295 - acc: 0.0550 - val_loss: 0.0358 - val_acc: 0.0133\n",
      "Epoch 418/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0302 - acc: 0.0550 - val_loss: 0.0347 - val_acc: 0.0133\n",
      "Epoch 419/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0308 - acc: 0.0533 - val_loss: 0.0317 - val_acc: 0.0133\n",
      "Epoch 420/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0307 - acc: 0.0317 - val_loss: 0.0273 - val_acc: 0.0133\n",
      "Epoch 421/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0294 - acc: 0.0217 - val_loss: 0.0250 - val_acc: 0.0133\n",
      "Epoch 422/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0271 - acc: 0.0167 - val_loss: 0.0254 - val_acc: 0.0133\n",
      "Epoch 423/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0252 - acc: 0.0167 - val_loss: 0.0247 - val_acc: 0.0133\n",
      "Epoch 424/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0242 - acc: 0.0167 - val_loss: 0.0239 - val_acc: 0.0133\n",
      "Epoch 425/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0239 - acc: 0.0200 - val_loss: 0.0250 - val_acc: 0.0133\n",
      "Epoch 426/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0248 - acc: 0.0233 - val_loss: 0.0270 - val_acc: 0.0133\n",
      "Epoch 427/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0259 - acc: 0.0317 - val_loss: 0.0285 - val_acc: 0.0133\n",
      "Epoch 428/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0269 - acc: 0.0383 - val_loss: 0.0293 - val_acc: 0.0133\n",
      "Epoch 429/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0274 - acc: 0.0417 - val_loss: 0.0289 - val_acc: 0.0133\n",
      "Epoch 430/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0274 - acc: 0.0350 - val_loss: 0.0272 - val_acc: 0.0133\n",
      "Epoch 431/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0268 - acc: 0.0333 - val_loss: 0.0255 - val_acc: 0.0133\n",
      "Epoch 432/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0261 - acc: 0.0233 - val_loss: 0.0247 - val_acc: 0.0133\n",
      "Epoch 433/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0252 - acc: 0.0167 - val_loss: 0.0243 - val_acc: 0.0133\n",
      "Epoch 434/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0243 - acc: 0.0183 - val_loss: 0.0238 - val_acc: 0.0133\n",
      "Epoch 435/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0237 - acc: 0.0183 - val_loss: 0.0240 - val_acc: 0.0133\n",
      "Epoch 436/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0238 - acc: 0.0183 - val_loss: 0.0250 - val_acc: 0.0133\n",
      "Epoch 437/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0245 - acc: 0.0217 - val_loss: 0.0272 - val_acc: 0.0133\n",
      "Epoch 438/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0259 - acc: 0.0350 - val_loss: 0.0299 - val_acc: 0.0133\n",
      "Epoch 439/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0272 - acc: 0.0517 - val_loss: 0.0321 - val_acc: 0.0133\n",
      "Epoch 440/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0282 - acc: 0.0600 - val_loss: 0.0331 - val_acc: 0.0267\n",
      "Epoch 441/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0291 - acc: 0.0583 - val_loss: 0.0316 - val_acc: 0.0133\n",
      "Epoch 442/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0295 - acc: 0.0383 - val_loss: 0.0278 - val_acc: 0.0133\n",
      "Epoch 443/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0287 - acc: 0.0283 - val_loss: 0.0250 - val_acc: 0.0133\n",
      "Epoch 444/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0269 - acc: 0.0183 - val_loss: 0.0246 - val_acc: 0.0133\n",
      "Epoch 445/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0251 - acc: 0.0167 - val_loss: 0.0244 - val_acc: 0.0133\n",
      "Epoch 446/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0238 - acc: 0.0167 - val_loss: 0.0236 - val_acc: 0.0133\n",
      "Epoch 447/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0233 - acc: 0.0200 - val_loss: 0.0239 - val_acc: 0.0133\n",
      "Epoch 448/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0237 - acc: 0.0200 - val_loss: 0.0256 - val_acc: 0.0133\n",
      "Epoch 449/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0248 - acc: 0.0333 - val_loss: 0.0280 - val_acc: 0.0133\n",
      "Epoch 450/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0263 - acc: 0.0450 - val_loss: 0.0306 - val_acc: 0.0133\n",
      "Epoch 451/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0277 - acc: 0.0600 - val_loss: 0.0323 - val_acc: 0.0133\n",
      "Epoch 452/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0286 - acc: 0.0650 - val_loss: 0.0314 - val_acc: 0.0267\n",
      "Epoch 453/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0291 - acc: 0.0550 - val_loss: 0.0282 - val_acc: 0.0200\n",
      "Epoch 454/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0287 - acc: 0.0317 - val_loss: 0.0251 - val_acc: 0.0133\n",
      "Epoch 455/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0271 - acc: 0.0217 - val_loss: 0.0247 - val_acc: 0.0133\n",
      "Epoch 456/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0252 - acc: 0.0183 - val_loss: 0.0246 - val_acc: 0.0133\n",
      "Epoch 457/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0240 - acc: 0.0167 - val_loss: 0.0238 - val_acc: 0.0133\n",
      "Epoch 458/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0233 - acc: 0.0183 - val_loss: 0.0236 - val_acc: 0.0133\n",
      "Epoch 459/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0234 - acc: 0.0200 - val_loss: 0.0244 - val_acc: 0.0133\n",
      "Epoch 460/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0240 - acc: 0.0233 - val_loss: 0.0255 - val_acc: 0.0133\n",
      "Epoch 461/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0246 - acc: 0.0250 - val_loss: 0.0261 - val_acc: 0.0133\n",
      "Epoch 462/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0250 - acc: 0.0283 - val_loss: 0.0268 - val_acc: 0.0133\n",
      "Epoch 463/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0252 - acc: 0.0300 - val_loss: 0.0265 - val_acc: 0.0133\n",
      "Epoch 464/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0254 - acc: 0.0250 - val_loss: 0.0269 - val_acc: 0.0133\n",
      "Epoch 465/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0259 - acc: 0.0283 - val_loss: 0.0272 - val_acc: 0.0133\n",
      "Epoch 466/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0262 - acc: 0.0267 - val_loss: 0.0267 - val_acc: 0.0133\n",
      "Epoch 467/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0258 - acc: 0.0200 - val_loss: 0.0260 - val_acc: 0.0133\n",
      "Epoch 468/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0253 - acc: 0.0233 - val_loss: 0.0252 - val_acc: 0.0133\n",
      "Epoch 469/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0251 - acc: 0.0217 - val_loss: 0.0263 - val_acc: 0.0133\n",
      "Epoch 470/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0248 - acc: 0.0200 - val_loss: 0.0249 - val_acc: 0.0133\n",
      "Epoch 471/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0247 - acc: 0.0283 - val_loss: 0.0253 - val_acc: 0.0133\n",
      "Epoch 472/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0246 - acc: 0.0233 - val_loss: 0.0261 - val_acc: 0.0133\n",
      "Epoch 473/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0244 - acc: 0.0217 - val_loss: 0.0259 - val_acc: 0.0133\n",
      "Epoch 474/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0245 - acc: 0.0300 - val_loss: 0.0263 - val_acc: 0.0133\n",
      "Epoch 475/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0246 - acc: 0.0350 - val_loss: 0.0276 - val_acc: 0.0133\n",
      "Epoch 476/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0249 - acc: 0.0417 - val_loss: 0.0288 - val_acc: 0.0133\n",
      "Epoch 477/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0256 - acc: 0.0533 - val_loss: 0.0306 - val_acc: 0.0133\n",
      "Epoch 478/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0265 - acc: 0.0600 - val_loss: 0.0330 - val_acc: 0.0333\n",
      "Epoch 479/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0276 - acc: 0.0717 - val_loss: 0.0350 - val_acc: 0.0333\n",
      "Epoch 480/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0288 - acc: 0.0800 - val_loss: 0.0365 - val_acc: 0.0333\n",
      "Epoch 481/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0298 - acc: 0.0833 - val_loss: 0.0346 - val_acc: 0.0333\n",
      "Epoch 482/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0304 - acc: 0.0667 - val_loss: 0.0292 - val_acc: 0.0133\n",
      "Epoch 483/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0301 - acc: 0.0417 - val_loss: 0.0246 - val_acc: 0.0333\n",
      "Epoch 484/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0277 - acc: 0.0183 - val_loss: 0.0255 - val_acc: 0.0133\n",
      "Epoch 485/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0252 - acc: 0.0200 - val_loss: 0.0248 - val_acc: 0.0133\n",
      "Epoch 486/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0236 - acc: 0.0200 - val_loss: 0.0230 - val_acc: 0.0133\n",
      "Epoch 487/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0230 - acc: 0.0250 - val_loss: 0.0242 - val_acc: 0.0133\n",
      "Epoch 488/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0238 - acc: 0.0233 - val_loss: 0.0258 - val_acc: 0.0133\n",
      "Epoch 489/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0248 - acc: 0.0333 - val_loss: 0.0254 - val_acc: 0.0133\n",
      "Epoch 490/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0248 - acc: 0.0267 - val_loss: 0.0243 - val_acc: 0.0133\n",
      "Epoch 491/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0241 - acc: 0.0233 - val_loss: 0.0237 - val_acc: 0.0133\n",
      "Epoch 492/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0234 - acc: 0.0233 - val_loss: 0.0233 - val_acc: 0.0133\n",
      "Epoch 493/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0230 - acc: 0.0200 - val_loss: 0.0232 - val_acc: 0.0133\n",
      "Epoch 494/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0227 - acc: 0.0217 - val_loss: 0.0234 - val_acc: 0.0133\n",
      "Epoch 495/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0225 - acc: 0.0167 - val_loss: 0.0234 - val_acc: 0.0133\n",
      "Epoch 496/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0226 - acc: 0.0217 - val_loss: 0.0242 - val_acc: 0.0133\n",
      "Epoch 497/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0230 - acc: 0.0250 - val_loss: 0.0250 - val_acc: 0.0133\n",
      "Epoch 498/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0234 - acc: 0.0350 - val_loss: 0.0263 - val_acc: 0.0133\n",
      "Epoch 499/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0240 - acc: 0.0433 - val_loss: 0.0281 - val_acc: 0.0333\n",
      "Epoch 500/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0249 - acc: 0.0533 - val_loss: 0.0298 - val_acc: 0.0333\n",
      "Epoch 501/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0259 - acc: 0.0667 - val_loss: 0.0320 - val_acc: 0.0333\n",
      "Epoch 502/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0269 - acc: 0.0717 - val_loss: 0.0347 - val_acc: 0.0333\n",
      "Epoch 503/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0281 - acc: 0.0850 - val_loss: 0.0384 - val_acc: 0.0333\n",
      "Epoch 504/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0299 - acc: 0.0883 - val_loss: 0.0378 - val_acc: 0.0333\n",
      "Epoch 505/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0318 - acc: 0.0750 - val_loss: 0.0303 - val_acc: 0.0533\n",
      "Epoch 506/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0319 - acc: 0.0450 - val_loss: 0.0251 - val_acc: 0.0333\n",
      "Epoch 507/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0287 - acc: 0.0183 - val_loss: 0.0283 - val_acc: 0.0067\n",
      "Epoch 508/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0255 - acc: 0.0267 - val_loss: 0.0236 - val_acc: 0.0000e+00\n",
      "Epoch 509/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0235 - acc: 0.0200 - val_loss: 0.0236 - val_acc: 0.0133\n",
      "Epoch 510/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0233 - acc: 0.0217 - val_loss: 0.0261 - val_acc: 0.0133\n",
      "Epoch 511/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0251 - acc: 0.0433 - val_loss: 0.0262 - val_acc: 0.0133\n",
      "Epoch 512/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0255 - acc: 0.0383 - val_loss: 0.0241 - val_acc: 0.0133\n",
      "Epoch 513/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0242 - acc: 0.0250 - val_loss: 0.0231 - val_acc: 0.0133\n",
      "Epoch 514/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0230 - acc: 0.0200 - val_loss: 0.0228 - val_acc: 0.0133\n",
      "Epoch 515/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0223 - acc: 0.0233 - val_loss: 0.0229 - val_acc: 0.0133\n",
      "Epoch 516/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0226 - acc: 0.0217 - val_loss: 0.0248 - val_acc: 0.0133\n",
      "Epoch 517/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0239 - acc: 0.0383 - val_loss: 0.0273 - val_acc: 0.0133\n",
      "Epoch 518/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0256 - acc: 0.0550 - val_loss: 0.0301 - val_acc: 0.0333\n",
      "Epoch 519/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0272 - acc: 0.0667 - val_loss: 0.0307 - val_acc: 0.0333\n",
      "Epoch 520/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0278 - acc: 0.0683 - val_loss: 0.0274 - val_acc: 0.0333\n",
      "Epoch 521/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0269 - acc: 0.0450 - val_loss: 0.0237 - val_acc: 0.0333\n",
      "Epoch 522/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0247 - acc: 0.0267 - val_loss: 0.0231 - val_acc: 0.0000e+00\n",
      "Epoch 523/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0230 - acc: 0.0217 - val_loss: 0.0230 - val_acc: 0.0000e+00\n",
      "Epoch 524/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0222 - acc: 0.0200 - val_loss: 0.0226 - val_acc: 0.0000e+00\n",
      "Epoch 525/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0222 - acc: 0.0183 - val_loss: 0.0236 - val_acc: 0.0133\n",
      "Epoch 526/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0228 - acc: 0.0233 - val_loss: 0.0241 - val_acc: 0.0133\n",
      "Epoch 527/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0233 - acc: 0.0283 - val_loss: 0.0240 - val_acc: 0.0133\n",
      "Epoch 528/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0234 - acc: 0.0267 - val_loss: 0.0239 - val_acc: 0.0133\n",
      "Epoch 529/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0231 - acc: 0.0217 - val_loss: 0.0237 - val_acc: 0.0133\n",
      "Epoch 530/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0229 - acc: 0.0217 - val_loss: 0.0238 - val_acc: 0.0133\n",
      "Epoch 531/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0227 - acc: 0.0267 - val_loss: 0.0242 - val_acc: 0.0133\n",
      "Epoch 532/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0228 - acc: 0.0283 - val_loss: 0.0249 - val_acc: 0.0133\n",
      "Epoch 533/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0230 - acc: 0.0317 - val_loss: 0.0260 - val_acc: 0.0133\n",
      "Epoch 534/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0235 - acc: 0.0350 - val_loss: 0.0280 - val_acc: 0.0133\n",
      "Epoch 535/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0240 - acc: 0.0433 - val_loss: 0.0287 - val_acc: 0.0133\n",
      "Epoch 536/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0247 - acc: 0.0617 - val_loss: 0.0294 - val_acc: 0.0333\n",
      "Epoch 537/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0254 - acc: 0.0617 - val_loss: 0.0293 - val_acc: 0.0133\n",
      "Epoch 538/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0264 - acc: 0.0517 - val_loss: 0.0302 - val_acc: 0.0133\n",
      "Epoch 539/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0273 - acc: 0.0667 - val_loss: 0.0371 - val_acc: 0.0333\n",
      "Epoch 540/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0297 - acc: 0.0733 - val_loss: 0.0346 - val_acc: 0.0400\n",
      "Epoch 541/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0300 - acc: 0.0767 - val_loss: 0.0268 - val_acc: 0.0400\n",
      "Epoch 542/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0280 - acc: 0.0433 - val_loss: 0.0239 - val_acc: 0.0000e+00\n",
      "Epoch 543/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0242 - acc: 0.0233 - val_loss: 0.0232 - val_acc: 0.0000e+00\n",
      "Epoch 544/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0223 - acc: 0.0250 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 545/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0219 - acc: 0.0183 - val_loss: 0.0233 - val_acc: 0.0133\n",
      "Epoch 546/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0230 - acc: 0.0267 - val_loss: 0.0260 - val_acc: 0.0133\n",
      "Epoch 547/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0247 - acc: 0.0450 - val_loss: 0.0275 - val_acc: 0.0333\n",
      "Epoch 548/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0255 - acc: 0.0550 - val_loss: 0.0263 - val_acc: 0.0133\n",
      "Epoch 549/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0254 - acc: 0.0467 - val_loss: 0.0241 - val_acc: 0.0133\n",
      "Epoch 550/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0242 - acc: 0.0250 - val_loss: 0.0228 - val_acc: 0.0133\n",
      "Epoch 551/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0231 - acc: 0.0250 - val_loss: 0.0231 - val_acc: 0.0000e+00\n",
      "Epoch 552/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0221 - acc: 0.0217 - val_loss: 0.0226 - val_acc: 0.0000e+00\n",
      "Epoch 553/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0216 - acc: 0.0200 - val_loss: 0.0222 - val_acc: 0.0067\n",
      "Epoch 554/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0216 - acc: 0.0233 - val_loss: 0.0228 - val_acc: 0.0133\n",
      "Epoch 555/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0220 - acc: 0.0200 - val_loss: 0.0238 - val_acc: 0.0133\n",
      "Epoch 556/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0225 - acc: 0.0250 - val_loss: 0.0242 - val_acc: 0.0133\n",
      "Epoch 557/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0227 - acc: 0.0267 - val_loss: 0.0248 - val_acc: 0.0133\n",
      "Epoch 558/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0230 - acc: 0.0317 - val_loss: 0.0250 - val_acc: 0.0267\n",
      "Epoch 559/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0231 - acc: 0.0350 - val_loss: 0.0258 - val_acc: 0.0333\n",
      "Epoch 560/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0232 - acc: 0.0367 - val_loss: 0.0264 - val_acc: 0.0267\n",
      "Epoch 561/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0232 - acc: 0.0367 - val_loss: 0.0278 - val_acc: 0.0333\n",
      "Epoch 562/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0239 - acc: 0.0550 - val_loss: 0.0285 - val_acc: 0.0333\n",
      "Epoch 563/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0241 - acc: 0.0483 - val_loss: 0.0281 - val_acc: 0.0333\n",
      "Epoch 564/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0242 - acc: 0.0617 - val_loss: 0.0300 - val_acc: 0.0333\n",
      "Epoch 565/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0253 - acc: 0.0717 - val_loss: 0.0308 - val_acc: 0.0333\n",
      "Epoch 566/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0255 - acc: 0.0517 - val_loss: 0.0289 - val_acc: 0.0200\n",
      "Epoch 567/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0254 - acc: 0.0467 - val_loss: 0.0278 - val_acc: 0.0333\n",
      "Epoch 568/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0250 - acc: 0.0367 - val_loss: 0.0252 - val_acc: 0.0333\n",
      "Epoch 569/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0229 - acc: 0.0267 - val_loss: 0.0240 - val_acc: 0.0333\n",
      "Epoch 570/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0226 - acc: 0.0333 - val_loss: 0.0244 - val_acc: 0.0133\n",
      "Epoch 571/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0229 - acc: 0.0267 - val_loss: 0.0231 - val_acc: 0.0133\n",
      "Epoch 572/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0232 - acc: 0.0267 - val_loss: 0.0236 - val_acc: 0.0133\n",
      "Epoch 573/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0227 - acc: 0.0200 - val_loss: 0.0233 - val_acc: 0.0200\n",
      "Epoch 574/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0218 - acc: 0.0250 - val_loss: 0.0227 - val_acc: 0.0133\n",
      "Epoch 575/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0221 - acc: 0.0217 - val_loss: 0.0232 - val_acc: 0.0133\n",
      "Epoch 576/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0223 - acc: 0.0267 - val_loss: 0.0275 - val_acc: 0.0133\n",
      "Epoch 577/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0240 - acc: 0.0417 - val_loss: 0.0278 - val_acc: 0.0333\n",
      "Epoch 578/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0250 - acc: 0.0567 - val_loss: 0.0293 - val_acc: 0.0333\n",
      "Epoch 579/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0260 - acc: 0.0633 - val_loss: 0.0312 - val_acc: 0.0333\n",
      "Epoch 580/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0265 - acc: 0.0567 - val_loss: 0.0308 - val_acc: 0.0333\n",
      "Epoch 581/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0274 - acc: 0.0667 - val_loss: 0.0275 - val_acc: 0.0333\n",
      "Epoch 582/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0271 - acc: 0.0450 - val_loss: 0.0237 - val_acc: 0.0333\n",
      "Epoch 583/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0252 - acc: 0.0283 - val_loss: 0.0230 - val_acc: 0.0000e+00\n",
      "Epoch 584/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0229 - acc: 0.0233 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
      "Epoch 585/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0217 - acc: 0.0167 - val_loss: 0.0219 - val_acc: 0.0000e+00\n",
      "Epoch 586/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0212 - acc: 0.0233 - val_loss: 0.0220 - val_acc: 0.0000e+00\n",
      "Epoch 587/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0210 - acc: 0.0217 - val_loss: 0.0222 - val_acc: 0.0133\n",
      "Epoch 588/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0214 - acc: 0.0267 - val_loss: 0.0236 - val_acc: 0.0267\n",
      "Epoch 589/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0218 - acc: 0.0350 - val_loss: 0.0247 - val_acc: 0.0333\n",
      "Epoch 590/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0223 - acc: 0.0483 - val_loss: 0.0263 - val_acc: 0.0333\n",
      "Epoch 591/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0233 - acc: 0.0600 - val_loss: 0.0274 - val_acc: 0.0333\n",
      "Epoch 592/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0241 - acc: 0.0683 - val_loss: 0.0296 - val_acc: 0.0333\n",
      "Epoch 593/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0252 - acc: 0.0800 - val_loss: 0.0313 - val_acc: 0.0333\n",
      "Epoch 594/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0263 - acc: 0.0783 - val_loss: 0.0337 - val_acc: 0.0333\n",
      "Epoch 595/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0276 - acc: 0.0917 - val_loss: 0.0319 - val_acc: 0.0333\n",
      "Epoch 596/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0281 - acc: 0.0733 - val_loss: 0.0265 - val_acc: 0.0333\n",
      "Epoch 597/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0272 - acc: 0.0600 - val_loss: 0.0230 - val_acc: 0.0067\n",
      "Epoch 598/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0249 - acc: 0.0300 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "Epoch 599/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0228 - acc: 0.0233 - val_loss: 0.0232 - val_acc: 0.0000e+00\n",
      "Epoch 600/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0211 - acc: 0.0267 - val_loss: 0.0213 - val_acc: 0.0000e+00\n",
      "Epoch 601/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0213 - acc: 0.0300 - val_loss: 0.0243 - val_acc: 0.0333\n",
      "Epoch 602/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0234 - acc: 0.0500 - val_loss: 0.0296 - val_acc: 0.0333\n",
      "Epoch 603/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0263 - acc: 0.0750 - val_loss: 0.0314 - val_acc: 0.0333\n",
      "Epoch 604/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0272 - acc: 0.0767 - val_loss: 0.0284 - val_acc: 0.0333\n",
      "Epoch 605/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0270 - acc: 0.0633 - val_loss: 0.0234 - val_acc: 0.0267\n",
      "Epoch 606/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0255 - acc: 0.0383 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
      "Epoch 607/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0234 - acc: 0.0250 - val_loss: 0.0228 - val_acc: 0.0000e+00\n",
      "Epoch 608/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0216 - acc: 0.0333 - val_loss: 0.0214 - val_acc: 0.0000e+00\n",
      "Epoch 609/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0207 - acc: 0.0267 - val_loss: 0.0213 - val_acc: 0.0000e+00\n",
      "Epoch 610/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0204 - acc: 0.0250 - val_loss: 0.0224 - val_acc: 0.0267\n",
      "Epoch 611/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0210 - acc: 0.0250 - val_loss: 0.0239 - val_acc: 0.0267\n",
      "Epoch 612/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0219 - acc: 0.0400 - val_loss: 0.0249 - val_acc: 0.0333\n",
      "Epoch 613/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0226 - acc: 0.0517 - val_loss: 0.0264 - val_acc: 0.0333\n",
      "Epoch 614/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0234 - acc: 0.0483 - val_loss: 0.0284 - val_acc: 0.0333\n",
      "Epoch 615/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0254 - acc: 0.0717 - val_loss: 0.0343 - val_acc: 0.0333\n",
      "Epoch 616/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0278 - acc: 0.0867 - val_loss: 0.0368 - val_acc: 0.0333\n",
      "Epoch 617/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0309 - acc: 0.0950 - val_loss: 0.0303 - val_acc: 0.0467\n",
      "Epoch 618/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0310 - acc: 0.0667 - val_loss: 0.0227 - val_acc: 0.0200\n",
      "Epoch 619/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0273 - acc: 0.0367 - val_loss: 0.0257 - val_acc: 0.0000e+00\n",
      "Epoch 620/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0238 - acc: 0.0433 - val_loss: 0.0225 - val_acc: 0.0000e+00\n",
      "Epoch 621/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0221 - acc: 0.0350 - val_loss: 0.0219 - val_acc: 0.0133\n",
      "Epoch 622/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0217 - acc: 0.0333 - val_loss: 0.0243 - val_acc: 0.0267\n",
      "Epoch 623/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0230 - acc: 0.0417 - val_loss: 0.0225 - val_acc: 0.0000e+00\n",
      "Epoch 624/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0221 - acc: 0.0317 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 625/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0210 - acc: 0.0300 - val_loss: 0.0213 - val_acc: 0.0000e+00\n",
      "Epoch 626/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0205 - acc: 0.0267 - val_loss: 0.0213 - val_acc: 0.0000e+00\n",
      "Epoch 627/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0203 - acc: 0.0283 - val_loss: 0.0219 - val_acc: 0.0000e+00\n",
      "Epoch 628/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0207 - acc: 0.0283 - val_loss: 0.0222 - val_acc: 0.0200\n",
      "Epoch 629/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0212 - acc: 0.0350 - val_loss: 0.0229 - val_acc: 0.0333\n",
      "Epoch 630/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0212 - acc: 0.0350 - val_loss: 0.0230 - val_acc: 0.0200\n",
      "Epoch 631/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0216 - acc: 0.0433 - val_loss: 0.0239 - val_acc: 0.0267\n",
      "Epoch 632/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0211 - acc: 0.0317 - val_loss: 0.0238 - val_acc: 0.0267\n",
      "Epoch 633/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0218 - acc: 0.0550 - val_loss: 0.0257 - val_acc: 0.0133\n",
      "Epoch 634/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0233 - acc: 0.0433 - val_loss: 0.0274 - val_acc: 0.0333\n",
      "Epoch 635/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0247 - acc: 0.0700 - val_loss: 0.0315 - val_acc: 0.0267\n",
      "Epoch 636/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0262 - acc: 0.0883 - val_loss: 0.0296 - val_acc: 0.0333\n",
      "Epoch 637/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0263 - acc: 0.0467 - val_loss: 0.0258 - val_acc: 0.0200\n",
      "Epoch 638/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0255 - acc: 0.0433 - val_loss: 0.0227 - val_acc: 0.0133\n",
      "Epoch 639/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0237 - acc: 0.0300 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 640/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0219 - acc: 0.0400 - val_loss: 0.0210 - val_acc: 0.0000e+00\n",
      "Epoch 641/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0203 - acc: 0.0300 - val_loss: 0.0215 - val_acc: 0.0067\n",
      "Epoch 642/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0210 - acc: 0.0350 - val_loss: 0.0244 - val_acc: 0.0267\n",
      "Epoch 643/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0228 - acc: 0.0567 - val_loss: 0.0288 - val_acc: 0.0267\n",
      "Epoch 644/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0246 - acc: 0.0667 - val_loss: 0.0296 - val_acc: 0.0333\n",
      "Epoch 645/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0257 - acc: 0.0717 - val_loss: 0.0263 - val_acc: 0.0267\n",
      "Epoch 646/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0257 - acc: 0.0550 - val_loss: 0.0233 - val_acc: 0.0133\n",
      "Epoch 647/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0241 - acc: 0.0317 - val_loss: 0.0215 - val_acc: 0.0000e+00\n",
      "Epoch 648/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0225 - acc: 0.0333 - val_loss: 0.0216 - val_acc: 0.0000e+00\n",
      "Epoch 649/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0209 - acc: 0.0267 - val_loss: 0.0208 - val_acc: 0.0000e+00\n",
      "Epoch 650/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0199 - acc: 0.0283 - val_loss: 0.0215 - val_acc: 0.0067\n",
      "Epoch 651/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0214 - acc: 0.0433 - val_loss: 0.0259 - val_acc: 0.0333\n",
      "Epoch 652/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0249 - acc: 0.0683 - val_loss: 0.0340 - val_acc: 0.0333\n",
      "Epoch 653/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0288 - acc: 0.0850 - val_loss: 0.0366 - val_acc: 0.0333\n",
      "Epoch 654/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0315 - acc: 0.0867 - val_loss: 0.0306 - val_acc: 0.0333\n",
      "Epoch 655/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0324 - acc: 0.0700 - val_loss: 0.0238 - val_acc: 0.0333\n",
      "Epoch 656/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0281 - acc: 0.0467 - val_loss: 0.0258 - val_acc: 0.0000e+00\n",
      "Epoch 657/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0244 - acc: 0.0550 - val_loss: 0.0245 - val_acc: 0.0133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 658/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0228 - acc: 0.0567 - val_loss: 0.0216 - val_acc: 0.0000e+00\n",
      "Epoch 659/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0218 - acc: 0.0300 - val_loss: 0.0239 - val_acc: 0.0533\n",
      "Epoch 660/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0233 - acc: 0.0500 - val_loss: 0.0229 - val_acc: 0.0333\n",
      "Epoch 661/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0228 - acc: 0.0383 - val_loss: 0.0225 - val_acc: 0.0000e+00\n",
      "Epoch 662/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0216 - acc: 0.0367 - val_loss: 0.0217 - val_acc: 0.0000e+00\n",
      "Epoch 663/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0211 - acc: 0.0317 - val_loss: 0.0216 - val_acc: 0.0000e+00\n",
      "Epoch 664/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0210 - acc: 0.0317 - val_loss: 0.0219 - val_acc: 0.0000e+00\n",
      "Epoch 665/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0214 - acc: 0.0317 - val_loss: 0.0219 - val_acc: 0.0000e+00\n",
      "Epoch 666/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0216 - acc: 0.0333 - val_loss: 0.0216 - val_acc: 0.0000e+00\n",
      "Epoch 667/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0215 - acc: 0.0300 - val_loss: 0.0214 - val_acc: 0.0000e+00\n",
      "Epoch 668/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0213 - acc: 0.0317 - val_loss: 0.0214 - val_acc: 0.0000e+00\n",
      "Epoch 669/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0211 - acc: 0.0300 - val_loss: 0.0212 - val_acc: 0.0000e+00\n",
      "Epoch 670/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0210 - acc: 0.0283 - val_loss: 0.0212 - val_acc: 0.0000e+00\n",
      "Epoch 671/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0209 - acc: 0.0283 - val_loss: 0.0212 - val_acc: 0.0000e+00\n",
      "Epoch 672/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0209 - acc: 0.0283 - val_loss: 0.0213 - val_acc: 0.0000e+00\n",
      "Epoch 673/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0210 - acc: 0.0333 - val_loss: 0.0214 - val_acc: 0.0000e+00\n",
      "Epoch 674/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0212 - acc: 0.0333 - val_loss: 0.0216 - val_acc: 0.0000e+00\n",
      "Epoch 675/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0214 - acc: 0.0350 - val_loss: 0.0218 - val_acc: 0.0000e+00\n",
      "Epoch 676/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0217 - acc: 0.0350 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 677/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0221 - acc: 0.0350 - val_loss: 0.0225 - val_acc: 0.0000e+00\n",
      "Epoch 678/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0225 - acc: 0.0350 - val_loss: 0.0232 - val_acc: 0.0333\n",
      "Epoch 679/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0230 - acc: 0.0367 - val_loss: 0.0237 - val_acc: 0.0333\n",
      "Epoch 680/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0236 - acc: 0.0367 - val_loss: 0.0240 - val_acc: 0.0333\n",
      "Epoch 681/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0241 - acc: 0.0383 - val_loss: 0.0239 - val_acc: 0.0333\n",
      "Epoch 682/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0245 - acc: 0.0400 - val_loss: 0.0232 - val_acc: 0.0333\n",
      "Epoch 683/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0245 - acc: 0.0367 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
      "Epoch 684/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0239 - acc: 0.0367 - val_loss: 0.0218 - val_acc: 0.0000e+00\n",
      "Epoch 685/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0228 - acc: 0.0333 - val_loss: 0.0217 - val_acc: 0.0000e+00\n",
      "Epoch 686/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0217 - acc: 0.0367 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 687/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0210 - acc: 0.0350 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 688/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0206 - acc: 0.0333 - val_loss: 0.0213 - val_acc: 0.0000e+00\n",
      "Epoch 689/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0203 - acc: 0.0400 - val_loss: 0.0208 - val_acc: 0.0000e+00\n",
      "Epoch 690/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0203 - acc: 0.0383 - val_loss: 0.0210 - val_acc: 0.0000e+00\n",
      "Epoch 691/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0206 - acc: 0.0433 - val_loss: 0.0218 - val_acc: 0.0000e+00\n",
      "Epoch 692/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0210 - acc: 0.0383 - val_loss: 0.0222 - val_acc: 0.0000e+00\n",
      "Epoch 693/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0213 - acc: 0.0383 - val_loss: 0.0223 - val_acc: 0.0133\n",
      "Epoch 694/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0217 - acc: 0.0367 - val_loss: 0.0225 - val_acc: 0.0200\n",
      "Epoch 695/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0220 - acc: 0.0350 - val_loss: 0.0227 - val_acc: 0.0267\n",
      "Epoch 696/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0224 - acc: 0.0350 - val_loss: 0.0231 - val_acc: 0.0333\n",
      "Epoch 697/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0228 - acc: 0.0383 - val_loss: 0.0235 - val_acc: 0.0333\n",
      "Epoch 698/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0232 - acc: 0.0417 - val_loss: 0.0235 - val_acc: 0.0333\n",
      "Epoch 699/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0237 - acc: 0.0450 - val_loss: 0.0237 - val_acc: 0.0333\n",
      "Epoch 700/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0245 - acc: 0.0417 - val_loss: 0.0236 - val_acc: 0.0333\n",
      "Epoch 701/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0249 - acc: 0.0417 - val_loss: 0.0230 - val_acc: 0.0333\n",
      "Epoch 702/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0248 - acc: 0.0350 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 703/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0237 - acc: 0.0400 - val_loss: 0.0215 - val_acc: 0.0000e+00\n",
      "Epoch 704/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0222 - acc: 0.0350 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 705/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0211 - acc: 0.0433 - val_loss: 0.0227 - val_acc: 0.0000e+00\n",
      "Epoch 706/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0207 - acc: 0.0417 - val_loss: 0.0214 - val_acc: 0.0000e+00\n",
      "Epoch 707/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0203 - acc: 0.0450 - val_loss: 0.0205 - val_acc: 0.0000e+00\n",
      "Epoch 708/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.0417 - val_loss: 0.0208 - val_acc: 0.0000e+00\n",
      "Epoch 709/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0203 - acc: 0.0417 - val_loss: 0.0213 - val_acc: 0.0000e+00\n",
      "Epoch 710/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0206 - acc: 0.0417 - val_loss: 0.0215 - val_acc: 0.0000e+00\n",
      "Epoch 711/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0208 - acc: 0.0383 - val_loss: 0.0214 - val_acc: 0.0000e+00\n",
      "Epoch 712/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0209 - acc: 0.0383 - val_loss: 0.0213 - val_acc: 0.0000e+00\n",
      "Epoch 713/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0210 - acc: 0.0367 - val_loss: 0.0213 - val_acc: 0.0000e+00\n",
      "Epoch 714/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0210 - acc: 0.0367 - val_loss: 0.0213 - val_acc: 0.0000e+00\n",
      "Epoch 715/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0211 - acc: 0.0367 - val_loss: 0.0215 - val_acc: 0.0000e+00\n",
      "Epoch 716/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0213 - acc: 0.0367 - val_loss: 0.0221 - val_acc: 0.0133\n",
      "Epoch 717/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0216 - acc: 0.0383 - val_loss: 0.0225 - val_acc: 0.0200\n",
      "Epoch 718/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0218 - acc: 0.0383 - val_loss: 0.0229 - val_acc: 0.0200\n",
      "Epoch 719/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0223 - acc: 0.0367 - val_loss: 0.0232 - val_acc: 0.0333\n",
      "Epoch 720/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0230 - acc: 0.0467 - val_loss: 0.0247 - val_acc: 0.0333\n",
      "Epoch 721/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0237 - acc: 0.0467 - val_loss: 0.0267 - val_acc: 0.0333\n",
      "Epoch 722/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0251 - acc: 0.0550 - val_loss: 0.0288 - val_acc: 0.0333\n",
      "Epoch 723/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0270 - acc: 0.0733 - val_loss: 0.0289 - val_acc: 0.0333\n",
      "Epoch 724/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0287 - acc: 0.0750 - val_loss: 0.0268 - val_acc: 0.0333\n",
      "Epoch 725/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0293 - acc: 0.0700 - val_loss: 0.0227 - val_acc: 0.0333\n",
      "Epoch 726/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0272 - acc: 0.0467 - val_loss: 0.0225 - val_acc: 0.0000e+00\n",
      "Epoch 727/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0237 - acc: 0.0500 - val_loss: 0.0260 - val_acc: 0.0000e+00\n",
      "Epoch 728/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0227 - acc: 0.0767 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
      "Epoch 729/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0216 - acc: 0.0550 - val_loss: 0.0210 - val_acc: 0.0000e+00\n",
      "Epoch 730/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0215 - acc: 0.0600 - val_loss: 0.0228 - val_acc: 0.0200\n",
      "Epoch 731/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0222 - acc: 0.0667 - val_loss: 0.0215 - val_acc: 0.0000e+00\n",
      "Epoch 732/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0208 - acc: 0.0450 - val_loss: 0.0216 - val_acc: 0.0000e+00\n",
      "Epoch 733/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.0450 - val_loss: 0.0207 - val_acc: 0.0000e+00\n",
      "Epoch 734/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0200 - acc: 0.0400 - val_loss: 0.0206 - val_acc: 0.0000e+00\n",
      "Epoch 735/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0200 - acc: 0.0400 - val_loss: 0.0209 - val_acc: 0.0000e+00\n",
      "Epoch 736/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0202 - acc: 0.0383 - val_loss: 0.0210 - val_acc: 0.0000e+00\n",
      "Epoch 737/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0202 - acc: 0.0383 - val_loss: 0.0207 - val_acc: 0.0000e+00\n",
      "Epoch 738/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.0350 - val_loss: 0.0205 - val_acc: 0.0000e+00\n",
      "Epoch 739/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0200 - acc: 0.0367 - val_loss: 0.0205 - val_acc: 0.0000e+00\n",
      "Epoch 740/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0200 - acc: 0.0350 - val_loss: 0.0204 - val_acc: 0.0000e+00\n",
      "Epoch 741/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0200 - acc: 0.0350 - val_loss: 0.0204 - val_acc: 0.0000e+00\n",
      "Epoch 742/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0200 - acc: 0.0333 - val_loss: 0.0203 - val_acc: 0.0000e+00\n",
      "Epoch 743/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0200 - acc: 0.0333 - val_loss: 0.0203 - val_acc: 0.0000e+00\n",
      "Epoch 744/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0200 - acc: 0.0350 - val_loss: 0.0204 - val_acc: 0.0000e+00\n",
      "Epoch 745/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.0333 - val_loss: 0.0205 - val_acc: 0.0000e+00\n",
      "Epoch 746/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.0333 - val_loss: 0.0207 - val_acc: 0.0000e+00\n",
      "Epoch 747/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0202 - acc: 0.0333 - val_loss: 0.0210 - val_acc: 0.0000e+00\n",
      "Epoch 748/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0204 - acc: 0.0367 - val_loss: 0.0214 - val_acc: 0.0133\n",
      "Epoch 749/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0206 - acc: 0.0383 - val_loss: 0.0220 - val_acc: 0.0200\n",
      "Epoch 750/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0210 - acc: 0.0417 - val_loss: 0.0229 - val_acc: 0.0200\n",
      "Epoch 751/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0214 - acc: 0.0417 - val_loss: 0.0239 - val_acc: 0.0333\n",
      "Epoch 752/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0220 - acc: 0.0517 - val_loss: 0.0268 - val_acc: 0.0333\n",
      "Epoch 753/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0236 - acc: 0.0700 - val_loss: 0.0287 - val_acc: 0.0333\n",
      "Epoch 754/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0244 - acc: 0.0733 - val_loss: 0.0311 - val_acc: 0.0333\n",
      "Epoch 755/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0257 - acc: 0.0867 - val_loss: 0.0328 - val_acc: 0.0333\n",
      "Epoch 756/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0276 - acc: 0.0883 - val_loss: 0.0336 - val_acc: 0.0333\n",
      "Epoch 757/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0305 - acc: 0.0917 - val_loss: 0.0320 - val_acc: 0.0333\n",
      "Epoch 758/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0333 - acc: 0.0933 - val_loss: 0.0243 - val_acc: 0.0333\n",
      "Epoch 759/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0307 - acc: 0.0550 - val_loss: 0.0228 - val_acc: 0.0133\n",
      "Epoch 760/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0250 - acc: 0.0700 - val_loss: 0.0284 - val_acc: 0.0133\n",
      "Epoch 761/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0240 - acc: 0.1100 - val_loss: 0.0218 - val_acc: 0.0000e+00\n",
      "Epoch 762/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0220 - acc: 0.0567 - val_loss: 0.0221 - val_acc: 0.0133\n",
      "Epoch 763/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0223 - acc: 0.0683 - val_loss: 0.0227 - val_acc: 0.0267\n",
      "Epoch 764/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0224 - acc: 0.0633 - val_loss: 0.0225 - val_acc: 0.0000e+00\n",
      "Epoch 765/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0206 - acc: 0.0600 - val_loss: 0.0215 - val_acc: 0.0000e+00\n",
      "Epoch 766/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0203 - acc: 0.0533 - val_loss: 0.0204 - val_acc: 0.0000e+00\n",
      "Epoch 767/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0204 - acc: 0.0517 - val_loss: 0.0214 - val_acc: 0.0000e+00\n",
      "Epoch 768/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0206 - acc: 0.0517 - val_loss: 0.0208 - val_acc: 0.0000e+00\n",
      "Epoch 769/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0200 - acc: 0.0383 - val_loss: 0.0208 - val_acc: 0.0000e+00\n",
      "Epoch 770/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0196 - acc: 0.0400 - val_loss: 0.0205 - val_acc: 0.0000e+00\n",
      "Epoch 771/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0194 - acc: 0.0383 - val_loss: 0.0204 - val_acc: 0.0000e+00\n",
      "Epoch 772/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.0383 - val_loss: 0.0203 - val_acc: 0.0000e+00\n",
      "Epoch 773/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.0367 - val_loss: 0.0203 - val_acc: 0.0000e+00\n",
      "Epoch 774/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.0367 - val_loss: 0.0204 - val_acc: 0.0000e+00\n",
      "Epoch 775/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0196 - acc: 0.0367 - val_loss: 0.0203 - val_acc: 0.0000e+00\n",
      "Epoch 776/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0198 - acc: 0.0350 - val_loss: 0.0203 - val_acc: 0.0000e+00\n",
      "Epoch 777/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0200 - acc: 0.0367 - val_loss: 0.0203 - val_acc: 0.0000e+00\n",
      "Epoch 778/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0203 - acc: 0.0367 - val_loss: 0.0210 - val_acc: 0.0000e+00\n",
      "Epoch 779/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0209 - acc: 0.0317 - val_loss: 0.0204 - val_acc: 0.0000e+00\n",
      "Epoch 780/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0207 - acc: 0.0350 - val_loss: 0.0201 - val_acc: 0.0000e+00\n",
      "Epoch 781/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0205 - acc: 0.0350 - val_loss: 0.0207 - val_acc: 0.0000e+00\n",
      "Epoch 782/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0204 - acc: 0.0250 - val_loss: 0.0215 - val_acc: 0.0000e+00\n",
      "Epoch 783/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0206 - acc: 0.0233 - val_loss: 0.0207 - val_acc: 0.0000e+00\n",
      "Epoch 784/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0202 - acc: 0.0367 - val_loss: 0.0201 - val_acc: 0.0000e+00\n",
      "Epoch 785/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0202 - acc: 0.0417 - val_loss: 0.0210 - val_acc: 0.0000e+00\n",
      "Epoch 786/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.0417 - val_loss: 0.0213 - val_acc: 0.0200\n",
      "Epoch 787/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0204 - acc: 0.0500 - val_loss: 0.0221 - val_acc: 0.0200\n",
      "Epoch 788/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0208 - acc: 0.0617 - val_loss: 0.0231 - val_acc: 0.0200\n",
      "Epoch 789/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0214 - acc: 0.0683 - val_loss: 0.0256 - val_acc: 0.0333\n",
      "Epoch 790/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0226 - acc: 0.0833 - val_loss: 0.0276 - val_acc: 0.0333\n",
      "Epoch 791/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0239 - acc: 0.1000 - val_loss: 0.0302 - val_acc: 0.0333\n",
      "Epoch 792/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0249 - acc: 0.0900 - val_loss: 0.0319 - val_acc: 0.0333\n",
      "Epoch 793/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0271 - acc: 0.0883 - val_loss: 0.0330 - val_acc: 0.0333\n",
      "Epoch 794/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0300 - acc: 0.1017 - val_loss: 0.0272 - val_acc: 0.0333\n",
      "Epoch 795/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0303 - acc: 0.0950 - val_loss: 0.0213 - val_acc: 0.0000e+00\n",
      "Epoch 796/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0242 - acc: 0.0600 - val_loss: 0.0254 - val_acc: 0.0000e+00\n",
      "Epoch 797/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0221 - acc: 0.0933 - val_loss: 0.0218 - val_acc: 0.0000e+00\n",
      "Epoch 798/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0214 - acc: 0.0600 - val_loss: 0.0212 - val_acc: 0.0000e+00\n",
      "Epoch 799/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0217 - acc: 0.0817 - val_loss: 0.0223 - val_acc: 0.0200\n",
      "Epoch 800/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0221 - acc: 0.0833 - val_loss: 0.0213 - val_acc: 0.0000e+00\n",
      "Epoch 801/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0203 - acc: 0.0450 - val_loss: 0.0215 - val_acc: 0.0000e+00\n",
      "Epoch 802/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0198 - acc: 0.0550 - val_loss: 0.0198 - val_acc: 0.0000e+00\n",
      "Epoch 803/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0198 - acc: 0.0567 - val_loss: 0.0208 - val_acc: 0.0000e+00\n",
      "Epoch 804/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.0650 - val_loss: 0.0207 - val_acc: 0.0000e+00\n",
      "Epoch 805/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.0567 - val_loss: 0.0204 - val_acc: 0.0000e+00\n",
      "Epoch 806/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0197 - acc: 0.0500 - val_loss: 0.0201 - val_acc: 0.0000e+00\n",
      "Epoch 807/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.0450 - val_loss: 0.0199 - val_acc: 0.0000e+00\n",
      "Epoch 808/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0192 - acc: 0.0433 - val_loss: 0.0200 - val_acc: 0.0000e+00\n",
      "Epoch 809/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0191 - acc: 0.0500 - val_loss: 0.0193 - val_acc: 0.0000e+00\n",
      "Epoch 810/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0191 - acc: 0.0483 - val_loss: 0.0199 - val_acc: 0.0000e+00\n",
      "Epoch 811/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0192 - acc: 0.0367 - val_loss: 0.0203 - val_acc: 0.0000e+00\n",
      "Epoch 812/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0194 - acc: 0.0400 - val_loss: 0.0202 - val_acc: 0.0000e+00\n",
      "Epoch 813/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0198 - acc: 0.0450 - val_loss: 0.0206 - val_acc: 0.0000e+00\n",
      "Epoch 814/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.0400 - val_loss: 0.0211 - val_acc: 0.0133\n",
      "Epoch 815/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0206 - acc: 0.0417 - val_loss: 0.0217 - val_acc: 0.0200\n",
      "Epoch 816/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0212 - acc: 0.0483 - val_loss: 0.0221 - val_acc: 0.0267\n",
      "Epoch 817/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0219 - acc: 0.0567 - val_loss: 0.0230 - val_acc: 0.0200\n",
      "Epoch 818/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0227 - acc: 0.0583 - val_loss: 0.0238 - val_acc: 0.0200\n",
      "Epoch 819/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0233 - acc: 0.0633 - val_loss: 0.0236 - val_acc: 0.0267\n",
      "Epoch 820/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0240 - acc: 0.0700 - val_loss: 0.0231 - val_acc: 0.0267\n",
      "Epoch 821/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0242 - acc: 0.0650 - val_loss: 0.0218 - val_acc: 0.0200\n",
      "Epoch 822/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0238 - acc: 0.0550 - val_loss: 0.0206 - val_acc: 0.0000e+00\n",
      "Epoch 823/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0226 - acc: 0.0567 - val_loss: 0.0205 - val_acc: 0.0000e+00\n",
      "Epoch 824/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0212 - acc: 0.0617 - val_loss: 0.0214 - val_acc: 0.0000e+00\n",
      "Epoch 825/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0208 - acc: 0.0700 - val_loss: 0.0233 - val_acc: 0.0000e+00\n",
      "Epoch 826/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0208 - acc: 0.0667 - val_loss: 0.0215 - val_acc: 0.0000e+00\n",
      "Epoch 827/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0202 - acc: 0.0467 - val_loss: 0.0198 - val_acc: 0.0000e+00\n",
      "Epoch 828/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.0567 - val_loss: 0.0198 - val_acc: 0.0000e+00\n",
      "Epoch 829/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0196 - acc: 0.0767 - val_loss: 0.0205 - val_acc: 0.0000e+00\n",
      "Epoch 830/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0197 - acc: 0.0717 - val_loss: 0.0202 - val_acc: 0.0000e+00\n",
      "Epoch 831/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0194 - acc: 0.0567 - val_loss: 0.0198 - val_acc: 0.0000e+00\n",
      "Epoch 832/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0192 - acc: 0.0517 - val_loss: 0.0197 - val_acc: 0.0000e+00\n",
      "Epoch 833/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0192 - acc: 0.0500 - val_loss: 0.0198 - val_acc: 0.0000e+00\n",
      "Epoch 834/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.0450 - val_loss: 0.0201 - val_acc: 0.0000e+00\n",
      "Epoch 835/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.0383 - val_loss: 0.0207 - val_acc: 0.0000e+00\n",
      "Epoch 836/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.0483 - val_loss: 0.0209 - val_acc: 0.0000e+00\n",
      "Epoch 837/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.0483 - val_loss: 0.0208 - val_acc: 0.0000e+00\n",
      "Epoch 838/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0191 - acc: 0.0500 - val_loss: 0.0205 - val_acc: 0.0000e+00\n",
      "Epoch 839/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.0483 - val_loss: 0.0196 - val_acc: 0.0000e+00\n",
      "Epoch 840/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0187 - acc: 0.0583 - val_loss: 0.0191 - val_acc: 0.0000e+00\n",
      "Epoch 841/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0188 - acc: 0.0617 - val_loss: 0.0193 - val_acc: 0.0000e+00\n",
      "Epoch 842/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0191 - acc: 0.0683 - val_loss: 0.0198 - val_acc: 0.0200\n",
      "Epoch 843/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0196 - acc: 0.0783 - val_loss: 0.0211 - val_acc: 0.0200\n",
      "Epoch 844/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0203 - acc: 0.0800 - val_loss: 0.0226 - val_acc: 0.0200\n",
      "Epoch 845/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0211 - acc: 0.0933 - val_loss: 0.0243 - val_acc: 0.0200\n",
      "Epoch 846/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0219 - acc: 0.0917 - val_loss: 0.0262 - val_acc: 0.0267\n",
      "Epoch 847/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0225 - acc: 0.0900 - val_loss: 0.0277 - val_acc: 0.0333\n",
      "Epoch 848/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0233 - acc: 0.0883 - val_loss: 0.0292 - val_acc: 0.0400\n",
      "Epoch 849/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0244 - acc: 0.0867 - val_loss: 0.0301 - val_acc: 0.0333\n",
      "Epoch 850/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0261 - acc: 0.0850 - val_loss: 0.0299 - val_acc: 0.0333\n",
      "Epoch 851/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0284 - acc: 0.0917 - val_loss: 0.0268 - val_acc: 0.0333\n",
      "Epoch 852/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0297 - acc: 0.0850 - val_loss: 0.0214 - val_acc: 0.0200\n",
      "Epoch 853/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0264 - acc: 0.0667 - val_loss: 0.0219 - val_acc: 0.0000e+00\n",
      "Epoch 854/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0232 - acc: 0.0867 - val_loss: 0.0264 - val_acc: 0.0133\n",
      "Epoch 855/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0230 - acc: 0.0950 - val_loss: 0.0207 - val_acc: 0.0000e+00\n",
      "Epoch 856/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0210 - acc: 0.0533 - val_loss: 0.0211 - val_acc: 0.0200\n",
      "Epoch 857/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0214 - acc: 0.0850 - val_loss: 0.0216 - val_acc: 0.0200\n",
      "Epoch 858/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0212 - acc: 0.0783 - val_loss: 0.0211 - val_acc: 0.0000e+00\n",
      "Epoch 859/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.0633 - val_loss: 0.0206 - val_acc: 0.0000e+00\n",
      "Epoch 860/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.0667 - val_loss: 0.0195 - val_acc: 0.0000e+00\n",
      "Epoch 861/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0191 - acc: 0.0633 - val_loss: 0.0200 - val_acc: 0.0000e+00\n",
      "Epoch 862/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.0733 - val_loss: 0.0199 - val_acc: 0.0000e+00\n",
      "Epoch 863/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0192 - acc: 0.0600 - val_loss: 0.0197 - val_acc: 0.0000e+00\n",
      "Epoch 864/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0190 - acc: 0.0567 - val_loss: 0.0195 - val_acc: 0.0000e+00\n",
      "Epoch 865/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0190 - acc: 0.0500 - val_loss: 0.0197 - val_acc: 0.0000e+00\n",
      "Epoch 866/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0192 - acc: 0.0550 - val_loss: 0.0200 - val_acc: 0.0000e+00\n",
      "Epoch 867/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0194 - acc: 0.0483 - val_loss: 0.0204 - val_acc: 0.0000e+00\n",
      "Epoch 868/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0194 - acc: 0.0500 - val_loss: 0.0207 - val_acc: 0.0000e+00\n",
      "Epoch 869/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0192 - acc: 0.0550 - val_loss: 0.0203 - val_acc: 0.0000e+00\n",
      "Epoch 870/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0189 - acc: 0.0533 - val_loss: 0.0197 - val_acc: 0.0000e+00\n",
      "Epoch 871/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0185 - acc: 0.0567 - val_loss: 0.0191 - val_acc: 0.0000e+00\n",
      "Epoch 872/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0184 - acc: 0.0567 - val_loss: 0.0191 - val_acc: 0.0000e+00\n",
      "Epoch 873/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0184 - acc: 0.0650 - val_loss: 0.0190 - val_acc: 0.0000e+00\n",
      "Epoch 874/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0185 - acc: 0.0650 - val_loss: 0.0193 - val_acc: 0.0000e+00\n",
      "Epoch 875/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0187 - acc: 0.0550 - val_loss: 0.0195 - val_acc: 0.0000e+00\n",
      "Epoch 876/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0190 - acc: 0.0533 - val_loss: 0.0195 - val_acc: 0.0200\n",
      "Epoch 877/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.0517 - val_loss: 0.0199 - val_acc: 0.0200\n",
      "Epoch 878/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.0533 - val_loss: 0.0204 - val_acc: 0.0200\n",
      "Epoch 879/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0208 - acc: 0.0533 - val_loss: 0.0210 - val_acc: 0.0200\n",
      "Epoch 880/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0215 - acc: 0.0533 - val_loss: 0.0217 - val_acc: 0.0200\n",
      "Epoch 881/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0219 - acc: 0.0600 - val_loss: 0.0219 - val_acc: 0.0200\n",
      "Epoch 882/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0223 - acc: 0.0700 - val_loss: 0.0216 - val_acc: 0.0200\n",
      "Epoch 883/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0225 - acc: 0.0650 - val_loss: 0.0209 - val_acc: 0.0200\n",
      "Epoch 884/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0220 - acc: 0.0567 - val_loss: 0.0201 - val_acc: 0.0200\n",
      "Epoch 885/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0213 - acc: 0.0600 - val_loss: 0.0197 - val_acc: 0.0000e+00\n",
      "Epoch 886/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0205 - acc: 0.0600 - val_loss: 0.0199 - val_acc: 0.0000e+00\n",
      "Epoch 887/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.0683 - val_loss: 0.0215 - val_acc: 0.0000e+00\n",
      "Epoch 888/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0206 - acc: 0.0767 - val_loss: 0.0240 - val_acc: 0.0000e+00\n",
      "Epoch 889/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0215 - acc: 0.0683 - val_loss: 0.0243 - val_acc: 0.0000e+00\n",
      "Epoch 890/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0212 - acc: 0.0633 - val_loss: 0.0204 - val_acc: 0.0000e+00\n",
      "Epoch 891/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.0583 - val_loss: 0.0194 - val_acc: 0.0000e+00\n",
      "Epoch 892/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0204 - acc: 0.0883 - val_loss: 0.0211 - val_acc: 0.0200\n",
      "Epoch 893/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0208 - acc: 0.1050 - val_loss: 0.0208 - val_acc: 0.0200\n",
      "Epoch 894/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0202 - acc: 0.0850 - val_loss: 0.0199 - val_acc: 0.0200\n",
      "Epoch 895/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.0800 - val_loss: 0.0194 - val_acc: 0.0000e+00\n",
      "Epoch 896/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0185 - acc: 0.0733 - val_loss: 0.0196 - val_acc: 0.0000e+00\n",
      "Epoch 897/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0186 - acc: 0.0650 - val_loss: 0.0199 - val_acc: 0.0000e+00\n",
      "Epoch 898/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0191 - acc: 0.0583 - val_loss: 0.0206 - val_acc: 0.0000e+00\n",
      "Epoch 899/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.0567 - val_loss: 0.0214 - val_acc: 0.0000e+00\n",
      "Epoch 900/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0198 - acc: 0.0550 - val_loss: 0.0222 - val_acc: 0.0000e+00\n",
      "Epoch 901/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0200 - acc: 0.0600 - val_loss: 0.0217 - val_acc: 0.0000e+00\n",
      "Epoch 902/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0197 - acc: 0.0567 - val_loss: 0.0196 - val_acc: 0.0000e+00\n",
      "Epoch 903/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.0617 - val_loss: 0.0187 - val_acc: 0.0000e+00\n",
      "Epoch 904/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0198 - acc: 0.0917 - val_loss: 0.0199 - val_acc: 0.0200\n",
      "Epoch 905/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.1067 - val_loss: 0.0210 - val_acc: 0.0200\n",
      "Epoch 906/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.1033 - val_loss: 0.0208 - val_acc: 0.0200\n",
      "Epoch 907/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0198 - acc: 0.0883 - val_loss: 0.0199 - val_acc: 0.0200\n",
      "Epoch 908/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0196 - acc: 0.0817 - val_loss: 0.0192 - val_acc: 0.0200\n",
      "Epoch 909/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0199 - acc: 0.0700 - val_loss: 0.0193 - val_acc: 0.0000e+00\n",
      "Epoch 910/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0205 - acc: 0.0567 - val_loss: 0.0192 - val_acc: 0.0000e+00\n",
      "Epoch 911/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0210 - acc: 0.0550 - val_loss: 0.0193 - val_acc: 0.0000e+00\n",
      "Epoch 912/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0209 - acc: 0.0633 - val_loss: 0.0198 - val_acc: 0.0000e+00\n",
      "Epoch 913/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0210 - acc: 0.0600 - val_loss: 0.0218 - val_acc: 0.0000e+00\n",
      "Epoch 914/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0214 - acc: 0.0750 - val_loss: 0.0251 - val_acc: 0.0000e+00\n",
      "Epoch 915/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0223 - acc: 0.0700 - val_loss: 0.0252 - val_acc: 0.0000e+00\n",
      "Epoch 916/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0219 - acc: 0.0600 - val_loss: 0.0196 - val_acc: 0.0000e+00\n",
      "Epoch 917/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0209 - acc: 0.0650 - val_loss: 0.0201 - val_acc: 0.0200\n",
      "Epoch 918/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0217 - acc: 0.1133 - val_loss: 0.0221 - val_acc: 0.0200\n",
      "Epoch 919/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0215 - acc: 0.0883 - val_loss: 0.0203 - val_acc: 0.0200\n",
      "Epoch 920/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.0967 - val_loss: 0.0197 - val_acc: 0.0000e+00\n",
      "Epoch 921/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0184 - acc: 0.0867 - val_loss: 0.0197 - val_acc: 0.0000e+00\n",
      "Epoch 922/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0183 - acc: 0.0833 - val_loss: 0.0191 - val_acc: 0.0000e+00\n",
      "Epoch 923/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0181 - acc: 0.0800 - val_loss: 0.0186 - val_acc: 0.0000e+00\n",
      "Epoch 924/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0180 - acc: 0.0733 - val_loss: 0.0186 - val_acc: 0.0000e+00\n",
      "Epoch 925/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0179 - acc: 0.0783 - val_loss: 0.0186 - val_acc: 0.0000e+00\n",
      "Epoch 926/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0180 - acc: 0.0733 - val_loss: 0.0186 - val_acc: 0.0000e+00\n",
      "Epoch 927/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0181 - acc: 0.0717 - val_loss: 0.0186 - val_acc: 0.0067\n",
      "Epoch 928/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0185 - acc: 0.0583 - val_loss: 0.0187 - val_acc: 0.0000e+00\n",
      "Epoch 929/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.0583 - val_loss: 0.0187 - val_acc: 0.0000e+00\n",
      "Epoch 930/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.0550 - val_loss: 0.0187 - val_acc: 0.0000e+00\n",
      "Epoch 931/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0194 - acc: 0.0567 - val_loss: 0.0188 - val_acc: 0.0000e+00\n",
      "Epoch 932/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.0583 - val_loss: 0.0187 - val_acc: 0.0000e+00\n",
      "Epoch 933/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0191 - acc: 0.0650 - val_loss: 0.0188 - val_acc: 0.0000e+00\n",
      "Epoch 934/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0190 - acc: 0.0717 - val_loss: 0.0189 - val_acc: 0.0000e+00\n",
      "Epoch 935/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0191 - acc: 0.0683 - val_loss: 0.0192 - val_acc: 0.0000e+00\n",
      "Epoch 936/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0191 - acc: 0.0750 - val_loss: 0.0200 - val_acc: 0.0000e+00\n",
      "Epoch 937/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0194 - acc: 0.0733 - val_loss: 0.0216 - val_acc: 0.0000e+00\n",
      "Epoch 938/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.0700 - val_loss: 0.0243 - val_acc: 0.0000e+00\n",
      "Epoch 939/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0211 - acc: 0.0717 - val_loss: 0.0254 - val_acc: 0.0000e+00\n",
      "Epoch 940/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0216 - acc: 0.0667 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
      "Epoch 941/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0208 - acc: 0.0767 - val_loss: 0.0187 - val_acc: 0.0000e+00\n",
      "Epoch 942/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0211 - acc: 0.0950 - val_loss: 0.0198 - val_acc: 0.0200\n",
      "Epoch 943/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0219 - acc: 0.1217 - val_loss: 0.0221 - val_acc: 0.0200\n",
      "Epoch 944/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0219 - acc: 0.1217 - val_loss: 0.0220 - val_acc: 0.0200\n",
      "Epoch 945/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0209 - acc: 0.1050 - val_loss: 0.0200 - val_acc: 0.0200\n",
      "Epoch 946/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0192 - acc: 0.1117 - val_loss: 0.0189 - val_acc: 0.0200\n",
      "Epoch 947/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0182 - acc: 0.1000 - val_loss: 0.0189 - val_acc: 0.0000e+00\n",
      "Epoch 948/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0179 - acc: 0.0867 - val_loss: 0.0189 - val_acc: 0.0000e+00\n",
      "Epoch 949/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0180 - acc: 0.0783 - val_loss: 0.0191 - val_acc: 0.0000e+00\n",
      "Epoch 950/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0182 - acc: 0.0800 - val_loss: 0.0195 - val_acc: 0.0000e+00\n",
      "Epoch 951/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0185 - acc: 0.0700 - val_loss: 0.0200 - val_acc: 0.0000e+00\n",
      "Epoch 952/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0187 - acc: 0.0667 - val_loss: 0.0205 - val_acc: 0.0000e+00\n",
      "Epoch 953/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0188 - acc: 0.0667 - val_loss: 0.0211 - val_acc: 0.0000e+00\n",
      "Epoch 954/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0188 - acc: 0.0700 - val_loss: 0.0211 - val_acc: 0.0000e+00\n",
      "Epoch 955/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0188 - acc: 0.0733 - val_loss: 0.0197 - val_acc: 0.0000e+00\n",
      "Epoch 956/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0191 - acc: 0.0833 - val_loss: 0.0182 - val_acc: 0.0000e+00\n",
      "Epoch 957/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0202 - acc: 0.0900 - val_loss: 0.0179 - val_acc: 0.0000e+00\n",
      "Epoch 958/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0215 - acc: 0.1083 - val_loss: 0.0188 - val_acc: 0.0200\n",
      "Epoch 959/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0221 - acc: 0.1233 - val_loss: 0.0208 - val_acc: 0.0200\n",
      "Epoch 960/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0225 - acc: 0.1317 - val_loss: 0.0239 - val_acc: 0.0200\n",
      "Epoch 961/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0230 - acc: 0.1333 - val_loss: 0.0265 - val_acc: 0.0200\n",
      "Epoch 962/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0234 - acc: 0.1317 - val_loss: 0.0288 - val_acc: 0.0333\n",
      "Epoch 963/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0246 - acc: 0.1117 - val_loss: 0.0279 - val_acc: 0.0267\n",
      "Epoch 964/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0272 - acc: 0.1033 - val_loss: 0.0238 - val_acc: 0.0200\n",
      "Epoch 965/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0277 - acc: 0.0867 - val_loss: 0.0200 - val_acc: 0.0333\n",
      "Epoch 966/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0259 - acc: 0.0850 - val_loss: 0.0284 - val_acc: 0.0133\n",
      "Epoch 967/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0290 - acc: 0.1000 - val_loss: 0.0345 - val_acc: 0.0267\n",
      "Epoch 968/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0280 - acc: 0.0967 - val_loss: 0.0190 - val_acc: 0.0133\n",
      "Epoch 969/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0235 - acc: 0.0933 - val_loss: 0.0258 - val_acc: 0.0600\n",
      "Epoch 970/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0249 - acc: 0.1050 - val_loss: 0.0218 - val_acc: 0.0200\n",
      "Epoch 971/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0207 - acc: 0.0950 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
      "Epoch 972/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0204 - acc: 0.0717 - val_loss: 0.0193 - val_acc: 0.0200\n",
      "Epoch 973/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0207 - acc: 0.0950 - val_loss: 0.0207 - val_acc: 0.0200\n",
      "Epoch 974/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.1033 - val_loss: 0.0208 - val_acc: 0.0000e+00\n",
      "Epoch 975/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.0833 - val_loss: 0.0195 - val_acc: 0.0000e+00\n",
      "Epoch 976/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0187 - acc: 0.0750 - val_loss: 0.0192 - val_acc: 0.0000e+00\n",
      "Epoch 977/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0187 - acc: 0.0983 - val_loss: 0.0195 - val_acc: 0.0200\n",
      "Epoch 978/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0186 - acc: 0.0983 - val_loss: 0.0193 - val_acc: 0.0200\n",
      "Epoch 979/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0182 - acc: 0.0900 - val_loss: 0.0192 - val_acc: 0.0000e+00\n",
      "Epoch 980/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0181 - acc: 0.0733 - val_loss: 0.0194 - val_acc: 0.0000e+00\n",
      "Epoch 981/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0184 - acc: 0.0800 - val_loss: 0.0198 - val_acc: 0.0000e+00\n",
      "Epoch 982/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.0767 - val_loss: 0.0205 - val_acc: 0.0000e+00\n",
      "Epoch 983/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0196 - acc: 0.0717 - val_loss: 0.0216 - val_acc: 0.0000e+00\n",
      "Epoch 984/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0202 - acc: 0.0750 - val_loss: 0.0225 - val_acc: 0.0000e+00\n",
      "Epoch 985/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0203 - acc: 0.0733 - val_loss: 0.0214 - val_acc: 0.0000e+00\n",
      "Epoch 986/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.0683 - val_loss: 0.0190 - val_acc: 0.0000e+00\n",
      "Epoch 987/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0186 - acc: 0.0683 - val_loss: 0.0190 - val_acc: 0.0200\n",
      "Epoch 988/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.1150 - val_loss: 0.0197 - val_acc: 0.0200\n",
      "Epoch 989/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0190 - acc: 0.1133 - val_loss: 0.0194 - val_acc: 0.0200\n",
      "Epoch 990/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0185 - acc: 0.1100 - val_loss: 0.0189 - val_acc: 0.0200\n",
      "Epoch 991/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0179 - acc: 0.1067 - val_loss: 0.0188 - val_acc: 0.0000e+00\n",
      "Epoch 992/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0178 - acc: 0.0967 - val_loss: 0.0189 - val_acc: 0.0000e+00\n",
      "Epoch 993/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0179 - acc: 0.0917 - val_loss: 0.0191 - val_acc: 0.0000e+00\n",
      "Epoch 994/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0183 - acc: 0.0933 - val_loss: 0.0195 - val_acc: 0.0000e+00\n",
      "Epoch 995/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.0917 - val_loss: 0.0202 - val_acc: 0.0000e+00\n",
      "Epoch 996/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.0900 - val_loss: 0.0214 - val_acc: 0.0000e+00\n",
      "Epoch 997/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0202 - acc: 0.0850 - val_loss: 0.0233 - val_acc: 0.0000e+00\n",
      "Epoch 998/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0212 - acc: 0.0783 - val_loss: 0.0250 - val_acc: 0.0000e+00\n",
      "Epoch 999/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0216 - acc: 0.0683 - val_loss: 0.0217 - val_acc: 0.0000e+00\n",
      "Epoch 1000/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0202 - acc: 0.0717 - val_loss: 0.0185 - val_acc: 0.0000e+00\n",
      "Epoch 1001/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0198 - acc: 0.0933 - val_loss: 0.0205 - val_acc: 0.0400\n",
      "Epoch 1002/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0206 - acc: 0.1300 - val_loss: 0.0208 - val_acc: 0.0200\n",
      "Epoch 1003/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0199 - acc: 0.1200 - val_loss: 0.0192 - val_acc: 0.0200\n",
      "Epoch 1004/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0183 - acc: 0.1317 - val_loss: 0.0190 - val_acc: 0.0200\n",
      "Epoch 1005/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.1167 - val_loss: 0.0190 - val_acc: 0.0000e+00\n",
      "Epoch 1006/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0176 - acc: 0.1100 - val_loss: 0.0184 - val_acc: 0.0000e+00\n",
      "Epoch 1007/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0175 - acc: 0.0983 - val_loss: 0.0183 - val_acc: 0.0200\n",
      "Epoch 1008/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0175 - acc: 0.0983 - val_loss: 0.0183 - val_acc: 0.0200\n",
      "Epoch 1009/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0175 - acc: 0.1033 - val_loss: 0.0185 - val_acc: 0.0200\n",
      "Epoch 1010/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.0983 - val_loss: 0.0187 - val_acc: 0.0200\n",
      "Epoch 1011/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0179 - acc: 0.1000 - val_loss: 0.0191 - val_acc: 0.0200\n",
      "Epoch 1012/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0182 - acc: 0.1000 - val_loss: 0.0197 - val_acc: 0.0200\n",
      "Epoch 1013/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.1017 - val_loss: 0.0205 - val_acc: 0.0200\n",
      "Epoch 1014/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.1000 - val_loss: 0.0220 - val_acc: 0.0267\n",
      "Epoch 1015/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0209 - acc: 0.0967 - val_loss: 0.0226 - val_acc: 0.0400\n",
      "Epoch 1016/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0214 - acc: 0.1217 - val_loss: 0.0225 - val_acc: 0.0200\n",
      "Epoch 1017/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0209 - acc: 0.1233 - val_loss: 0.0219 - val_acc: 0.0200\n",
      "Epoch 1018/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0207 - acc: 0.1183 - val_loss: 0.0217 - val_acc: 0.0200\n",
      "Epoch 1019/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0207 - acc: 0.1033 - val_loss: 0.0221 - val_acc: 0.0200\n",
      "Epoch 1020/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0219 - acc: 0.1017 - val_loss: 0.0228 - val_acc: 0.0400\n",
      "Epoch 1021/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0234 - acc: 0.0983 - val_loss: 0.0219 - val_acc: 0.0400\n",
      "Epoch 1022/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0246 - acc: 0.0983 - val_loss: 0.0203 - val_acc: 0.0200\n",
      "Epoch 1023/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0240 - acc: 0.0817 - val_loss: 0.0193 - val_acc: 0.0333\n",
      "Epoch 1024/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0232 - acc: 0.0850 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
      "Epoch 1025/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0238 - acc: 0.1067 - val_loss: 0.0296 - val_acc: 0.0133\n",
      "Epoch 1026/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0251 - acc: 0.0933 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 1027/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0215 - acc: 0.0700 - val_loss: 0.0194 - val_acc: 0.0200\n",
      "Epoch 1028/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0215 - acc: 0.1150 - val_loss: 0.0215 - val_acc: 0.0200\n",
      "Epoch 1029/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0215 - acc: 0.1083 - val_loss: 0.0203 - val_acc: 0.0200\n",
      "Epoch 1030/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0188 - acc: 0.1217 - val_loss: 0.0206 - val_acc: 0.0000e+00\n",
      "Epoch 1031/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0187 - acc: 0.1050 - val_loss: 0.0186 - val_acc: 0.0000e+00\n",
      "Epoch 1032/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0186 - acc: 0.1083 - val_loss: 0.0190 - val_acc: 0.0200\n",
      "Epoch 1033/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0186 - acc: 0.1267 - val_loss: 0.0190 - val_acc: 0.0200\n",
      "Epoch 1034/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0183 - acc: 0.1183 - val_loss: 0.0188 - val_acc: 0.0200\n",
      "Epoch 1035/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0178 - acc: 0.1183 - val_loss: 0.0186 - val_acc: 0.0200\n",
      "Epoch 1036/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.1150 - val_loss: 0.0188 - val_acc: 0.0000e+00\n",
      "Epoch 1037/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0181 - acc: 0.1083 - val_loss: 0.0194 - val_acc: 0.0000e+00\n",
      "Epoch 1038/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0190 - acc: 0.1017 - val_loss: 0.0207 - val_acc: 0.0000e+00\n",
      "Epoch 1039/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0202 - acc: 0.0950 - val_loss: 0.0231 - val_acc: 0.0000e+00\n",
      "Epoch 1040/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0212 - acc: 0.0867 - val_loss: 0.0241 - val_acc: 0.0000e+00\n",
      "Epoch 1041/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0212 - acc: 0.0817 - val_loss: 0.0205 - val_acc: 0.0000e+00\n",
      "Epoch 1042/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.0800 - val_loss: 0.0183 - val_acc: 0.0200\n",
      "Epoch 1043/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.1217 - val_loss: 0.0198 - val_acc: 0.0200\n",
      "Epoch 1044/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0199 - acc: 0.1350 - val_loss: 0.0196 - val_acc: 0.0200\n",
      "Epoch 1045/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.1350 - val_loss: 0.0189 - val_acc: 0.0200\n",
      "Epoch 1046/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.1400 - val_loss: 0.0186 - val_acc: 0.0200\n",
      "Epoch 1047/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0174 - acc: 0.1283 - val_loss: 0.0186 - val_acc: 0.0000e+00\n",
      "Epoch 1048/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0173 - acc: 0.1117 - val_loss: 0.0182 - val_acc: 0.0000e+00\n",
      "Epoch 1049/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0172 - acc: 0.1050 - val_loss: 0.0181 - val_acc: 0.0200\n",
      "Epoch 1050/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0170 - acc: 0.1017 - val_loss: 0.0178 - val_acc: 0.0200\n",
      "Epoch 1051/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0171 - acc: 0.0983 - val_loss: 0.0179 - val_acc: 0.0200\n",
      "Epoch 1052/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0173 - acc: 0.0983 - val_loss: 0.0179 - val_acc: 0.0200\n",
      "Epoch 1053/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0176 - acc: 0.0983 - val_loss: 0.0179 - val_acc: 0.0200\n",
      "Epoch 1054/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0178 - acc: 0.0983 - val_loss: 0.0180 - val_acc: 0.0200\n",
      "Epoch 1055/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0182 - acc: 0.0983 - val_loss: 0.0181 - val_acc: 0.0200\n",
      "Epoch 1056/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0181 - acc: 0.1017 - val_loss: 0.0182 - val_acc: 0.0200\n",
      "Epoch 1057/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0182 - acc: 0.1000 - val_loss: 0.0190 - val_acc: 0.0200\n",
      "Epoch 1058/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0182 - acc: 0.1133 - val_loss: 0.0190 - val_acc: 0.0000e+00\n",
      "Epoch 1059/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0186 - acc: 0.1083 - val_loss: 0.0209 - val_acc: 0.0000e+00\n",
      "Epoch 1060/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.1083 - val_loss: 0.0228 - val_acc: 0.0000e+00\n",
      "Epoch 1061/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0206 - acc: 0.0917 - val_loss: 0.0261 - val_acc: 0.0000e+00\n",
      "Epoch 1062/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0220 - acc: 0.0883 - val_loss: 0.0256 - val_acc: 0.0000e+00\n",
      "Epoch 1063/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0216 - acc: 0.0850 - val_loss: 0.0197 - val_acc: 0.0000e+00\n",
      "Epoch 1064/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0209 - acc: 0.0867 - val_loss: 0.0187 - val_acc: 0.0200\n",
      "Epoch 1065/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0216 - acc: 0.1417 - val_loss: 0.0216 - val_acc: 0.0200\n",
      "Epoch 1066/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0216 - acc: 0.1367 - val_loss: 0.0203 - val_acc: 0.0200\n",
      "Epoch 1067/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0197 - acc: 0.1400 - val_loss: 0.0189 - val_acc: 0.0200\n",
      "Epoch 1068/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0176 - acc: 0.1550 - val_loss: 0.0187 - val_acc: 0.0200\n",
      "Epoch 1069/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0175 - acc: 0.1283 - val_loss: 0.0184 - val_acc: 0.0200\n",
      "Epoch 1070/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0175 - acc: 0.1083 - val_loss: 0.0177 - val_acc: 0.0200\n",
      "Epoch 1071/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0174 - acc: 0.1050 - val_loss: 0.0175 - val_acc: 0.0200\n",
      "Epoch 1072/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0174 - acc: 0.1167 - val_loss: 0.0180 - val_acc: 0.0200\n",
      "Epoch 1073/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0174 - acc: 0.1183 - val_loss: 0.0184 - val_acc: 0.0200\n",
      "Epoch 1074/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0176 - acc: 0.1200 - val_loss: 0.0188 - val_acc: 0.0200\n",
      "Epoch 1075/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0180 - acc: 0.1200 - val_loss: 0.0196 - val_acc: 0.0200\n",
      "Epoch 1076/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0186 - acc: 0.1267 - val_loss: 0.0209 - val_acc: 0.0400\n",
      "Epoch 1077/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.1300 - val_loss: 0.0225 - val_acc: 0.0267\n",
      "Epoch 1078/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0206 - acc: 0.1317 - val_loss: 0.0246 - val_acc: 0.0200\n",
      "Epoch 1079/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0220 - acc: 0.1283 - val_loss: 0.0270 - val_acc: 0.0200\n",
      "Epoch 1080/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0241 - acc: 0.1233 - val_loss: 0.0293 - val_acc: 0.0200\n",
      "Epoch 1081/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0276 - acc: 0.1067 - val_loss: 0.0292 - val_acc: 0.0333\n",
      "Epoch 1082/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0316 - acc: 0.0983 - val_loss: 0.0210 - val_acc: 0.0200\n",
      "Epoch 1083/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0275 - acc: 0.1133 - val_loss: 0.0207 - val_acc: 0.0200\n",
      "Epoch 1084/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0262 - acc: 0.1017 - val_loss: 0.0344 - val_acc: 0.0267\n",
      "Epoch 1085/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0292 - acc: 0.0867 - val_loss: 0.0192 - val_acc: 0.0000e+00\n",
      "Epoch 1086/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0228 - acc: 0.0700 - val_loss: 0.0231 - val_acc: 0.0400\n",
      "Epoch 1087/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0240 - acc: 0.0933 - val_loss: 0.0208 - val_acc: 0.0200\n",
      "Epoch 1088/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0202 - acc: 0.1217 - val_loss: 0.0227 - val_acc: 0.0000e+00\n",
      "Epoch 1089/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0196 - acc: 0.0817 - val_loss: 0.0182 - val_acc: 0.0200\n",
      "Epoch 1090/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0199 - acc: 0.1067 - val_loss: 0.0199 - val_acc: 0.0200\n",
      "Epoch 1091/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.1183 - val_loss: 0.0198 - val_acc: 0.0200\n",
      "Epoch 1092/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0180 - acc: 0.1317 - val_loss: 0.0191 - val_acc: 0.0000e+00\n",
      "Epoch 1093/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0178 - acc: 0.1033 - val_loss: 0.0182 - val_acc: 0.0200\n",
      "Epoch 1094/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0176 - acc: 0.1167 - val_loss: 0.0183 - val_acc: 0.0200\n",
      "Epoch 1095/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0175 - acc: 0.1283 - val_loss: 0.0184 - val_acc: 0.0200\n",
      "Epoch 1096/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0173 - acc: 0.1233 - val_loss: 0.0180 - val_acc: 0.0200\n",
      "Epoch 1097/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0173 - acc: 0.1100 - val_loss: 0.0181 - val_acc: 0.0200\n",
      "Epoch 1098/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0186 - acc: 0.1083 - val_loss: 0.0199 - val_acc: 0.0000e+00\n",
      "Epoch 1099/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0210 - acc: 0.1067 - val_loss: 0.0232 - val_acc: 0.0000e+00\n",
      "Epoch 1100/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0229 - acc: 0.0967 - val_loss: 0.0256 - val_acc: 0.0000e+00\n",
      "Epoch 1101/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0225 - acc: 0.0867 - val_loss: 0.0189 - val_acc: 0.0000e+00\n",
      "Epoch 1102/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0196 - acc: 0.0833 - val_loss: 0.0195 - val_acc: 0.0200\n",
      "Epoch 1103/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0200 - acc: 0.1383 - val_loss: 0.0197 - val_acc: 0.0400\n",
      "Epoch 1104/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.1450 - val_loss: 0.0196 - val_acc: 0.0200\n",
      "Epoch 1105/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0178 - acc: 0.1417 - val_loss: 0.0194 - val_acc: 0.0200\n",
      "Epoch 1106/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0179 - acc: 0.1250 - val_loss: 0.0183 - val_acc: 0.0200\n",
      "Epoch 1107/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.1267 - val_loss: 0.0180 - val_acc: 0.0200\n",
      "Epoch 1108/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0176 - acc: 0.1367 - val_loss: 0.0182 - val_acc: 0.0400\n",
      "Epoch 1109/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0174 - acc: 0.1367 - val_loss: 0.0181 - val_acc: 0.0200\n",
      "Epoch 1110/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0172 - acc: 0.1350 - val_loss: 0.0180 - val_acc: 0.0200\n",
      "Epoch 1111/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0175 - acc: 0.1183 - val_loss: 0.0188 - val_acc: 0.0200\n",
      "Epoch 1112/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.1167 - val_loss: 0.0211 - val_acc: 0.0000e+00\n",
      "Epoch 1113/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0208 - acc: 0.1050 - val_loss: 0.0246 - val_acc: 0.0000e+00\n",
      "Epoch 1114/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0223 - acc: 0.1000 - val_loss: 0.0230 - val_acc: 0.0000e+00\n",
      "Epoch 1115/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0206 - acc: 0.0867 - val_loss: 0.0180 - val_acc: 0.0200\n",
      "Epoch 1116/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.1133 - val_loss: 0.0193 - val_acc: 0.0400\n",
      "Epoch 1117/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0200 - acc: 0.1417 - val_loss: 0.0194 - val_acc: 0.0400\n",
      "Epoch 1118/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0190 - acc: 0.1467 - val_loss: 0.0192 - val_acc: 0.0200\n",
      "Epoch 1119/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0174 - acc: 0.1483 - val_loss: 0.0190 - val_acc: 0.0200\n",
      "Epoch 1120/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0175 - acc: 0.1233 - val_loss: 0.0184 - val_acc: 0.0200\n",
      "Epoch 1121/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0173 - acc: 0.1133 - val_loss: 0.0177 - val_acc: 0.0200\n",
      "Epoch 1122/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0171 - acc: 0.1267 - val_loss: 0.0175 - val_acc: 0.0200\n",
      "Epoch 1123/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0169 - acc: 0.1250 - val_loss: 0.0176 - val_acc: 0.0200\n",
      "Epoch 1124/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0169 - acc: 0.1250 - val_loss: 0.0176 - val_acc: 0.0200\n",
      "Epoch 1125/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0170 - acc: 0.1250 - val_loss: 0.0177 - val_acc: 0.0200\n",
      "Epoch 1126/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0175 - acc: 0.1167 - val_loss: 0.0181 - val_acc: 0.0200\n",
      "Epoch 1127/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0188 - acc: 0.1100 - val_loss: 0.0196 - val_acc: 0.0200\n",
      "Epoch 1128/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0199 - acc: 0.1117 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "Epoch 1129/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0216 - acc: 0.1050 - val_loss: 0.0277 - val_acc: 0.0000e+00\n",
      "Epoch 1130/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0232 - acc: 0.0867 - val_loss: 0.0208 - val_acc: 0.0000e+00\n",
      "Epoch 1131/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0204 - acc: 0.0900 - val_loss: 0.0179 - val_acc: 0.0200\n",
      "Epoch 1132/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0206 - acc: 0.1400 - val_loss: 0.0204 - val_acc: 0.0400\n",
      "Epoch 1133/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0207 - acc: 0.1383 - val_loss: 0.0193 - val_acc: 0.0200\n",
      "Epoch 1134/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0186 - acc: 0.1550 - val_loss: 0.0192 - val_acc: 0.0200\n",
      "Epoch 1135/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0174 - acc: 0.1483 - val_loss: 0.0187 - val_acc: 0.0200\n",
      "Epoch 1136/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.1233 - val_loss: 0.0178 - val_acc: 0.0200\n",
      "Epoch 1137/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.1283 - val_loss: 0.0177 - val_acc: 0.0200\n",
      "Epoch 1138/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.1367 - val_loss: 0.0182 - val_acc: 0.0200\n",
      "Epoch 1139/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0175 - acc: 0.1450 - val_loss: 0.0181 - val_acc: 0.0400\n",
      "Epoch 1140/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0174 - acc: 0.1367 - val_loss: 0.0179 - val_acc: 0.0200\n",
      "Epoch 1141/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0172 - acc: 0.1233 - val_loss: 0.0176 - val_acc: 0.0400\n",
      "Epoch 1142/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0173 - acc: 0.1200 - val_loss: 0.0174 - val_acc: 0.0200\n",
      "Epoch 1143/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.1100 - val_loss: 0.0176 - val_acc: 0.0200\n",
      "Epoch 1144/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0183 - acc: 0.1050 - val_loss: 0.0177 - val_acc: 0.0200\n",
      "Epoch 1145/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.1033 - val_loss: 0.0186 - val_acc: 0.0200\n",
      "Epoch 1146/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0194 - acc: 0.1133 - val_loss: 0.0209 - val_acc: 0.0000e+00\n",
      "Epoch 1147/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0205 - acc: 0.0967 - val_loss: 0.0260 - val_acc: 0.0000e+00\n",
      "Epoch 1148/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0228 - acc: 0.0900 - val_loss: 0.0253 - val_acc: 0.0000e+00\n",
      "Epoch 1149/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0220 - acc: 0.0750 - val_loss: 0.0180 - val_acc: 0.0200\n",
      "Epoch 1150/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0208 - acc: 0.1100 - val_loss: 0.0196 - val_acc: 0.0400\n",
      "Epoch 1151/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0215 - acc: 0.1500 - val_loss: 0.0206 - val_acc: 0.0400\n",
      "Epoch 1152/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0208 - acc: 0.1383 - val_loss: 0.0192 - val_acc: 0.0400\n",
      "Epoch 1153/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0178 - acc: 0.1650 - val_loss: 0.0192 - val_acc: 0.0200\n",
      "Epoch 1154/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0176 - acc: 0.1333 - val_loss: 0.0183 - val_acc: 0.0200\n",
      "Epoch 1155/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.1200 - val_loss: 0.0175 - val_acc: 0.0200\n",
      "Epoch 1156/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0175 - acc: 0.1283 - val_loss: 0.0175 - val_acc: 0.0200\n",
      "Epoch 1157/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0175 - acc: 0.1417 - val_loss: 0.0180 - val_acc: 0.0400\n",
      "Epoch 1158/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0174 - acc: 0.1400 - val_loss: 0.0181 - val_acc: 0.0400\n",
      "Epoch 1159/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0173 - acc: 0.1317 - val_loss: 0.0178 - val_acc: 0.0200\n",
      "Epoch 1160/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0173 - acc: 0.1250 - val_loss: 0.0175 - val_acc: 0.0200\n",
      "Epoch 1161/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0174 - acc: 0.1150 - val_loss: 0.0174 - val_acc: 0.0400\n",
      "Epoch 1162/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.1133 - val_loss: 0.0173 - val_acc: 0.0400\n",
      "Epoch 1163/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0181 - acc: 0.1017 - val_loss: 0.0174 - val_acc: 0.0200\n",
      "Epoch 1164/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0185 - acc: 0.1000 - val_loss: 0.0178 - val_acc: 0.0200\n",
      "Epoch 1165/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0187 - acc: 0.1083 - val_loss: 0.0192 - val_acc: 0.0200\n",
      "Epoch 1166/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.1100 - val_loss: 0.0230 - val_acc: 0.0000e+00\n",
      "Epoch 1167/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0215 - acc: 0.1000 - val_loss: 0.0280 - val_acc: 0.0000e+00\n",
      "Epoch 1168/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0234 - acc: 0.0833 - val_loss: 0.0210 - val_acc: 0.0000e+00\n",
      "Epoch 1169/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0206 - acc: 0.0950 - val_loss: 0.0176 - val_acc: 0.0400\n",
      "Epoch 1170/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0208 - acc: 0.1433 - val_loss: 0.0201 - val_acc: 0.0400\n",
      "Epoch 1171/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0208 - acc: 0.1433 - val_loss: 0.0195 - val_acc: 0.0400\n",
      "Epoch 1172/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0192 - acc: 0.1600 - val_loss: 0.0188 - val_acc: 0.0400\n",
      "Epoch 1173/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0171 - acc: 0.1583 - val_loss: 0.0186 - val_acc: 0.0200\n",
      "Epoch 1174/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0173 - acc: 0.1200 - val_loss: 0.0181 - val_acc: 0.0200\n",
      "Epoch 1175/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0172 - acc: 0.1117 - val_loss: 0.0170 - val_acc: 0.0200\n",
      "Epoch 1176/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0171 - acc: 0.1250 - val_loss: 0.0171 - val_acc: 0.0400\n",
      "Epoch 1177/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0170 - acc: 0.1333 - val_loss: 0.0176 - val_acc: 0.0400\n",
      "Epoch 1178/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0170 - acc: 0.1317 - val_loss: 0.0179 - val_acc: 0.0267\n",
      "Epoch 1179/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0172 - acc: 0.1333 - val_loss: 0.0180 - val_acc: 0.0200\n",
      "Epoch 1180/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0174 - acc: 0.1350 - val_loss: 0.0181 - val_acc: 0.0400\n",
      "Epoch 1181/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0177 - acc: 0.1250 - val_loss: 0.0182 - val_acc: 0.0400\n",
      "Epoch 1182/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0181 - acc: 0.1183 - val_loss: 0.0181 - val_acc: 0.0400\n",
      "Epoch 1183/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0185 - acc: 0.1167 - val_loss: 0.0180 - val_acc: 0.0400\n",
      "Epoch 1184/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.1133 - val_loss: 0.0178 - val_acc: 0.0400\n",
      "Epoch 1185/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0191 - acc: 0.1100 - val_loss: 0.0175 - val_acc: 0.0400\n",
      "Epoch 1186/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0192 - acc: 0.1133 - val_loss: 0.0176 - val_acc: 0.0400\n",
      "Epoch 1187/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0195 - acc: 0.1000 - val_loss: 0.0193 - val_acc: 0.0200\n",
      "Epoch 1188/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0214 - acc: 0.1050 - val_loss: 0.0261 - val_acc: 0.0000e+00\n",
      "Epoch 1189/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0257 - acc: 0.0650 - val_loss: 0.0311 - val_acc: 0.0000e+00\n",
      "Epoch 1190/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0261 - acc: 0.0717 - val_loss: 0.0172 - val_acc: 0.0200\n",
      "Epoch 1191/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0228 - acc: 0.1217 - val_loss: 0.0226 - val_acc: 0.0733\n",
      "Epoch 1192/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0237 - acc: 0.1217 - val_loss: 0.0198 - val_acc: 0.0400\n",
      "Epoch 1193/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0198 - acc: 0.1767 - val_loss: 0.0219 - val_acc: 0.0200\n",
      "Epoch 1194/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0188 - acc: 0.1217 - val_loss: 0.0181 - val_acc: 0.0200\n",
      "Epoch 1195/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0190 - acc: 0.1183 - val_loss: 0.0184 - val_acc: 0.0200\n",
      "Epoch 1196/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0191 - acc: 0.1467 - val_loss: 0.0184 - val_acc: 0.0400\n",
      "Epoch 1197/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0181 - acc: 0.1583 - val_loss: 0.0185 - val_acc: 0.0400\n",
      "Epoch 1198/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0169 - acc: 0.1367 - val_loss: 0.0180 - val_acc: 0.0200\n",
      "Epoch 1199/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0168 - acc: 0.1167 - val_loss: 0.0177 - val_acc: 0.0200\n",
      "Epoch 1200/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0167 - acc: 0.1133 - val_loss: 0.0173 - val_acc: 0.0200\n",
      "Epoch 1201/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0164 - acc: 0.1100 - val_loss: 0.0169 - val_acc: 0.0200\n",
      "Epoch 1202/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0163 - acc: 0.1133 - val_loss: 0.0169 - val_acc: 0.0200\n",
      "Epoch 1203/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0162 - acc: 0.1133 - val_loss: 0.0168 - val_acc: 0.0400\n",
      "Epoch 1204/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0162 - acc: 0.1133 - val_loss: 0.0168 - val_acc: 0.0200\n",
      "Epoch 1205/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0163 - acc: 0.1083 - val_loss: 0.0168 - val_acc: 0.0200\n",
      "Epoch 1206/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0164 - acc: 0.1083 - val_loss: 0.0168 - val_acc: 0.0200\n",
      "Epoch 1207/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0166 - acc: 0.1100 - val_loss: 0.0170 - val_acc: 0.0200\n",
      "Epoch 1208/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0167 - acc: 0.1100 - val_loss: 0.0172 - val_acc: 0.0200\n",
      "Epoch 1209/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0169 - acc: 0.1067 - val_loss: 0.0179 - val_acc: 0.0200\n",
      "Epoch 1210/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0171 - acc: 0.1083 - val_loss: 0.0191 - val_acc: 0.0200\n",
      "Epoch 1211/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0178 - acc: 0.1233 - val_loss: 0.0214 - val_acc: 0.0200\n",
      "Epoch 1212/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.1133 - val_loss: 0.0233 - val_acc: 0.0000e+00\n",
      "Epoch 1213/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0200 - acc: 0.1100 - val_loss: 0.0239 - val_acc: 0.0000e+00\n",
      "Epoch 1214/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0203 - acc: 0.1100 - val_loss: 0.0205 - val_acc: 0.0000e+00\n",
      "Epoch 1215/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0207 - acc: 0.1033 - val_loss: 0.0171 - val_acc: 0.0200\n",
      "Epoch 1216/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0219 - acc: 0.1283 - val_loss: 0.0190 - val_acc: 0.0333\n",
      "Epoch 1217/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0221 - acc: 0.1467 - val_loss: 0.0225 - val_acc: 0.0200\n",
      "Epoch 1218/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0218 - acc: 0.1450 - val_loss: 0.0229 - val_acc: 0.0400\n",
      "Epoch 1219/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0230 - acc: 0.1250 - val_loss: 0.0189 - val_acc: 0.0200\n",
      "Epoch 1220/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0191 - acc: 0.1683 - val_loss: 0.0184 - val_acc: 0.0200\n",
      "Epoch 1221/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0173 - acc: 0.1200 - val_loss: 0.0196 - val_acc: 0.0200\n",
      "Epoch 1222/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0179 - acc: 0.0983 - val_loss: 0.0181 - val_acc: 0.0200\n",
      "Epoch 1223/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0175 - acc: 0.1100 - val_loss: 0.0167 - val_acc: 0.0200\n",
      "Epoch 1224/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0172 - acc: 0.1433 - val_loss: 0.0175 - val_acc: 0.0200\n",
      "Epoch 1225/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0171 - acc: 0.1433 - val_loss: 0.0179 - val_acc: 0.0200\n",
      "Epoch 1226/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0170 - acc: 0.1450 - val_loss: 0.0174 - val_acc: 0.0200\n",
      "Epoch 1227/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0168 - acc: 0.1417 - val_loss: 0.0171 - val_acc: 0.0200\n",
      "Epoch 1228/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0166 - acc: 0.1267 - val_loss: 0.0169 - val_acc: 0.0400\n",
      "Epoch 1229/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0167 - acc: 0.1083 - val_loss: 0.0169 - val_acc: 0.0400\n",
      "Epoch 1230/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0171 - acc: 0.1067 - val_loss: 0.0170 - val_acc: 0.0200\n",
      "Epoch 1231/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0175 - acc: 0.1100 - val_loss: 0.0175 - val_acc: 0.0200\n",
      "Epoch 1232/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0178 - acc: 0.1117 - val_loss: 0.0186 - val_acc: 0.0200\n",
      "Epoch 1233/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0183 - acc: 0.1083 - val_loss: 0.0212 - val_acc: 0.0000e+00\n",
      "Epoch 1234/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0197 - acc: 0.1033 - val_loss: 0.0250 - val_acc: 0.0000e+00\n",
      "Epoch 1235/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0218 - acc: 0.1000 - val_loss: 0.0255 - val_acc: 0.0000e+00\n",
      "Epoch 1236/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0216 - acc: 0.0917 - val_loss: 0.0181 - val_acc: 0.0000e+00\n",
      "Epoch 1237/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0204 - acc: 0.1150 - val_loss: 0.0183 - val_acc: 0.0200\n",
      "Epoch 1238/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0207 - acc: 0.1500 - val_loss: 0.0208 - val_acc: 0.0400\n",
      "Epoch 1239/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0209 - acc: 0.1433 - val_loss: 0.0195 - val_acc: 0.0400\n",
      "Epoch 1240/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0191 - acc: 0.1600 - val_loss: 0.0180 - val_acc: 0.0400\n",
      "Epoch 1241/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0168 - acc: 0.1600 - val_loss: 0.0183 - val_acc: 0.0200\n",
      "Epoch 1242/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0172 - acc: 0.1200 - val_loss: 0.0181 - val_acc: 0.0200\n",
      "Epoch 1243/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0172 - acc: 0.1117 - val_loss: 0.0167 - val_acc: 0.0200\n",
      "Epoch 1244/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0170 - acc: 0.1267 - val_loss: 0.0167 - val_acc: 0.0200\n",
      "Epoch 1245/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0169 - acc: 0.1500 - val_loss: 0.0176 - val_acc: 0.0200\n",
      "Epoch 1246/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0171 - acc: 0.1533 - val_loss: 0.0182 - val_acc: 0.0400\n",
      "Epoch 1247/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0174 - acc: 0.1467 - val_loss: 0.0186 - val_acc: 0.0200\n",
      "Epoch 1248/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0179 - acc: 0.1450 - val_loss: 0.0190 - val_acc: 0.0200\n",
      "Epoch 1249/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0184 - acc: 0.1300 - val_loss: 0.0193 - val_acc: 0.0533\n",
      "Epoch 1250/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0189 - acc: 0.1267 - val_loss: 0.0193 - val_acc: 0.0533\n",
      "Epoch 1251/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.1267 - val_loss: 0.0190 - val_acc: 0.0400\n",
      "Epoch 1252/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0199 - acc: 0.1200 - val_loss: 0.0185 - val_acc: 0.0400\n",
      "Epoch 1253/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0197 - acc: 0.1233 - val_loss: 0.0175 - val_acc: 0.0400\n",
      "Epoch 1254/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.1183 - val_loss: 0.0174 - val_acc: 0.0400\n",
      "Epoch 1255/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.1050 - val_loss: 0.0197 - val_acc: 0.0200\n",
      "Epoch 1256/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0214 - acc: 0.1050 - val_loss: 0.0273 - val_acc: 0.0000e+00\n",
      "Epoch 1257/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0256 - acc: 0.0700 - val_loss: 0.0259 - val_acc: 0.0000e+00\n",
      "Epoch 1258/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0229 - acc: 0.0717 - val_loss: 0.0170 - val_acc: 0.0200\n",
      "Epoch 1259/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0212 - acc: 0.1283 - val_loss: 0.0204 - val_acc: 0.0533\n",
      "Epoch 1260/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0212 - acc: 0.1500 - val_loss: 0.0187 - val_acc: 0.0400\n",
      "Epoch 1261/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0185 - acc: 0.1917 - val_loss: 0.0200 - val_acc: 0.0400\n",
      "Epoch 1262/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.1300 - val_loss: 0.0182 - val_acc: 0.0200\n",
      "Epoch 1263/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0178 - acc: 0.1233 - val_loss: 0.0170 - val_acc: 0.0200\n",
      "Epoch 1264/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0176 - acc: 0.1417 - val_loss: 0.0176 - val_acc: 0.0400\n",
      "Epoch 1265/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0174 - acc: 0.1617 - val_loss: 0.0177 - val_acc: 0.0400\n",
      "Epoch 1266/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0168 - acc: 0.1650 - val_loss: 0.0171 - val_acc: 0.0400\n",
      "Epoch 1267/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0162 - acc: 0.1367 - val_loss: 0.0168 - val_acc: 0.0200\n",
      "Epoch 1268/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0163 - acc: 0.1133 - val_loss: 0.0175 - val_acc: 0.0200\n",
      "Epoch 1269/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0173 - acc: 0.1067 - val_loss: 0.0190 - val_acc: 0.0200\n",
      "Epoch 1270/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0190 - acc: 0.1050 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 1271/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0206 - acc: 0.0967 - val_loss: 0.0241 - val_acc: 0.0133\n",
      "Epoch 1272/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0218 - acc: 0.0967 - val_loss: 0.0200 - val_acc: 0.0133\n",
      "Epoch 1273/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0198 - acc: 0.1267 - val_loss: 0.0180 - val_acc: 0.0467\n",
      "Epoch 1274/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0190 - acc: 0.1350 - val_loss: 0.0188 - val_acc: 0.0867\n",
      "Epoch 1275/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.1600 - val_loss: 0.0184 - val_acc: 0.0333\n",
      "Epoch 1276/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0181 - acc: 0.1733 - val_loss: 0.0181 - val_acc: 0.0400\n",
      "Epoch 1277/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0168 - acc: 0.1633 - val_loss: 0.0180 - val_acc: 0.0200\n",
      "Epoch 1278/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0167 - acc: 0.1233 - val_loss: 0.0180 - val_acc: 0.0200\n",
      "Epoch 1279/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0168 - acc: 0.1133 - val_loss: 0.0177 - val_acc: 0.0200\n",
      "Epoch 1280/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0167 - acc: 0.1150 - val_loss: 0.0172 - val_acc: 0.0200\n",
      "Epoch 1281/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0165 - acc: 0.1183 - val_loss: 0.0738 - val_acc: 0.2067\n",
      "Epoch 1282/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0989 - acc: 0.0250 - val_loss: 0.0738 - val_acc: 0.0000e+00\n",
      "Epoch 1283/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0753 - acc: 0.1383 - val_loss: 0.0788 - val_acc: 0.0667\n",
      "Epoch 1284/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0667 - acc: 0.0633 - val_loss: 0.0452 - val_acc: 0.0000e+00\n",
      "Epoch 1285/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0659 - acc: 0.0167 - val_loss: 0.0557 - val_acc: 0.0400\n",
      "Epoch 1286/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0562 - acc: 0.0333 - val_loss: 0.0474 - val_acc: 0.0000e+00\n",
      "Epoch 1287/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0410 - acc: 0.0267 - val_loss: 0.0331 - val_acc: 0.0533\n",
      "Epoch 1288/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0362 - acc: 0.0300 - val_loss: 0.0344 - val_acc: 0.0000e+00\n",
      "Epoch 1289/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0360 - acc: 0.0167 - val_loss: 0.0302 - val_acc: 0.0000e+00\n",
      "Epoch 1290/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0282 - acc: 0.0283 - val_loss: 0.0278 - val_acc: 0.0000e+00\n",
      "Epoch 1291/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0269 - acc: 0.0117 - val_loss: 0.0233 - val_acc: 0.0000e+00\n",
      "Epoch 1292/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0261 - acc: 0.0267 - val_loss: 0.0251 - val_acc: 0.0000e+00\n",
      "Epoch 1293/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0245 - acc: 0.0283 - val_loss: 0.0234 - val_acc: 0.0133\n",
      "Epoch 1294/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0226 - acc: 0.0650 - val_loss: 0.0215 - val_acc: 0.0000e+00\n",
      "Epoch 1295/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0225 - acc: 0.0633 - val_loss: 0.0217 - val_acc: 0.0200\n",
      "Epoch 1296/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0223 - acc: 0.0550 - val_loss: 0.0214 - val_acc: 0.0667\n",
      "Epoch 1297/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0213 - acc: 0.0750 - val_loss: 0.0204 - val_acc: 0.0200\n",
      "Epoch 1298/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0207 - acc: 0.0650 - val_loss: 0.0205 - val_acc: 0.0200\n",
      "Epoch 1299/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0205 - acc: 0.0750 - val_loss: 0.0201 - val_acc: 0.0200\n",
      "Epoch 1300/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0204 - acc: 0.1050 - val_loss: 0.0198 - val_acc: 0.0533\n",
      "Epoch 1301/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0205 - acc: 0.1000 - val_loss: 0.0197 - val_acc: 0.0200\n",
      "Epoch 1302/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0206 - acc: 0.1017 - val_loss: 0.0201 - val_acc: 0.0533\n",
      "Epoch 1303/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0206 - acc: 0.1050 - val_loss: 0.0202 - val_acc: 0.0533\n",
      "Epoch 1304/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0207 - acc: 0.1067 - val_loss: 0.0207 - val_acc: 0.0200\n",
      "Epoch 1305/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0209 - acc: 0.1183 - val_loss: 0.0213 - val_acc: 0.0333\n",
      "Epoch 1306/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0214 - acc: 0.1250 - val_loss: 0.0228 - val_acc: 0.0400\n",
      "Epoch 1307/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0216 - acc: 0.1450 - val_loss: 0.0232 - val_acc: 0.0200\n",
      "Epoch 1308/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0227 - acc: 0.1217 - val_loss: 0.0248 - val_acc: 0.0333\n",
      "Epoch 1309/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0240 - acc: 0.1083 - val_loss: 0.0237 - val_acc: 0.0533\n",
      "Epoch 1310/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0245 - acc: 0.0967 - val_loss: 0.0203 - val_acc: 0.0400\n",
      "Epoch 1311/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0223 - acc: 0.1083 - val_loss: 0.0223 - val_acc: 0.0200\n",
      "Epoch 1312/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0220 - acc: 0.0967 - val_loss: 0.0252 - val_acc: 0.0133\n",
      "Epoch 1313/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0218 - acc: 0.0733 - val_loss: 0.0194 - val_acc: 0.0200\n",
      "Epoch 1314/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0227 - acc: 0.1167 - val_loss: 0.0226 - val_acc: 0.0333\n",
      "Epoch 1315/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0236 - acc: 0.1033 - val_loss: 0.0205 - val_acc: 0.0200\n",
      "Epoch 1316/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0213 - acc: 0.1100 - val_loss: 0.0211 - val_acc: 0.0200\n",
      "Epoch 1317/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0194 - acc: 0.1283 - val_loss: 0.0192 - val_acc: 0.0200\n",
      "Epoch 1318/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.1250 - val_loss: 0.0195 - val_acc: 0.0200\n",
      "Epoch 1319/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0209 - acc: 0.1233 - val_loss: 0.0196 - val_acc: 0.0333\n",
      "Epoch 1320/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0198 - acc: 0.1267 - val_loss: 0.0187 - val_acc: 0.0200\n",
      "Epoch 1321/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0192 - acc: 0.1217 - val_loss: 0.0191 - val_acc: 0.0200\n",
      "Epoch 1322/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.1233 - val_loss: 0.0189 - val_acc: 0.0200\n",
      "Epoch 1323/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0199 - acc: 0.1350 - val_loss: 0.0188 - val_acc: 0.0200\n",
      "Epoch 1324/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0200 - acc: 0.1317 - val_loss: 0.0193 - val_acc: 0.0333\n",
      "Epoch 1325/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0192 - acc: 0.1450 - val_loss: 0.0185 - val_acc: 0.0200\n",
      "Epoch 1326/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.1433 - val_loss: 0.0181 - val_acc: 0.0200\n",
      "Epoch 1327/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0190 - acc: 0.0933 - val_loss: 0.0188 - val_acc: 0.0200\n",
      "Epoch 1328/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0199 - acc: 0.0950 - val_loss: 0.0185 - val_acc: 0.0200\n",
      "Epoch 1329/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0210 - acc: 0.1117 - val_loss: 0.0195 - val_acc: 0.0200\n",
      "Epoch 1330/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0207 - acc: 0.1050 - val_loss: 0.0197 - val_acc: 0.0200\n",
      "Epoch 1331/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0196 - acc: 0.1217 - val_loss: 0.0192 - val_acc: 0.0400\n",
      "Epoch 1332/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.1333 - val_loss: 0.0208 - val_acc: 0.0200\n",
      "Epoch 1333/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0201 - acc: 0.1050 - val_loss: 0.0231 - val_acc: 0.0000e+00\n",
      "Epoch 1334/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0202 - acc: 0.0800 - val_loss: 0.0175 - val_acc: 0.0200\n",
      "Epoch 1335/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0249 - acc: 0.1217 - val_loss: 0.0233 - val_acc: 0.0800\n",
      "Epoch 1336/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0274 - acc: 0.1183 - val_loss: 0.0255 - val_acc: 0.0200\n",
      "Epoch 1337/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0261 - acc: 0.0917 - val_loss: 0.0199 - val_acc: 0.0200\n",
      "Epoch 1338/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0200 - acc: 0.1167 - val_loss: 0.0218 - val_acc: 0.0200\n",
      "Epoch 1339/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0195 - acc: 0.0850 - val_loss: 0.0188 - val_acc: 0.0200\n",
      "Epoch 1340/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0203 - acc: 0.1200 - val_loss: 0.0197 - val_acc: 0.0333\n",
      "Epoch 1341/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0199 - acc: 0.1200 - val_loss: 0.0196 - val_acc: 0.0200\n",
      "Epoch 1342/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0185 - acc: 0.1300 - val_loss: 0.0190 - val_acc: 0.0200\n",
      "Epoch 1343/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0187 - acc: 0.1250 - val_loss: 0.0183 - val_acc: 0.0200\n",
      "Epoch 1344/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0191 - acc: 0.1383 - val_loss: 0.0185 - val_acc: 0.0200\n",
      "Epoch 1345/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0187 - acc: 0.1417 - val_loss: 0.0188 - val_acc: 0.0200\n",
      "Epoch 1346/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0181 - acc: 0.1500 - val_loss: 0.0182 - val_acc: 0.0200\n",
      "Epoch 1347/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0182 - acc: 0.1367 - val_loss: 0.0181 - val_acc: 0.0200\n",
      "Epoch 1348/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0185 - acc: 0.1400 - val_loss: 0.0183 - val_acc: 0.0200\n",
      "Epoch 1349/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0185 - acc: 0.1450 - val_loss: 0.0183 - val_acc: 0.0400\n",
      "Epoch 1350/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0182 - acc: 0.1550 - val_loss: 0.0182 - val_acc: 0.0400\n",
      "Epoch 1351/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0179 - acc: 0.1517 - val_loss: 0.0184 - val_acc: 0.0200\n",
      "Epoch 1352/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.1350 - val_loss: 0.0182 - val_acc: 0.0200\n",
      "Epoch 1353/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0177 - acc: 0.1217 - val_loss: 0.0177 - val_acc: 0.0200\n",
      "Epoch 1354/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0179 - acc: 0.1250 - val_loss: 0.0174 - val_acc: 0.0200\n",
      "Epoch 1355/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0182 - acc: 0.1333 - val_loss: 0.0178 - val_acc: 0.0200\n",
      "Epoch 1356/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0188 - acc: 0.1333 - val_loss: 0.0194 - val_acc: 0.0400\n",
      "Epoch 1357/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0193 - acc: 0.1333 - val_loss: 0.0199 - val_acc: 0.0200\n",
      "Epoch 1358/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0198 - acc: 0.1350 - val_loss: 0.0183 - val_acc: 0.0200\n",
      "Epoch 1359/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0230 - acc: 0.1383 - val_loss: 0.0320 - val_acc: 0.0733\n",
      "Epoch 1360/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0230 - acc: 0.1067 - val_loss: 0.0213 - val_acc: 0.0200\n",
      "Epoch 1361/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0234 - acc: 0.1133 - val_loss: 0.0217 - val_acc: 0.0200\n",
      "Epoch 1362/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0236 - acc: 0.0833 - val_loss: 0.0251 - val_acc: 0.0667\n",
      "Epoch 1363/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0234 - acc: 0.0550 - val_loss: 0.0259 - val_acc: 0.0200\n",
      "Epoch 1364/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0248 - acc: 0.0583 - val_loss: 0.0187 - val_acc: 0.0000e+00\n",
      "Epoch 1365/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0296 - acc: 0.0967 - val_loss: 0.0325 - val_acc: 0.0333\n",
      "Epoch 1366/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0302 - acc: 0.0783 - val_loss: 0.0271 - val_acc: 0.0333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1367/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0290 - acc: 0.0767 - val_loss: 0.0224 - val_acc: 0.0333\n",
      "Epoch 1368/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0219 - acc: 0.1133 - val_loss: 0.0223 - val_acc: 0.0133\n",
      "Epoch 1369/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0211 - acc: 0.0767 - val_loss: 0.0202 - val_acc: 0.0400\n",
      "Epoch 1370/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0213 - acc: 0.1033 - val_loss: 0.0225 - val_acc: 0.0400\n",
      "Epoch 1371/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0221 - acc: 0.1083 - val_loss: 0.0197 - val_acc: 0.0200\n",
      "Epoch 1372/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0192 - acc: 0.1333 - val_loss: 0.0192 - val_acc: 0.0200\n",
      "Epoch 1373/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0183 - acc: 0.1317 - val_loss: 0.0191 - val_acc: 0.0667\n",
      "Epoch 1374/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0192 - acc: 0.1267 - val_loss: 0.0204 - val_acc: 0.0400\n",
      "Epoch 1375/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0203 - acc: 0.1183 - val_loss: 0.0201 - val_acc: 0.0400\n",
      "Epoch 1376/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0197 - acc: 0.1267 - val_loss: 0.0193 - val_acc: 0.0400\n",
      "Epoch 1377/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0186 - acc: 0.1283 - val_loss: 0.0190 - val_acc: 0.0533\n",
      "Epoch 1378/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0186 - acc: 0.1300 - val_loss: 0.0196 - val_acc: 0.0533\n",
      "Epoch 1379/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0193 - acc: 0.1283 - val_loss: 0.0204 - val_acc: 0.0400\n",
      "Epoch 1380/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0198 - acc: 0.1350 - val_loss: 0.0204 - val_acc: 0.0400\n",
      "Epoch 1381/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0198 - acc: 0.1367 - val_loss: 0.0197 - val_acc: 0.0400\n",
      "Epoch 1382/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.1333 - val_loss: 0.0195 - val_acc: 0.0400\n",
      "Epoch 1383/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0190 - acc: 0.1367 - val_loss: 0.0196 - val_acc: 0.0400\n",
      "Epoch 1384/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0190 - acc: 0.1400 - val_loss: 0.0198 - val_acc: 0.0400\n",
      "Epoch 1385/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0192 - acc: 0.1383 - val_loss: 0.0201 - val_acc: 0.0400\n",
      "Epoch 1386/1500\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.0194 - acc: 0.1367 - val_loss: 0.0202 - val_acc: 0.0400\n",
      "Epoch 1387/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.1367 - val_loss: 0.0200 - val_acc: 0.0400\n",
      "Epoch 1388/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0194 - acc: 0.1400 - val_loss: 0.0198 - val_acc: 0.0400\n",
      "Epoch 1389/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0192 - acc: 0.1367 - val_loss: 0.0196 - val_acc: 0.0400\n",
      "Epoch 1390/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.1400 - val_loss: 0.0196 - val_acc: 0.0400\n",
      "Epoch 1391/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.1417 - val_loss: 0.0197 - val_acc: 0.0400\n",
      "Epoch 1392/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0190 - acc: 0.1450 - val_loss: 0.0199 - val_acc: 0.0400\n",
      "Epoch 1393/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0192 - acc: 0.1417 - val_loss: 0.0201 - val_acc: 0.0400\n",
      "Epoch 1394/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.1383 - val_loss: 0.0200 - val_acc: 0.0400\n",
      "Epoch 1395/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0194 - acc: 0.1400 - val_loss: 0.0198 - val_acc: 0.0400\n",
      "Epoch 1396/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0192 - acc: 0.1433 - val_loss: 0.0195 - val_acc: 0.0400\n",
      "Epoch 1397/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.1400 - val_loss: 0.0194 - val_acc: 0.0400\n",
      "Epoch 1398/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0187 - acc: 0.1383 - val_loss: 0.0195 - val_acc: 0.0400\n",
      "Epoch 1399/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0188 - acc: 0.1417 - val_loss: 0.0197 - val_acc: 0.0400\n",
      "Epoch 1400/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.1433 - val_loss: 0.0199 - val_acc: 0.0400\n",
      "Epoch 1401/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0191 - acc: 0.1433 - val_loss: 0.0200 - val_acc: 0.0400\n",
      "Epoch 1402/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0192 - acc: 0.1450 - val_loss: 0.0199 - val_acc: 0.0400\n",
      "Epoch 1403/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0192 - acc: 0.1433 - val_loss: 0.0196 - val_acc: 0.0400\n",
      "Epoch 1404/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0190 - acc: 0.1417 - val_loss: 0.0194 - val_acc: 0.0400\n",
      "Epoch 1405/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0187 - acc: 0.1400 - val_loss: 0.0193 - val_acc: 0.0400\n",
      "Epoch 1406/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0186 - acc: 0.1383 - val_loss: 0.0194 - val_acc: 0.0400\n",
      "Epoch 1407/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0187 - acc: 0.1400 - val_loss: 0.0196 - val_acc: 0.0400\n",
      "Epoch 1408/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0188 - acc: 0.1417 - val_loss: 0.0198 - val_acc: 0.0400\n",
      "Epoch 1409/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0190 - acc: 0.1400 - val_loss: 0.0199 - val_acc: 0.0400\n",
      "Epoch 1410/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0191 - acc: 0.1417 - val_loss: 0.0198 - val_acc: 0.0400\n",
      "Epoch 1411/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0191 - acc: 0.1417 - val_loss: 0.0196 - val_acc: 0.0400\n",
      "Epoch 1412/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.1400 - val_loss: 0.0193 - val_acc: 0.0400\n",
      "Epoch 1413/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0186 - acc: 0.1367 - val_loss: 0.0192 - val_acc: 0.0400\n",
      "Epoch 1414/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0184 - acc: 0.1333 - val_loss: 0.0192 - val_acc: 0.0400\n",
      "Epoch 1415/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0184 - acc: 0.1367 - val_loss: 0.0194 - val_acc: 0.0400\n",
      "Epoch 1416/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0186 - acc: 0.1367 - val_loss: 0.0197 - val_acc: 0.0400\n",
      "Epoch 1417/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0189 - acc: 0.1400 - val_loss: 0.0200 - val_acc: 0.0400\n",
      "Epoch 1418/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0191 - acc: 0.1400 - val_loss: 0.0200 - val_acc: 0.0400\n",
      "Epoch 1419/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.1383 - val_loss: 0.0197 - val_acc: 0.0400\n",
      "Epoch 1420/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0192 - acc: 0.1333 - val_loss: 0.0193 - val_acc: 0.0400\n",
      "Epoch 1421/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0188 - acc: 0.1333 - val_loss: 0.0190 - val_acc: 0.0400\n",
      "Epoch 1422/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0184 - acc: 0.1317 - val_loss: 0.0188 - val_acc: 0.0400\n",
      "Epoch 1423/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0181 - acc: 0.1350 - val_loss: 0.0189 - val_acc: 0.0400\n",
      "Epoch 1424/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0182 - acc: 0.1383 - val_loss: 0.0193 - val_acc: 0.0400\n",
      "Epoch 1425/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0185 - acc: 0.1333 - val_loss: 0.0197 - val_acc: 0.0400\n",
      "Epoch 1426/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0188 - acc: 0.1333 - val_loss: 0.0200 - val_acc: 0.0400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1427/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0192 - acc: 0.1333 - val_loss: 0.0200 - val_acc: 0.0400\n",
      "Epoch 1428/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0194 - acc: 0.1300 - val_loss: 0.0196 - val_acc: 0.0400\n",
      "Epoch 1429/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.1250 - val_loss: 0.0191 - val_acc: 0.0400\n",
      "Epoch 1430/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0188 - acc: 0.1333 - val_loss: 0.0188 - val_acc: 0.0400\n",
      "Epoch 1431/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0182 - acc: 0.1383 - val_loss: 0.0185 - val_acc: 0.0333\n",
      "Epoch 1432/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0178 - acc: 0.1417 - val_loss: 0.0185 - val_acc: 0.0400\n",
      "Epoch 1433/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0179 - acc: 0.1400 - val_loss: 0.0190 - val_acc: 0.0400\n",
      "Epoch 1434/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0182 - acc: 0.1400 - val_loss: 0.0197 - val_acc: 0.0400\n",
      "Epoch 1435/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0187 - acc: 0.1367 - val_loss: 0.0201 - val_acc: 0.0400\n",
      "Epoch 1436/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0191 - acc: 0.1317 - val_loss: 0.0201 - val_acc: 0.0400\n",
      "Epoch 1437/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.1233 - val_loss: 0.0198 - val_acc: 0.0400\n",
      "Epoch 1438/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0197 - acc: 0.1167 - val_loss: 0.0193 - val_acc: 0.0400\n",
      "Epoch 1439/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0194 - acc: 0.1283 - val_loss: 0.0188 - val_acc: 0.0200\n",
      "Epoch 1440/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0188 - acc: 0.1367 - val_loss: 0.0184 - val_acc: 0.0400\n",
      "Epoch 1441/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0180 - acc: 0.1383 - val_loss: 0.0180 - val_acc: 0.0400\n",
      "Epoch 1442/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0175 - acc: 0.1350 - val_loss: 0.0182 - val_acc: 0.0400\n",
      "Epoch 1443/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0174 - acc: 0.1417 - val_loss: 0.0186 - val_acc: 0.0333\n",
      "Epoch 1444/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0179 - acc: 0.1417 - val_loss: 0.0197 - val_acc: 0.0333\n",
      "Epoch 1445/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0186 - acc: 0.1433 - val_loss: 0.0208 - val_acc: 0.0400\n",
      "Epoch 1446/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.1350 - val_loss: 0.0210 - val_acc: 0.0400\n",
      "Epoch 1447/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.1267 - val_loss: 0.0206 - val_acc: 0.0400\n",
      "Epoch 1448/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0209 - acc: 0.1150 - val_loss: 0.0196 - val_acc: 0.0400\n",
      "Epoch 1449/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0209 - acc: 0.1167 - val_loss: 0.0186 - val_acc: 0.0400\n",
      "Epoch 1450/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0198 - acc: 0.1300 - val_loss: 0.0182 - val_acc: 0.0400\n",
      "Epoch 1451/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0186 - acc: 0.1400 - val_loss: 0.0181 - val_acc: 0.0400\n",
      "Epoch 1452/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0175 - acc: 0.1383 - val_loss: 0.0193 - val_acc: 0.0200\n",
      "Epoch 1453/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0175 - acc: 0.1317 - val_loss: 0.0190 - val_acc: 0.0200\n",
      "Epoch 1454/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0183 - acc: 0.1283 - val_loss: 0.0208 - val_acc: 0.0333\n",
      "Epoch 1455/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0192 - acc: 0.1383 - val_loss: 0.0230 - val_acc: 0.0400\n",
      "Epoch 1456/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0199 - acc: 0.1400 - val_loss: 0.0242 - val_acc: 0.0400\n",
      "Epoch 1457/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0219 - acc: 0.1183 - val_loss: 0.0236 - val_acc: 0.0467\n",
      "Epoch 1458/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0251 - acc: 0.1067 - val_loss: 0.0232 - val_acc: 0.0667\n",
      "Epoch 1459/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0280 - acc: 0.1200 - val_loss: 0.0199 - val_acc: 0.0267\n",
      "Epoch 1460/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0296 - acc: 0.1167 - val_loss: 0.0337 - val_acc: 0.0133\n",
      "Epoch 1461/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0262 - acc: 0.1100 - val_loss: 0.0169 - val_acc: 0.0333\n",
      "Epoch 1462/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0209 - acc: 0.1300 - val_loss: 0.0236 - val_acc: 0.0533\n",
      "Epoch 1463/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0227 - acc: 0.1117 - val_loss: 0.0200 - val_acc: 0.0733\n",
      "Epoch 1464/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0192 - acc: 0.1367 - val_loss: 0.0184 - val_acc: 0.0200\n",
      "Epoch 1465/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0187 - acc: 0.1300 - val_loss: 0.0197 - val_acc: 0.0333\n",
      "Epoch 1466/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0179 - acc: 0.1117 - val_loss: 0.0194 - val_acc: 0.0400\n",
      "Epoch 1467/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0190 - acc: 0.1233 - val_loss: 0.0196 - val_acc: 0.0800\n",
      "Epoch 1468/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0191 - acc: 0.1300 - val_loss: 0.0190 - val_acc: 0.0333\n",
      "Epoch 1469/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0173 - acc: 0.1250 - val_loss: 0.0179 - val_acc: 0.0200\n",
      "Epoch 1470/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0175 - acc: 0.1350 - val_loss: 0.0193 - val_acc: 0.0200\n",
      "Epoch 1471/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0184 - acc: 0.1250 - val_loss: 0.0202 - val_acc: 0.0533\n",
      "Epoch 1472/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0187 - acc: 0.1350 - val_loss: 0.0194 - val_acc: 0.0400\n",
      "Epoch 1473/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0184 - acc: 0.1367 - val_loss: 0.0188 - val_acc: 0.0200\n",
      "Epoch 1474/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.1367 - val_loss: 0.0185 - val_acc: 0.0200\n",
      "Epoch 1475/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0174 - acc: 0.1417 - val_loss: 0.0187 - val_acc: 0.0400\n",
      "Epoch 1476/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.1417 - val_loss: 0.0192 - val_acc: 0.0467\n",
      "Epoch 1477/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0179 - acc: 0.1383 - val_loss: 0.0192 - val_acc: 0.0400\n",
      "Epoch 1478/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0180 - acc: 0.1367 - val_loss: 0.0190 - val_acc: 0.0200\n",
      "Epoch 1479/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0180 - acc: 0.1367 - val_loss: 0.0188 - val_acc: 0.0200\n",
      "Epoch 1480/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0178 - acc: 0.1350 - val_loss: 0.0185 - val_acc: 0.0200\n",
      "Epoch 1481/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0176 - acc: 0.1400 - val_loss: 0.0184 - val_acc: 0.0200\n",
      "Epoch 1482/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0174 - acc: 0.1400 - val_loss: 0.0185 - val_acc: 0.0200\n",
      "Epoch 1483/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0174 - acc: 0.1400 - val_loss: 0.0187 - val_acc: 0.0200\n",
      "Epoch 1484/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0175 - acc: 0.1383 - val_loss: 0.0189 - val_acc: 0.0200\n",
      "Epoch 1485/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.1383 - val_loss: 0.0192 - val_acc: 0.0200\n",
      "Epoch 1486/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0180 - acc: 0.1350 - val_loss: 0.0195 - val_acc: 0.0400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1487/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0185 - acc: 0.1283 - val_loss: 0.0195 - val_acc: 0.0400\n",
      "Epoch 1488/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0190 - acc: 0.1167 - val_loss: 0.0193 - val_acc: 0.0200\n",
      "Epoch 1489/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0194 - acc: 0.1167 - val_loss: 0.0189 - val_acc: 0.0200\n",
      "Epoch 1490/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0195 - acc: 0.1183 - val_loss: 0.0183 - val_acc: 0.0200\n",
      "Epoch 1491/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0190 - acc: 0.1150 - val_loss: 0.0179 - val_acc: 0.0200\n",
      "Epoch 1492/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0182 - acc: 0.1183 - val_loss: 0.0179 - val_acc: 0.0200\n",
      "Epoch 1493/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0170 - acc: 0.1400 - val_loss: 0.0175 - val_acc: 0.0200\n",
      "Epoch 1494/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0165 - acc: 0.1317 - val_loss: 0.0177 - val_acc: 0.0200\n",
      "Epoch 1495/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0170 - acc: 0.1383 - val_loss: 0.0196 - val_acc: 0.0333\n",
      "Epoch 1496/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0181 - acc: 0.1483 - val_loss: 0.0222 - val_acc: 0.0200\n",
      "Epoch 1497/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0194 - acc: 0.1400 - val_loss: 0.0225 - val_acc: 0.0200\n",
      "Epoch 1498/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0207 - acc: 0.1250 - val_loss: 0.0215 - val_acc: 0.0200\n",
      "Epoch 1499/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0229 - acc: 0.1133 - val_loss: 0.0198 - val_acc: 0.0400\n",
      "Epoch 1500/1500\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0240 - acc: 0.1350 - val_loss: 0.0185 - val_acc: 0.0467\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1500\n",
    "n_batch = 150\n",
    "n_neurons = 200\n",
    "print(len(train))\n",
    "print(train.shape)\n",
    "#model, architecture = fit_lstm(train, n_prev, n_forecast, n_batch, n_epochs, n_neurons)\n",
    "model, architecture,history = fit_lstm(train, n_prev, n_forecast, n_batch, n_epochs, n_neurons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-66997b9143b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../training_histories/square_train_history_dict.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_pi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_pi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "#Save model history\n",
    "import pickle\n",
    "with open('../training_histories/square_train_history_dict.pickle', 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b6ad09c29fe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../training_histories/square_train_history_dict'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".pickle\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m   1382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload_eof\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_eof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_eof\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "name = '../training_histories/square_train_history_dict'\n",
    "filename = open(name + \".pickle\",\"rb\")\n",
    "history = pickle.load(filename)\n",
    "filename.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acc', 'loss', 'val_acc', 'val_loss']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb4a41c6690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.keys())\n",
    "# summarize history for accuracy\n",
    "# plt.plot(history.history['mean_absolute_error'])\n",
    "# plt.plot(history.history['val_mean_absolute_error'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'validation'], loc='upper left')\n",
    "# plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(np.log(history['loss']))\n",
    "plt.plot(np.log(history['val_loss']))\n",
    "plt.title('Square Wave Training')\n",
    "plt.ylabel('Log Loss [MSE]', fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/tensorflow/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3f94d23c2bc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../checkpoints/squareLSTM_1500epoch_200neuron_180dense.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#model.save('loadtime_3000epoch_100batch_200neuron_180dense_div2point3.h5')  # creates a HDF5 file 'my_model.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#del model  # deletes the existing model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model.save('../checkpoints/squareLSTM_1500epoch_200neuron_180dense.h5')\n",
    "#model.save('loadtime_3000epoch_100batch_200neuron_180dense_div2point3.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "#del model  # deletes the existing model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "#model = load_model('loadtime_200epoch_200batch_200neuron_180dense.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='../assets/lstm_architecture_triangle.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_lstm(model, X, n_batch):\n",
    "    \n",
    "    Xs = [X for _ in range(n_batch)]\n",
    "    \n",
    "    x_stack = np.stack((Xs))\n",
    "    print(x_stack.shape)\n",
    "    \n",
    "    X = x_stack.reshape(n_batch,len(X),1)\n",
    "    \n",
    "    #Make a forecast    \n",
    "    forecast = model.predict(X, batch_size=n_batch)\n",
    "    \n",
    "    #Convert to array\n",
    "    return [y_hat for y_hat in forecast[0,:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 186)\n",
      "Average RMSE: 0.0611355323629\n"
     ]
    }
   ],
   "source": [
    "#Run on test data\n",
    "test_x, test_y = test[0:n_prev], test[n_prev:]\n",
    "test_x = np.array(test_x)\n",
    "y_hat = forecast_lstm(model, test_x, n_batch)\n",
    "avg_RMSE = math.sqrt(mean_squared_error(test_y, y_hat))\n",
    "\n",
    "print(\"Average RMSE: \" + str(avg_RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error (MAE) accuracy on the Validation Data, as percentage of maximum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_abs_error_percentage: 1.1917286342190156\n"
     ]
    }
   ],
   "source": [
    "#This should be using the \"height\" of the wave\n",
    "max_height = max(test_y) - min(test_y)\n",
    "# print(max(test_y))\n",
    "# print(min(test_y))\n",
    "# print(max_height)\n",
    "\n",
    "error_sum = 0\n",
    "for i in range(len(test_y)):\n",
    "    error_sum += abs(test_y[i] - y_hat[i])\n",
    "    \n",
    "avg_error_sum = error_sum/len(test_y)\n",
    "mean_abs_error_percentage = (avg_error_sum/max_height)*100\n",
    "print(\"mean_abs_error_percentage: \"+str(mean_abs_error_percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_abs_error_percentage: 1.1407432058832507\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Display Results\n",
    "* Blue: Sample used for prediction\n",
    "* Green: What we are trying to predict\n",
    "* Red: Actual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1500\n",
      "Average RMSE: 0.0611355323629\n",
      "mean_abs_error_percentage: 1.1917286342190156\n",
      "Average predicted value: -0.007219385716222947\n",
      "Variance of predicted values: 0.9853093\n",
      "Min: -1.06299 Max: 1.0849483\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1881314b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Build a larger figure\n",
    "fig=plt.figure(figsize=(20, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "\n",
    "#Training data in Blue\n",
    "plt.plot(data, color=\"gray\")\n",
    "train_plot_x = [x for x in range(len(train_points))]\n",
    "plt.scatter(train_plot_x, train_points, color=\"gray\", s=30)\n",
    "\n",
    "#Display the input (X) for the prediction purple\n",
    "test_x_start = len(data) - n_test\n",
    "test_x_end = len(data) - len(y_hat)\n",
    "\n",
    "test_x_indices = [x for x in range(test_x_start, test_x_end)]\n",
    "test_x_labels = [data[y] for y in test_x_indices]\n",
    "\n",
    "plt.scatter(test_x_indices, test_x_labels, color=\"blue\", s=30)\n",
    "\n",
    "\n",
    "#Define indices for which to plot the prediction\n",
    "index_start = test_x_end\n",
    "index_end = len(data)\n",
    "labels_x = [x for x in range(index_start, index_end)]\n",
    "\n",
    "\n",
    "#Display true labels in green\n",
    "true_labels = [data[y] for y in range(index_start, index_end)]\n",
    "plt.scatter(labels_x, true_labels, color=\"green\", s=30)\n",
    "\n",
    "#Display forecasted labels in red\n",
    "plt.scatter(labels_x, y_hat, color=\"red\", s=30)\n",
    "\n",
    "\n",
    "\n",
    "gray_patch = mpatches.Patch(color='gray', label='Training/Validation data')\n",
    "\n",
    "blue_patch = mpatches.Patch(color='blue', label='Test input example(X)')\n",
    "\n",
    "green_patch = mpatches.Patch(color='green', label='True labels (Y)')\n",
    "\n",
    "red_patch = mpatches.Patch(color='red', label='Forecast (Y-hat)')\n",
    "\n",
    "#plt.legend(handles=[gray_patch, blue_patch, green_patch, red_patch],bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
    "\n",
    "\n",
    "\n",
    "# from matplotlib.font_manager import FontProperties\n",
    "\n",
    "# fontP = FontProperties()\n",
    "# fontP.set_size('large')\n",
    "# plt.legend([red_patch], \"dfdfd\", prop=fontP)\n",
    "\n",
    "#plt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Epochs: \" + str(n_epochs))\n",
    "print(\"Average RMSE: \" + str(avg_RMSE))\n",
    "print(\"mean_abs_error_percentage: \"+str(mean_abs_error_percentage))\n",
    "print(\"Average predicted value: \" + str(sum(y_hat)/len(y_hat)))\n",
    "print(\"Variance of predicted values: \" + str(np.var(y_hat)))\n",
    "print(\"Min: \" + str(min(y_hat)) + \" Max: \"+str(max(y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
